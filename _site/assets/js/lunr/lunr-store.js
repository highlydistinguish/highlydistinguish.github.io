var store = [{
        "title": "GIT useful scripts or error solutions",
        "excerpt":"Script bible   to list merge conflicts files in command line  You can use either one of below three commands  git diff --name-only --diff-filter=U git status --short | grep \"^UU \" git ls-files -u   One line command to add, commit and push one changed file  git status --short | awk '{split($0, a);print a[2]}' | xargs git add &amp;&amp; git commit -m 'commit changes' &amp;&amp; git push origin BRANCH_NAME   to show files commited but not pushed  git diff --stat --cached origin/feature/BRANCH_NAME   to view file content changed  git show PATH/abc.sql   show file change logs   git log is the powerful command for this kind of tasks, as below sample commands  git log --pretty=format:\"%h [%an] %s\" --graph  git log --pretty=format:\"%h [%an] %s\" --graph --since=7.days     %h means short hash   %s is subject   git log --pretty=format:\"%h [%an] %s\" --graph --since=7.days -S bower.json  git log --pretty=format:\"%h [%an] %s\" --graph --since=7.days --grep Npm git log --pretty=format:\"%h [%an] %s\" --graph --since=7.days --committer todd     -S keyword_of_filter_files   Get correct branch name   Sometimes, if you checkout new branch with incorrect case. It still can check it out to local but you’ll get errors when you try to push it to remote.   To solve this issue, please use following command to get correct branch to checkout  git fetch &amp;&amp; git for-each-ref | grep -i 'THE KEY WORD'  | awk '{split($0,a);print a[3]}' git checkout -b BRANCH_NAME_FROM_ABOVE   Errors   failed to push change  Errors as below  fatal: unable to access 'https://tzhang@stash.xxx.com/scm/abc.git/': SSL certificate prob lem: self signed certificate in certificate chain  Solutions:   git config --global http.sslVerify false  ","categories": [],
        "tags": ["DevOps"],
        "url": "/2007/04/28/GIT-scripts-bible-errors.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Java Class Loader",
        "excerpt":"Codecache     The maximum size of the code cache is set via the -XX:ReservedCodeCacheSize=N flag (where N is the default just mentioned for the particular compiler). The code cache is managed like most memory in the JVM: there is an initial size (specified by -XX:InitialCodeCacheSize=N). Allocation of the code cache size starts at the initial size and increases as the cache fills up. The total of native and heap memory used by the JVM yields the total footprint of an application.      The code cache is a resource with a defined maximum size that affects the total amount of compiled code the JVM can run. Tiered compilation can easily use up the entire code cache in its default configuration (particularly in Java 7); monitor the code cache and increase its size if necessary when using tiered compilation. Compilation Thresholds The major factor involved here is how often the code is executed; once it is executed a certain number of times, its compilation threshold is reached, and the com‐ piler deems that it has enough information to compile the code. Compilation is based on two counters in the JVM: the number of times the method has been called, and the number of times any loops in the method have branched back. Branching back can effectively be thought of as the number of times a loop has com‐ pleted execution, either because it reached the end of the loop itself or because it executed a branching statement like continue. When the JVM executes a Java method, it checks the sum of those two counters and decides whether or not the method is eligible for compilation. If it is, the method is queued for compilation So every time the loop completes an execution, the branching counter is incremented and inspected. If the branching counter has exceeded its indi‐ vidual threshold, then the loop (and not the entire method) becomes eligible for compilation. This kind of compilation is called on-stack replacement (OSR), because even if the loop is compiled, that isn’t sufficient: the JVM has to have the ability to start executing the compiled version of the loop while the loop is still running. When the code for the loop has finished compiling, the JVM replaces the code (on-stack), and the next iteration of the loop will execute the much-faster compiled version of the code. Standard compilation is triggered by the value of the -XX:CompileThreshold=N flag. The default value of N for the client compiler is 1,500; for the server compiler it is 10,000. Changing the value of the CompileThreshold flag will cause the the compiler to choose to compile the code sooner (or later) than it normally would have. Periodically (specifically, when the JVM reaches a safepoint), the value of each counter is reduced. Practically speaking, this means that the counters are a relative measure of the recent hotness of the method or loop. One side effect of this is that somewhat-frequently executed code may never be compiled, even for programs that run forever (these methods are sometimes called lukewarm [as opposed to hot]). This is one case where reducing the compilation threshold can be beneficial, and it is another reason why tiered compilation is usually slightly faster than the server compiler alone. Quick Summary   Compilation occurs when the number of times a method or loop has been executed reaches a certain threshold. Changing the threshold values can cause the code to be com‐ piled sooner than it otherwise would. “Lukewarm” methods will never reach the compilation thresh‐ old (particularly for the   server compiler) since the counters de‐ cay over time. that give visibility into the working of the compiler. The most important of these is -XX:+PrintCompilation (which by default is false). If PrintCompilation is enabled, every time a method (or loop) is compiled, the JVM prints out a line with information about what has just been compiled. Usually this number will simply increase monotonically Inspecting Compilation with jstat Seeing the compilation log requires that the program be started with the -XX:+PrintCompilation flag. If the program was started without that flag, you can get some limited visibility into the working of the compiler by using jstat. jstat has two options to provide information about the compiler. The -compiler option supplies summary information about how many methods have been compiled (here 5003 is the process ID of the program to be inspected): % jstat -compiler 5003 Compiled Failed Invalid Time FailedType FailedMethod 206 0   0   1.97    0 Note this also lists the number of methods that failed to compile and the name of the last method that failed to compile; if profiles or other information lead you to suspect that a method is slow because it hasn’t been compiled, this is an easy way to verify that hypothesis. Because jstat takes an optional argument to repeat its operation, you can see over time which methods are being compiled. In this example, jstat repeats the information for process ID 5003 every second (1,000 ms): % jstat -printcompilation 5003 1000 Compiled   It’s easy to read OSR lines like this example as 25% and wonder about the other 75%, but remember that the number is the compilation ID, and the % just signifies OSR compilation.     The best way to gain visibility into how code is being compiled is by enabling PrintCompilation. Output from enabling PrintCompilation can be used to make sure that compilation is proceeding as expected. If a method is compiled using standard compilation, then the next method invocation will execute the compiled method; if a loop is compiled using OSR, then the next iteration of the loop will execute the compiled code. These queues are not strictly first in, first out: methods whose invocation counters are higher have priority. this priority ordering helps to ensure that the most important code will be compiled first. (This is another reason why the compilation ID in the PrintCompilation output can appear out of order.) When the client compiler is in use, the JVM starts one compilation thread; the server compiler has two such threads. When tiered compilation is in effect, the JVM will by default start multiple client and server threads based on a somewhat complex equation involving double logs of the number of CPUs on the target platform. The number of compiler threads (for all three compiler options) can be adjusted by setting the -XX:CICompilerCount=N flag (with a default value given in the previous table). Quick Summary   Compilation occurs asynchronously for methods that are placed on the compilation queue. The queue is not strictly ordered; hot methods are compiled before other methods in the queue. This is another reason why compilation IDs can appear out of order in the compilation log. Inlining One of the most important optimizations the compiler makes is to inline methods. Code that follows good object-oriented design often contains a number of attributes that are accessed via getters (and perhaps setters): public class Point { private int x, y; public void getX() { return x; } public void setX(int i) { x = i; } } The overhead for invoking a method call like this is quite high, especially relative to the amount of code in the method. In fact, in the early days of Java, performance tips often argued against this sort of encapsulation precisely because of the performance impact of all those method calls. Fortunately, JVMs now routinely perform code inlining for these kinds of methods. Hence, you can write this code: Point p = getPoint(); p.setX(p.getX() * 2); and the compiled code will essentially execute this: Point p = getPoint(); p.x = p.x * 2; Inlining is enabled by default. It can be disabled using the -XX:-Inline flag, though it is such an important performance boost that you would never actually do that (for example, disabling inlining reduces the performance of the stock batching test by over 50%).   The basic decision about whether to inline a method depends on how hot it is and its size. The JVM determines if a method is hot (i.e., called frequently) based on an internal calculation; it is not directly subject to any tunable parameters. If a method is eligible for inlining because it is called frequently, then it will be inlined only if its bytecode size is less than 325 bytes (or whatever is specified as the -XX:MaxFreqInlineSize=N flag). Otherwise, it is eligible for inlining only if it is small: less than 35 bytes (or whatever is specified as the -XX:MaxInlineSize=N flag). Sometimes you will see recommendations that the value of the MaxInlineSize flag be increased so that more methods are inlined. Inlining is the most beneficial optimization the compiler can make, particularly for object-oriented code where attributes are well encapsulated.     Tuning the inlining flags is rarely needed, and recommendations to do so often fail to account for the relationship between normal inlining and frequent inlining. Make sure to account for both cases when investigating the effects of inlining. Escape Analysis The server compiler performs some very aggressive optimizations if escape analysis is enabled (-XX:+DoEscapeAnalysis, which is true by default). Escape analysis is the most sophisticated of the optimizations the compiler can perform. This is the kind of optimization that fre¬quently causes microbenchmarks to go awry.   Escape analysis can often introduce “bugs” into improperly synchronized code.   Escape analysis is a technical that evaluate the scope of a Java object. In particular, if a java object allocated by some execting thread can ever be seen by a different thread, the object ‘escapes’. For example, consider this class to work with factorials:  public class Factorial { private BigInteger factorial; private int n; public Factorial(int n) { this.n = n; } public synchronized BigInteger getFactorial() { if (factorial == null) factorial = ...; return factorial; } } To store the first 100 factorial values in an array, this code would be used: ArrayList&lt;BigInteger&gt; list = new ArrayList&lt;BigInteger&gt;(); for (int i = 0; i &lt; 100; i++) { Factorial factorial = new Factorial(i); list.add(factorial.getFactorial()); }   The factorial object is referenced only inside that loop; no other code can ever access that object. Hence, the JVM is free to perform a number of optimizations on that object: ·   It needn’t get a synchronization lock when calling the getFactorial() method. ·   It needn’t store the field n in memory; it can keep that value in a register. Similarly it can store the factorial object reference in a register. ·   In fact, it needn’t allocate an actual factorial object at all; it can just keep track of the individual fields of the object. Deoptimization   There are two cases of deoptimization: when code is “made not entrant,” and when code is “made zombie.” This generates a deoptimization trap, and the previous optimizations are discarded. If a lot of additional calls are made with logging enabled, the JVM will quickly end up compiling that code and making new optimizations. The second thing that can cause code to be made not entrant is due to the way tiered compilation works. In tiered compilation, code is compiled by the client compiler, and then later compiled by the server compiler (and actually it’s a little more complicated than that,   Deoptimizing Zombie Code   When the compilation log reports that it has made zombie code, it is saying that it has reclaimed some previous code that was made not entrant. But there were still objects of the StockPriceHistoryImpl class around. Eventually all those objects were reclaimed by GC. When that happened, the compiler noticed that the methods of that class were now eligible to be marked as zombie code.   The heap (usually) accounts for the largest amount of memory used by the JVM, but the JVM also uses memory for its internal operations. This nonheap memory is native memory. Native memory can also be allocated in applications (via JNI calls to malloc() and similar methods, or when using New I/O, or NIO). The total of native and heap memory used by the JVM yields the total footprint of an application.     Deoptimization allows the compiler to back out previous versions of compiled code. Code is deoptimized when previous optimizations are no longer valid (e.g., because the type of the objects in question has changed).   There is usually a small, momentary effect in performance when code is deoptimized, but the new code usually warms up quick‐ ly again.   Under tiered compilation, code is deoptimized when it had previously been compiled by the client compiler and has now been optimized by the server compiler. Tiered Compilation Levels It turns out that there are five levels of execution, because the client compiler has three different levels. So the level of compilation runs from: ·   0: Interpreted code ·   1: Simple C1 compiled code ·   2: Limited C1 compiled code ·   3: Full C1 compiled code ·   4: C2 compiled code A typical compilation log shows that most methods are first compiled at level 3: full C1 compilation. (All methods start at level 0, of course.) If they run often enough, they will get compiled at level 4 (and the level 3 code will be made not entrant). This is the most frequent path: the client compiler waits to compile something until it has information about how the code is used that it can leverage to perform optimizations. If the server compiler queue is full, methods will be pulled from the server queue and compiled at level 2, which is the level at which the C1 compiler uses the invocation and back-edge counters (but doesn’t require profile feedback). That gets the method com‐ piled more quickly; the method will later be compiled at level 3 after the C1 compiler has gathered profile information, and finally compiled at level 4 when the server compiler queue is less busy.   And of course when code is deoptimized, it goes to level 0. Summary: Tiered compilation can operate at five distinct levels among the two compilers Changing the path between levels is not recommended; this section just helps to explain the output of the compilation log. This chapter has provided a lot of details about how just-in-time compilation works. From a tuning perspective, the simple choice here is to use the server compiler with tiered compilation for virtually everything; this will solve 90% of compiler-related performance issues. Just make sure that the code cache is sized large enough, and the compiler will provide pretty much all the performance that is possible. If you have some experience with Java performance, you may be surprised that compilation has been discussed for an entire chapter without mentioning the final keyword. In some circles, the final keyword is thought to be an important factor in performance because it is believed to allow the JIT compiler to make better choices about inlining and other optimizations. Still, it is a persistent rumor. For the record, then, you should use the final keyword whenever it makes sense: for an immutable object or primitive value you don’t want to change, for parameters to certain inner classes, and so on. But the presence or absence of the final keyword will not affect the performance of an application. Don’t be afraid of small methods—and in particular getters and setters—because they are easily inlined. If you have a feeling that the method overhead can be ex‐ pensive, you’re correct in theory (we showed that removing inlining has a huge impact on performance). But it’s not the case in practice, since the compiler fixes that problem.     Code that needs to be compiled sits in a compilation queue. The more code in the queue, the longer the program will take to achieve optimal performance.   Although you can (and should) size the code cache, it is still a finite resource.   The simpler the code, the more optimizations that can be performed on it. Profile feedback and escape analysis can yield much faster code, but complex loop struc‐ tures and large methods limit their effectiveness.   That concept is the essential difference between committed (or allocated) memory and reserved memory (sometimes called the virtual size of a process). The JVM must tell the operating system that it might need as much as 2 GB of memory for the heap, so that memory is reserved: the operating system promises that when the JVM attempts to allocate additional memory when it increases the size of the heap, that memory will be available. Still, only 512 MB of that memory is actually allocated initially, and that 512 MB is all of the memory that actually is being used (for the heap). That (actually allocated) mem‐ ory is known as the committed memory. The amount of committed memory will fluc‐ tuate as the heap resizes; in particular, as the heap size increases, the committed memory correspondingly increases. When we look at performance, only committed memory really matters: there is never a performance problem from reserving too much memory. However, sometimes you want to make sure that the JVM does not reserve too much memory. This is particularly true for 32-bit JVMs. Since the maximum process size of a 32-bit application is 4 GB (or less, depending on the operating system), over-reserving memory can be an issue. A JVM that reserves 3.5 GB of memory for the heap is left with only 0.5 GB of native memory for its stacks, code cache, and so on. It doesn’t matter if the heap only expands to commit 1 GB of memory: because of the 3.5 GB reservation, the amount of memory for other operations is limited to 0.5 GB. 64-bit JVMs aren’t limited that way by the process size, but they are limited by the total amount of virtual memory on the machine. Say that you have a small server with 4 GB of physical memory and 10 GB of virtual memory and start a JVM with a maximum   One exception to this is thread stacks. Every time the JVM creates a thread, the OS allocates some native memory to hold that thread’s stack, committing more memory to the process (until the thread exits, at least). Thread stacks, though, are fully allocated when they are created.   Code cache The code cache uses native memory to hold compiled code. As discussed in Chap‐ ter 4, this can be tuned (though performance will suffer if all the code cannot be compiled due to space limitations). Developers can allocate native memory via JNI calls, but NIO byte buffers will also allocate native memory if they are created via the allocateDirect() method. Native byte buffers are quite important from a performance perspective, since they allow native code and Java code to share data without copying it. The most common example here is buffers that are used for filesystem and socket operations. Writing data to a native NIO buffer and then sending that data to the channel or socket) requires no copying of data between the JVM and the C library used to transmit the data. If a heap byte buffer is used instead, contents of the buffer must be copied by the JVM.   The allocateDirect() method call is quite expensive; direct byte buffers should be reused as much as possible. The ideal situation is when threads are independent and each can keep a direct byte buffer as a thread-local variable. That can sometimes use too much native memory if there are many threads that need buffers of variable sizes, since eventually each thread will end up with a buffer at the maximum possible size. For that kind of situation—or when thread-local buffers don’t fit the application design— an object pool of direct byte buffers may be more useful.      From a tuning perspective, the footprint of the JVM can be limi¬ted in the amount of native memory it uses for direct byte buf¬fers, thread stack sizes, and the code cache (as well as the heap).   Class loader          A class loader in Java is simply an object whose type extends the ClassLoader class. When the virtual machine needs access to a particular class, it asks the appropriate class loader.            Class loaders are organized into a tree hierarchy. At the root of this tree is the system class loader. This class loader is also called the primordial class loader or the null class loader. It is used only to load classes from the core Java API.            The system class loader has one or more children. It has at least one child; the URL class loader that is used to load classes from the classpath. It may have other direct children, though typically any other class loaders are children of the URL class loader that reads the classpath.            The hierarchy comes into play when it is time to load a class. Classes are loaded in one of three ways: either explicitly by calling the loadClass( ) method of a class loader, explicitly by calling the Class.forName( ) method, or implicitly when they are referenced by an already−loaded class. In any case, a class loader is asked to load the class. In the first case, the class loader is the object on which the loadClass( ) method is invoked. In the case of the forName( ) method, the class loader is either passed to that method, or it is the class loader that loaded the class that is calling the forName( ) method. The implicit case is similar: the class loader that was used to load the referencing class is also used to load the referenced class. Class loaders are responsible for asking their parent to load a class; only if that operation fails will the class loader attempt to define the class itself.            The net effect of this is that system classes will always be loaded from the system class loader, classes on the class path will always be loaded by the class loader that knows about the classpath, and in general, a class will be loaded by the oldest class loader in the ancestor hierarchy that knows where to find a class.            When you create a class loader, you can insert it anywhere into the hierarchy of class loaders (except at the root). Typically, when a class loader is created, its parent is the class loader of the class that is instantiating the new class loader.            Implementing a Class Loader            Now we’ll look at how to implement a class loader. The class loader we implement will be able to extend the normal permissions that are granted via policy files, and it will enforce certain optional security features of the class loader.            The basic class that defines a class loader is the ClassLoader class (java.lang.ClassLoader): public abstract class ClassLoader Turn a series of Java bytecodes into a class definition. This class does not define how the bytecodes are obtained but provides all other functionality needed to create the class definition.            However, the preferred class to use as the basis of a class loader is the SecureClassLoader class (java.security.SecureClassLoader): public class SecureClassLoader extends ClassLoader Turn a series of Java bytecodes into a class definition. This class adds secure functionality to the ClassLoader class, but it still does not define how bytecodes are obtained. Although this class is not abstract, you must subclass it in order to use it. The secure class loader provides additional functionality in dealing with code sources and protection domains. You should always use this class as the basis of any class loader you work with; in fact, the ClassLoader class would be private were it not for historical reasons.       public class URLClassLoader extends SecureClassLoader Load classes securely by obtaining the bytecodes from a set of given URLs.   Key Methods of the Class Loader           The ClassLoader class and its subclasses have three key methods that you work with when creating your own class loader. 6.3.2.1 The loadClass( ) method The loadClass( ) method is the only public entry into the class loader: public Class loadClass(String name)            Load the named class. A ClassNotFoundException is thrown if the class cannot be found. This is the simplest way to use a class loader directly: it requires that the class loader be instantiated and then be used via the loadClass( ) method. Once the Class object has been constructed, there are three ways in which a method in the class can be executed: The correct implementation of the loadClass( ) method is crucial to the security of the virtual machine. For instance, one operation this method performs is to call the parent class loader to see if it has already defined a particular class; this allows all the core Java classes to be loaded by the primordial class loader. If that operation is not performed correctly, security could suffer. As a developer you should be careful when you override this method; as an administrator, this is one of the reasons to prevent untrusted code from creating a class loader.       6.3.2.2 The findClass( ) method           The loadClass( ) method performs a lot of setup and bookkeeping related to defining a class, but from a developer perspective, the bulk of the work in creating a Class class object is performed by the findClass( ) method: protected Class findClass(String name)            The findClass( ) method uses whatever mechanism it deems appropriate to load the class (e.g., by reading a class file from the file system or from an HTTP server). It is then responsible for creating the protection domain associated with the class and using the next method to create the Class class object.            The defineClass( ) methods These methods all take an array of Java bytecodes and some information that specifies the permissions associated with the class represented by those bytecodes. They all return the Class class object: protected final Class defineClass(String name, byte[] b, int off, int len)       Responsibilities of the Class Loader  When you implement a class loader, you override some or all of the methods we’ve just listed. In sum, the class loader must perform the following steps: The security manager is consulted to see if this program is allowed to access the class in question. If it is not, a security exception is thrown. This step is optional; it should be implemented at the beginning of the loadClass( ) method. This           corresponds to the use of the accessClassInPackage permission. If the class loader has already loaded this class, it finds the previously defined class object and returns that object. This step is built into the loadClass( ) method.            corresponds to the use of the accessClassInPackage permission. If the class loader has already loaded this class, it finds the previously defined class object and returns that object. This step is built into the loadClass( ) method.            Otherwise, the class loader consults its parent to see if the parent knows how to load the class. This is a recursive operation, so the system class loader       will always be asked first to load a class. This prevents programs from providing alternate definitions of classes in the core API (but a clever class loader can defeat that protection). This step is built into the loadClass( ) method. The security manager is consulted to see if this program is allowed to create the class in question. If it is not, a security exception is thrown. This step is optional; if implemented, it should appear at the beginning of the findClass( ) method. Note that this step should take place after the parent class loader is queried rather than at the beginning of the operation (as is done with the access check). No Sun−supplied class loader implements this step; it corresponds to the defineClassInPackage permission.   The class file is read into an array of bytes. The mechanism by which the class loader reads the file and creates the byte array will vary depending on the class loader (which, after all, is one of the points of having different class loaders). This occurs in the findClass( ) method. The appropriate protection domain is created for the class. This can come from the default security model (i.e., from the policy files), and it   can be augmented (or even replaced) by the class loader. Alternately, you can create a code source object and defer definition of the protection domain. This occurs in the findClass( ) method.   Within the findClass( ) method, a Class object is constructed from the bytecodes by calling the defineClass( ) method. If you used a code source in step 6, the getPermissions( ) method will be called to find the permissions associated with the code source. The defineClass( ) method also ensures that the bytecodes are run through the bytecode verifier.   Before the class can be used, it must be resolved −− which is to say that any classes that it immediately references must also be found by this class loader. The set of classes that are immediately referenced contains any classes that the class extends as well as any classes used by the static initializers of the class. Note that classes that are used only as instance variables, method parameters, or local variables are not normally loaded in this phase: they are loaded when the class actually references them (although certain compiler optimizations may require that these classes be loaded when the class is resolved). This step happens in the loadClass( ) method.   If you want to use a custom class loader, the easiest route is to use the URL class loader. This limits the number of methods that you have to override. To construct an instance of this class, use one of the following constructors: public URLClassLoader(URL urls[])   URL urls[] = new URL[2]; urls[0] = new URL(\"http://piccolo.East/~sdo/\"); urls[1] = new URL(\"file:/home/classes/LocalClasses.jar\"); ClassLoader parent = this.getClass().getClassLoader( ); URLClassLoader ucl = new URLClassLoader(urls, parent);   public final synchronized Class loadClass(String name, boolean resolve) throws ClassNotFoundException { // First check if we have permission to access the package. SecurityManager sm = System.getSecurityManager( ); if (sm != null) { int i = name.lastIndexOf('.'); if (i != −1) { sm.checkPackageAccess(name.substring(0, i)); } } return super.loadClass(name, resolve); }   6.3.4.2 Step 2: Use the previously−defined class, if available The loadClass( ) method of the ClassLoader class performs this operation for you, which is why we’ve called the super.loadClass( ) method.   6.3.4.3 Step 3: Defer class loading to the parent The loadClass( ) method of the ClassLoader class performs this operation. 6.3.4.4 Step 4: Optionally call the checkPackageDefinition( ) method In order to call the checkPackageDefinition( ) method, you must override the findClass( ) method:   protected Class findClass(final String name) throws ClassNotFoundException { // First check if we have permission to access the package. SecurityManager sm = System.getSecurityManager( ); if (sm != null) { int i = name.lastIndexOf('.'); if (i != −1) { sm.checkPackageDefinition(name.substring(0, i)); } } return super.findClass(name); }   6.3.4.5 Step 5: Read in the class bytes The URL class loader performs this operation for you by consulting the URLs that were passed to its constructor. If you need to adjust the way in which the class bytes are read, you should use the SecureClassLoader class instead. 6.3.4.6 Step 6: Create the appropriate protection domain The URL class loader will create a code source for each class based on the URL from which the class was loaded and the signers (if any) of the class. The permissions associated with this code source will be obtained by using the getPermissions( ) method of the Policy class, which by default will return the permissions read in from the active policy files. In addition, the URL class loader will add additional permissions to that set: If the URL has a file protocol, it must specify a file permission that allows all files that descend from the URL path to be read. For example, if the URL is file:///xyz/classes/, then a file permission with a name of /xyz/classes/− and an action list of read will be added to the set of permiss ions. If the URL is a jar file (file:///xyz/MyApp.jar), the name file permission will be the URL itself. If you want to associate different permissions with the class, then you should override the getPermissions( ) method. For example, if we wanted the above rules to apply and also allow the class to exit the virtual machine, we’d use this code:  protected PermissionCollection getPermissions(CodeSource codesource) { PermissionCollection pc = super.getPermissions(codesource); pc.add(new RuntimePermission(\"exitVM\")); return pc; }  We could completely change the permissions associated with the class (bypassing the Policy class altogether) by constructing a new permission collection in this method rather than calling super.getPermissions( ). The URL class loader will use whatever permissions are returned from this getPermissions( ) method to define the protection domain that will be associated with the class. If you need to load bytes from a source that is not a URL (or from a URL for which you don’t have a protocol handler, like FTP), then you’ll need to extend the SecureClassLoader class. A subclass is required because the constructors of this class are protected, and in any case you need to override the findClass( )   The steps to use this class are exactly like the steps for the URLClassLoader class, except for step 5. To implement step 5, you must override the findClass( ) method like this:   protected Class findClass(final String name) throws ClassNotFoundException { // First check if we have permission to access the package. // You could remove these 7 lines to skip the optional step 4. SecurityManager sm = System.getSecurityManager( ); if (sm != null) { int i = name.lastIndexOf('.'); if (i != −1) { sm.checkPackageDefinition(name.substring(0, i)); } } // Now read in the bytes and define the class try { return (Class) AccessController.doPrivileged( new PrivilegedExceptionAction( ) { public Object run( ) throws ClassNotFoundException { byte[] buf = null; try { // Acutally load the class bytes buf = readClassBytes(name); } catch (Exception e) { throw new ClassNotFoundException(name, e); } // Create an appropriate code source CodeSource cs = getCodeSource(name); // Define the class return defineClass(name, buf, 0, buf.length, cs); } } ); } catch (java.security.PrivilegedActionException pae) { throw (ClassNotFoundException) pae.getException( ); }   The syntax of this method is complicated by the fact that we need to load the class bytes in a privileged block. Depending on your circumstances, that isn’t strictly necessary, but it’s by far the most common case for class loaders. Say that your class loader loads class A from the database; that class is given minimal permissions. When that class references class B, the class loader will be asked to load class B and class A will be on the stack. When it’s time to load the new class bytes, we need to load them with the permissions of the class loader rather than the entire stack, which is why we use a privileged block. Notwithstanding, the try block has three operations: it loads the class bytes, it defines a code source for that class, and it calls the defineClass( ) method to create the class. The first two of the opera tions are encapsulated in the readClassBytes( ) and getCodeSource( ) methods; these are methods that you must implement. Loading the class bytes is an operation left to the reader. The reason for providing your own class loader is that you want to read the class bytes in some special way; otherwise, you’d use the URLClassLoader class. The code source is another matter: we must determine a URL and a set of certificates that should be associated with the class. In a signed jar file, the certificates are read from the jar file and the URL is the location of the jar file. In Chapter 12, we’ll show how to get the certificates from a standard jar file and construct the appropriate URLClassLoader class. The code source is another matter: we must determine a URL and a set of certificates that should be associated with the class. In a signed jar file, the certificates are read from the jar file and the URL is the location of the jar file. In Chapter 12, we’ll show how to get the certificates from a standard jar file and construct the appropriate The defineClass( ) method will call back to the getPermissions( ) method in order to complete the definition of the protection domain for this class. And that’s why the URL used to construct the code source can be arbitrary: when you write the getPermissions( ) method, just make sure that you understand what the URL actually is. In default usage, the URL would be used to find entries in the policy files, but since you’re defining your own permissions anyway, the contents of the URL don’t matter. What matters is that you follow a consistent convention between the definition of your getCodeSource( ) and findClass( ) methods. Hence, possible implementations of the getPermissions( ) and getCodeSource( ) methods are as follows:  protected CodeSource getCodeSource(String name) { try { return new CodeSource(new URL(\"file\", \"localhost\", name), null); } catch (MalformedURLException mue) { mue.printStackTrace( ); } return null; } protected PermissionCollection getPermissions(CodeSource codesource) { PermissionCollection pc = new Permissions( ); pc.add(new RuntimePermission(\"exitVM\")); return pc; }  If you’re reading the class bytes from, say, a database, it would be more useful if you could pass an arbitrary string to construct the code source. That doesn’t work directly since the code source requires a URL but the file part of the URL can be any arbitrary string. In this case, we just use the class name. Note that the getPermissions( ) method of the SecureClassLoader class does not add the additional permissions that the same method of the URLClassLoader class adds. As a result, we do not call the super.getPermissions( )   Delegation  As we’ve mentioned, class loading follows a delegation model. This model permits a class loader to be instantiated with this constructor: protected ClassLoader(ClassLoader parent) Create a class loader that is associated with the given class loader. This class loader delegates all operations to the parent first: if the parent is able to fulfill the operation, this class loader takes no action. For example, when the class loader is asked to load a class via the loadClass( ) method, it first calls the loadClass( ) method of the parent. If that succeeds, the class returned by the delegate will ultimately be returned by this class. If that fails, the class loader then uses its original logic to complete its task, something like this:  public Class loadClass(String name) { Class cl; cl = delegate.loadClass(name); if (cl != null) return cl; // else continue with the loadClass( ) logic }  You may retrieve the delegate associated with a class loader with the following method  public final ClassLoader getParent( ) Return the class loader to which operations are being delegated. The class loader that exists at the root of the class loader hierarchy is retrieved via this method: Return the system class loader (the class loader that was used to load the base application classes). If a security manager is in place, you must have the getClassLoader runtime permission to use this method.   Loading Resources  A class loader can load not only classes, but any arbitrary resource: an audio file, an image file, or anything else. Instead of calling the loadClass( ) method, a resource is obtained by invoking one of these methods: public URL getResource(String name) public InputStream getResourceAsStream(String name) The getResource( ) method calls the getSystemResource( ) method; if it does not find a system resource, it returns the object retrieved by a call to the findResource( ) method (which by default will be null). The getResourceAsStream( ) method simply   Loading Libraries  Loading classes with native methods creates a call to this method of the ClassLoader class: protected String findLibrary(String libname) Return the directory from which native libraries should be loaded. This method is used by the System.loadLibrary( ) method to determine the directory in which the native library in question should be found. If this method returns null (the default), the native library must be in one of the di   谈到常量池，在Java体系中，共用三种常量池。分别是字符串常量池、Class常量池和运行时常量池。  ","categories": [],
        "tags": ["Java","class loader`"],
        "url": "/2016/02/25/Java-Class-Loader.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Microservices vs. SOA",
        "excerpt":"Microservice     Services are organized around capabilities, e.g., user interface front-end, recommendation, logistics, billing, etc.   Services are small in size, messaging enabled, bounded by contexts, autonomously developed, independently deployable, decentralized and built and released with automated processes.   resource-oriented computing (ROC) as a generalized form of the Web abstraction. If in the Unix abstraction “everything is a file”, in ROC, everything is a “Micro-Web-Service”   Philosophy   The philosophy of the microservices architecture essentially equates to the Unix philosophy of “Do one thing and do it well”. It is described as follows:      The services are small - fine-grained to perform a single function.   The organization culture must embrace automation of testing and deployment. This eases the burden on management and operations and allows for different development teams to work on independently deployable units of code.   The culture and design principles must embrace failure and faults, similar to anti-fragile systems.   Each service is elastic, resilient, composable, minimal, and complete.   service mesh  In a service mesh, each service instance is paired with an instance of a reverse proxy server, called a service proxy, sidecar proxy, or sidecar. The service instance and sidecar proxy share a container, and the containers are managed by a container orchestration tool such as Kubernetes.   Differences between Microservices and SOA   Definitaion of SOA      Boundaries are explicit   Services are autonomous   Services share __ schema __ and contract, not class   Service compatibility is based on policy        In short, the microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and independently deployable by fully automated deployment machinery. There is a bare mininum of centralized management of these services, which may be written in different programming languages and use different data storage technologies.    Related links:      InfoQ Discussions   StackOverFlow   Microservices tag desc   MartinFowler  ","categories": [],
        "tags": ["Microservices","SOA"],
        "url": "/2016/05/31/Microservices-Vs-SOA.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Database sharding",
        "excerpt":"DB sharding in YHD   There are two solutions when DB becoming bottleneck in yihaodian.      Scale up Upgrade Oracle DB, adding more CPU , Disk and memory to incrase I/O performance. This is for short term only, high cost.   Scale out Divide the order table to multiple DBs, which is support horizontal extension, for long term purpose.   Orgional Oracle is replaced by multiple MySQL DB, supporintg one master and multiple slaves, supporitng segratation of read and write. Leveraging MySQL built-in Master-slave replication (SLA&lt;1 second)   sharding dimensions     DB Field chosing, it should chose the filed that lead to least SQL and code change, to make the access fall in one database, instead of multiple DBs, which result in high I/O and significant logic change.   Here is one practice – Get all SQL – Pick up top fields appear in where clause. – List break down from three categories            Single ID, i.e. userID=?       Multiple ID. i.e. userID in (?,?,?)       Not show           |Field| Single ID | Multiple ID | Not show| |:—| —:| —:| —:| |userID | 120 | 40| 330| |orderID | 60 | 80| 360| |shopID | 15 | 0| 485| It’s obviously we should chose userID for sharding. Hold on, this is just static analysis, we should conduct *dynamic** study as well, so list most executed SQLs, e.g. top 15 SQL (account to 85% of SQL calls), if we conduct sharding by user ID, 85% of those SQL will fall in single DB and 13% fall in multiple DB, and only 2% will scan all DB, so the performance is must better than sharding on other ID fields.   sharding strategy   There are two type of strategies     By value range, e.g. user ID 1-9999 to DB1, and 10000-20000 to DB2. For this option,   By value mod, e.g. userID mod n, when reminder is 0, go to DB1, reminder is 1, to DB2, etc.   Pros and Cons:                  Criteria       By Range       By Mod                       number of DBs       initially only require small amount of DBs, can increasse by business requests       initially number based on mod number, normally a big number                 Accessibility       initially only few DBs, perforamce cost is small, single DB performance query is poor       initially big number of DBs, query acorss DBs may consume many resources, better for query on single DB                 DBs adjustment       easy, just add new DB, and impact is limit when split existing DB       not easy, change mod value  may result in DB migration across DBs                 Data hotspot       there are data hotspot issues       no data hotspot issues           In practice, for the sake of simplicity, mod sharding is often used. To manage further sharding, and for smooth data migration, normally new DBs are added by folds, e.g. intially 4 DBs, furhter split will be 8 DBs, then 16 DBs. This is becuase only half of data in existing DB will be migrated to new DB, while the rest half will be remain unchanged. However, there are some super IDs, e.g. one big shop with massive records than normal, if we shard DB by user ID, there will one DB will many records than others. For this case, we need to provide separate DB for those super IDs.   sharding numbers  Firslty, that’s depends on the ability of single DB, e.g. normally one MySQL DB can support upto 50mio records, and Oracle can support 100mio. Normally multiple DBs may leads to certain perforamnce issues, when data query across multiple DBs, if there are multithreading call, it will cost precious thread resource, while it’s single thread, the wating time will be unacceptable. Normally, the initial sharding is 4-8 DBs.   Router transparency  To certain extent, DB sharding means change of DBSChema, which inevitable result in application, however, this is irrelavent to business logic, so the DB sharding should be transparent to business logic code, therefore, DB sharding should be handled at DAL (Data Access Layer) or DDAL (Distributed Data Access Layer).      For access to single DB, e.g. query by certain user id, DAL will automatically route to that DB, even further split by mod, still no applicaiton logic code change impacted.   For simple across DB query, DAL in charge to aggregate results from every DB query, still transparent to upper application logic.   For query involves multiple DBs with aggretation functions, e.g. groupBy, order by, min, max, avg. It’s recommended DAL consolidate request from single DB, while upper layers do further processing. That’s becuase if rely on DAL, it would be too complex, and such case is relatively rare case, so leave it to upper layer.   Oracle Sharding  It’s required in Web 2.0 and high availability technologies   Shardingis an application-managed scaling technique using many (hundreds /thousands of) independent databases     Data is split into multiple databases (shards)   Each database holds a subset (either range or hash) of the data   Split the shards as data volume or access grows   Shards are replicated for availability and scalability   Sharding is the dominant approach for scaling massive websites      Application code dispatches request to a specific database based on key value   Queries are constrained -simple queries on shard-key   Data isdenormalizedto avoid cross-shard operations (no joins)   Each database holds all the data   Request dispatched to a specific database based on read/write,key value   Updates go to one database, changes are replicated to the other databases. The other databases are available for reads   Provides read scalability   Can be combined with horizontal sharding so that each shard is replicated to a different degree   Main benefit is that you do not need to reshard   Downsides of DB replica      Only async log shipping which can lose data in case of failure   Slaves can return inconsistent data   Statement based replication has correctness issues &amp; row-based replication is immature   Replication is slow (high overhead on each reader, slaves are single-threaded)   No support for failover between master (primary) &amp; slaves (backup)   Does not handle failure conditions such as missing or damaged logs   Storage engine and replication state may become inconsistent after a crash   Bringing a failed master back requires copying the database   –End–  ","categories": [],
        "tags": ["DB","Sharding","MobileInternet"],
        "url": "/2016/06/01/DB-Sharding.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Setup Git in Mint Linux",
        "excerpt":"How to setup Git in Mint Linux  =================================================      git config –global user.name “Todd Zhang”   git config –global user.email phray.zhang@gmail.com   git config –list   git clone https://github.com/todzhanglei/todzhanglei.github.io   git config –global credential.helper cache   git config –global credential.helper ‘cache –timeout=36000’   To add remote  git remote add origin https://github.com/CloudsDocker/cloudsdocker.github.io.git  Above command will add the remote URL with alias “origin”   To pull specific branch  git pull origin blogSrc   Errors   Error: The following untracked working tree files would be overwritten by checkout   git clean  -d  -fx \"\"   ","categories": [],
        "tags": ["Git","Mint"],
        "url": "/2016/06/01/Setup-Git-In-Mint.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "XA Transactions in 2PC",
        "excerpt":"Description      2 phase commit protocol referred to as XA(eXtended Architecture)   This protocol provides ACID-like properties for global transaction processing   2 phase commit protocol is an atomic commitment protocol for distributed systems.   The first one is commit-request phase in which transaction manager coordinates all of the transaction resources to commit or abort.   In the commit-phase, transaction manager decides to finalize operation by committing or aborting according to the votes of the each transaction resource.   XA transactions need a global transaction id and local transaction id(xid) for each XA resource.   Each XA Resource is enlisted to XA Manager by start(xid) method.   This method tells that XA Resource is being involved in the transaction(be ready for operations).   the first phase of the 2PC protocol is realized by calling prepare(xid) method. This method requests OK or ABORT vote from XA Resource.   After receiving vote from each of XA Resource, XA Manager decides to execute a commit(xid) operation if all XA Resources send OK or decides to execute a rollback(xid) if an XA Resource sends ABORT.   Finally, the end(xid) method is called for each of XA Resource telling that the transaction is completed.   Reference links:      (DZone XA 2PC)[https://dzone.com/articles/xa-transactions-2-phase-commit]  ","categories": [],
        "tags": ["XA","MobileInternet"],
        "url": "/2016/06/01/XA-2PC.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "CI and CD",
        "excerpt":"Concepts   CI/CD are usage of Docker, CI is continuous integration, is the process of eliciting fast, automated feedback on the correctness of your application every time there is a change to the code, while CD means continuous delivery, build upon the earlier concept by providing fast, automated feedback on the correctness and production readiness of your application every time there is a chance to code, infrastructure, or configuration.   Some “best practices”      Frequent commits to a common code stream   Disallow commits into a “broken” build   A “broken” build on CI should be attended to immediately and its resolutin should be of utmost priority   Use cases  ","categories": [],
        "tags": ["DevOps","MobileInternet"],
        "url": "/2016/06/03/CI-CD.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Storage Management",
        "excerpt":"RAID  RAID is Reductant Array Independent Disk,   JBOD  JBOD is abbreviated from “Just a bunch of disks”, is an architecutre using multiple hard drives, but not in a RAID configuration, thus providing neither redundancy nor performance improvement.Hard drives may be handled independently as separate logical volumnes, or they may be combined into a single logical volume using a volume manager like LVM, such volumes are usually called “spanned”   LVM  ‘LVM’ means Logical Volume Manager, is part of Linux kenel, is a device mapper. LVM is used to manage large hard disk farms by allowing disks to be added and replaced without downtime or service distruption.  ","categories": [],
        "tags": ["CTO","MobileInternet"],
        "url": "/2016/06/03/Storage-Management.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Python",
        "excerpt":"(‘—–Unexpected error:’, &lt;type ‘exceptions.TypeError’&gt;) datetime.datetime.now()   Python items   Test item,   MapReduce的设计灵感来自于函数式编程，这里不打算提MapReduce，就拿python中的map()函数来学习一下   Single Qutoe vs Double Quote  There is no difference between using single quotes and double quotes in Python   Generators presentation:  http://www.dabeaz.com/generators/Generators.pdf   wwwlog=open('weblog.debug') bytecolumn=(line.rsplit(None,1)[1] for line in wwwlog) bytes=(int(x) for x in bytecolumn if x!='-') print \"total:\", sum(bytes)   Generator as a pipeline   At each step, we declare an operation that will be applied to the entire input stream, like rsplit to all lines of the input log file. Rather than take a huge memory to process a huge file.   The key is ‘think big’. Instead of focusing on the problem at a line-by-line level, you just break it down into big operations that operate on the whole file.   Iteration is the glue   rsplit  API doc Returns a list of the words in the string, separated by the delimiter string (starting from right).  &gt;&gt;&gt; ' a b c '.rsplit(None, 1) [' a b', 'c'] &gt;&gt;&gt; ' a b c '.rsplit(None, 2) [' a', 'b', 'c']  ","categories": [],
        "tags": ["Coding","Python"],
        "url": "/2016/06/05/Python.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Load Balancing",
        "excerpt":"Concepts  LVS means Linux Virtual Server, which is one Linux built-in component.   Some logics of LVS   Never Queue Shceduling     The never queue scheduling algorithm adpots a two-speed model       When there is an idel server avaiable, the job will be sent to the idel server, instead of waiting for a fast one.   When there is no idel server avaiable, the job will be sent to the server that minimize it’s expected delay.   Examples   Setup LVS  ipvsadm -A -t 192.168.0.1:80 -s rr ipvsadm -a -t 192.168.0.1:80 -r 172.16.0.1:80 -m ipvsadm -a -t 192.168.0.1:80 -r 172.16.0.2:80 -m     The first command assign TCP port 80 on IP address 192.168.0.1 to the virtual server, the shceduling algorithm for load balancing is -s rr means using round-robin   The 2nd and 3rd commands are adding IP addresss of real servers to the LVS setup   the forwarded network packets shall be masked -m   To query LVS status  ipvsadm -L -n IP Virtual Server version 1.0.8 (size=65536) Prot LocalAddress:Port Scheduler Flags   -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn TCP  192.168.0.1:80 rr   -&gt; 172.16.0.2:80                Masq    1      3          1   -&gt; 172.16.0.1:80                Masq    1      4          0   Strucutre of LVS in wikipedia    ","categories": [],
        "tags": ["CTO","MobileInternet","DevOps"],
        "url": "/2016/06/07/Load-Balancing.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Docker Errors and Fixes",
        "excerpt":"Docker Errors      Cannot connect to the Docker daemon. Is the docker daemon running on this host? The solution is to run under root user, e.g.   sudo docker run hello-world      Docker service    sudo service docker status  sudo service docker start  sudo docker run hello-world   ","categories": [],
        "tags": ["DevOps","Docker"],
        "url": "/2016/06/09/Docker-Errors-Fixes.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Linux Tips",
        "excerpt":"Get permission denied error when sudo su (or hyphen in sudo command)  bash: /home/YOURNAME/.bashrc: Permission denied That’s because you didn’t add “-“ hyphen in your sudo command.   The difference between “-“ and “no hyphen” is that the latter keeps your existing environment (variables, etc); the former creates a new environment (with the settings of the actual user, not your own).   The hyphen has two effects:   1) switches from the current directory to the home directory of the new user (e.g., to /root in the case of the root user) by logging in as that user   2) changes the environmental variables to those of the new user as dictated by their ~/.bashrc. That is, if the first argument to su is a hyphen, the current directory and environment will be changed to what would be expected if the new user had actually logged on to a new session (rather than just taking over an existing session).   To delete lines in files contain pattern  sed -i '/.*167\\=OPT.*/d' testdata.txt   to select only only one element value of XML file :  grep -oPm1 “(?&lt;=)[^&lt;]+\"   To check Linux release name  cat /etc/os-release   How to check whether your linux is 32bit or 64 bit  To run “arch” command,  this is similar to “uname -m” , it prints to the screen whether your system is running 32-bit (“i686”) or 64-bit (“x86_64”).   convert line ending to unix (sometimes git submit is dos format)  dos2unix the_script_file_name   To check redhat Linux version  cat  /etc/redhat-release   To list all users in linux  cat /etc/passwd   Show IP address in Linux  $ifconfig eth0      Link encap:Ethernet  HWaddr 00:50:56:9B:19:81           inet addr:133.14.16.5  Bcast:133.14.16.255  Mask:255.255.255.0   Check system resource  execute `cat /proc/cpuinfo` and `free -m` to gain information about the server’s CPU and memory.   chmod command  From one to four octal digits Any omitted digits are assumed to be leading zeros.   The first digit = selects attributes for the set user ID (4) and set group ID (2) and save text image (1)S The second digit = permissions for the user who owns the file: read (4), write (2), and execute (1) The third digit = permissions for other users in the file’s group: read (4), write (2), and execute (1) The fourth digit = permissions for other users NOT in the file’s group: read (4), write (2), and execute (1)   The octal (0-7) value is calculated by adding up the values for each digit User (rwx) = 4+2+1 = 7 Group(rx) = 4+1 = 5 World (rx) = 4+1 = 5 chmode mode = 0755   Examples  chmod 400 file - Read by owner chmod 040 file - Read by group chmod 004 file - Read by world   chmod 200 file - Write by owner chmod 020 file - Write by group chmod 002 file - Write by world   top     enter u, then user id to show only user process   Z: global color scheme, e.g. red or green   B: global bold for all   z: show color, then b to hightlight, and x highlight sort fidl, y highlight running tasks   #3: show only 3 threads   c: show command line   F: sort, e.g. Fk sort by CPU%   R: reverse order   Sample config files   .vimrc    set number set incsearch set hlsearch syntax on colorscheme desert   ==== screenrc  =  https://gist.githubusercontent.com/ChrisWills/1337178/raw/8275b66c3ea86a562cdaa16f1cc6d9931d521e1b/.screenrc-main-example  # GNU Screen - main configuration file  # All other .screenrc files will source this file to inherit settings. # Author: Christian Wills - cwills.sys@gmail.com  # Allow bold colors - necessary for some reason attrcolor b \".I\"  # Tell screen how to set colors. AB = background, AF=foreground termcapinfo xterm 'Co#256:AB=\\E[48;5;%dm:AF=\\E[38;5;%dm'  # Enables use of shift-PgUp and shift-PgDn termcapinfo xterm|xterms|xs|rxvt ti@:te@  # Erase background with current bg color defbce \"on\"  # Enable 256 color term term xterm-256color  # Cache 30000 lines for scroll back defscrollback 30000  # New mail notification # backtick 101 30 15 $HOME/bin/mailstatus.sh  hardstatus alwayslastline  # Very nice tabbed colored hardstatus line hardstatus string '%{= Kd} %{= Kd}%-w%{= Kr}[%{= KW}%n %t%{= Kr}]%{= Kd}%+w %-= %{KG} %H%{KW}|%{KY}%101`%{KW}|%D %M %d %Y%{= Kc} %C%A%{-}'  # change command character from ctrl-a to ctrl-b (emacs users may want this) #escape ^Bb  # Hide hardstatus: ctrl-a f  bind f eval \"hardstatus ignore\" # Show hardstatus: ctrl-a F bind F eval \"hardstatus alwayslastline\"   ====================.bashrc ==========  # .bashrc  # Source global definitions if [ -f /etc/bashrc ]; then         . /etc/bashrc fi # source ./prompt export PS1=''  export PS1='[\\e[104mLight blue \\u \\A\\]$ '  export PS1=\"\\[\\e[32m\\]\\u@\\h \\d \\t \\w \\[\\e[m\\] \\\\$\"  \\e[104mLight blue  # Welcome message echo -ne \"Good Morning ! It's \"; date '+%A, %B %-d %Y' echo -e \"And now your moment of Zen:\"; fortune echo echo \"Hardware Information:\" sensors  # Needs: 'sudo apt-get install lm-sensors' uptime   # Needs: 'sudo apt-get install lsscsi' free -m  # User specific aliases and functions  PS1='\\[`[ $? = 0 ] &amp;&amp; X=2 || X=1; tput setaf $X`\\]\\h\\[`tput sgr0`\\]:$PWD\\n\\$ '  ============vimrc ========= set number set incsearch set hlsearch  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  grep -v \"unwanted_word\" file | grep XXXXXXXX  // find command exclude “permission denied” $ find . -name \"java\" 2&gt;/dev/null   Passwordless connection in putty  1. Generate Public &amp; private key pair by keygen 2. Log into Linux, nano .ssh/authorized_keys and paste the public key 3. Save the private key in putty and load it in Putty session   find java files older than 3 days  find . -name \"*.java\" -atime -3d   Remove capitalization  sed -ie 's/Return /return /g' ReverseString.java    replace string in files  #grep -r \"pack.*me\" . sed -ie 's/package.*me.*;/package com.todzhang;/g' *.java sed -ie 's/package me.todzhang;/package com.todzhang;/g' ~/dev/git/algo/algoWS/src/main/java/com/todzhang/*.java  to update layout from post to posts for jekyll in batch  sed -ie ‘s/layout: posts/layout: posts/g’ _posts/2016-06-03*.md   layout: posts   create directory hierarchy via path  mkdir -p ~/abc/def/egg   -p means create intermediary folders, if not exist. Those intermediary folders with permission 777   lsof to locate whether/who allocated port 8080  lsof means list open files.  lsof -n -P -i | grep 8080   To get rid of ‘’  Sometimes got “403 Forbidden” error when trying to downalod file via wget, e.g.   $ wget http://www.xmind.net/xmind/downloads/xmind-7.5-update1-macosx.dmg --2016-09-09 23:27:29--  http://www.xmind.net/xmind/downloads/xmind-7.5-update1-macosx.dmg Resolving www.xmind.net... 23.23.188.223 Connecting to www.xmind.net|23.23.188.223|:80... connected. HTTP request sent, awaiting response... 403 Forbidden 2016-09-09 23:27:29 ERROR 403: Forbidden.   To solve this problem, using following syntax, adding -U xx   wget -U 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.1.6) Gecko/20070802 SeaMonkey/1.1.4' http://www.xmind.net/xmind/downloads/xmind-7.5-update1-macosx.dmg   case insensitive ls command in bash  Update .bashrc or current active window  shopt -s nocaseglob  one line command to download and extract files   $cd /tmp;curl https://www.kernel.org/pub/linux/utils/util-linux/v2.24/util-linux-2.24.tar.gz\t| tar -zxf-;cd\tutil-linux-2.24;   Redirect request for HTTP 3xxx code  curl -LSso ~/.vim/autoload/pathogen.vim https://tpo.pe/pathogen.vim   -L means redirect upon HTTP code 3xxx -Ss work together to make the curl show errors if there are -o means output to a specified file, rather than stdout   search files contains keyword  grep -ri 'architect' . | awk -F ':' '{print $1}'   Show Linux kernel and name  lsb_release  lsb means Linux Standard Base , -a means print all information  lsb_release -a -u  will output  phray@phray-VirtualBox ~ $ lsb_release -a -u No LSB modules are available. Distributor ID:\tUbuntu Description:\tUbuntu 14.04 LTS Release:\t14.04 Codename:\ttrusty  /etc/os-release  cat /etc/os-release  will output Following  phray@phray-VirtualBox ~ $ cat /etc/os-release NAME=\"Ubuntu\" VERSION=\"14.04.2 LTS, Trusty Tahr\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 14.04.2 LTS\" VERSION_ID=\"14.04\" HOME_URL=\"http://www.ubuntu.com/\" SUPPORT_URL=\"http://help.ubuntu.com/\" BUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\"   Following is the command found in docker.sh  lsb_dist=$(lsb_release -a -u 2&gt;&amp;1 | tr '[:upper:]' '[:lower:]' | grep -E 'id' | cut -d ':' -f 2 | tr -d '[[:space:]]')   show the 2nd column  lsb_release --Codename | cut -f2   gpsswd   DESCRIPTION gpasswd is used to administer the /etc/group file (and /etc/gshadow file if compiled with SHADOWGRP defined). Every group can have administrators, members and a password. System administrator can use -A option to define group administrator(s) and -M option to define members and has all rights of group administrators and members.   Notes about group passwords Group passwords are an inherent security problem since more than one person is permitted to know the password. However, groups are a useful tool for permitting co-operation between different users.   OPTIONS Group administrator can add and delete users using -a and -d options respectively. Administrators can use -r option to remove group password. When no password is set only group members can use newgrp to join the group. Option -R disables access via a password to the group through newgrp command (however members will still be able to switch to this group).   gpasswd called by a group administrator with group name only prompts for the group password. If password is set the members can still newgrp(1) without a password, non-members must supply the password.   FILES Tag\tDescription /etc/group  \tGroup account information. /etc/gshadow  \tSecure group account information.   sudo gpasswd -a USER docker   Compare files difference in two folders  diff -rq ~/dev/pa ~/dev/hexo/myblog/source/_posts  This used option -r (recursive) and -q quite, means only show differences   To execut shell/unix command within vim  :~ls -lt   To open find result with sublime  find . -name \"*Linux*.md\" | xargs sublime  find . -name \"*Linux*.md\" | xargs sublime ~ # open in new Sublime window   To vim/vim edit directly on file output by find command  find . -name \"*tmux*\" -exec vim {} \\;   Be advised you may experience following error message     find: missing argument to `-exec’    actually you should add a slash in front of semi colon   quite mode in apt-get     apt-get will in verbose mode   apt-get -q will be in less verbose , a.k.a quite mode   apt-get -qq in extreme least verbose mode   ","categories": [],
        "tags": ["Linux","DevOps"],
        "url": "/2016/06/10/Linux-Tips.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "JavaScript tips",
        "excerpt":"includes() vs some()           The includes() method is used to check if a specific string exists in a collection, and returns true or false. Keep in mind that it is case sensitive: if the item inside the collection is SCHOOL, and you search for school, it will return false.            The some() method checks if some elements exists in an array, and returns true or false. This is somewhat similar to the concept of the includes() method, the key diffence is the argument is a function but not a string.       splice vs slice   splice: Join or connect (a rope or ropes) by interweaving the strands at the ends.   ‘we learned how to weave and splice ropes’   slice: Cut (something, especially food) into slices.   ‘slice the onion into rings’           The splice() method returns the removed item(s) in an array and slice() method returns the selected element(s) in an array, as a new array object.            The splice() method changes the original array and slice() method doesn’t change the original array.            The splice() method can take n number of arguments:       Argument 1: Index, Required. An integer that specifies at what position to add /remove items, Use negative values to specify the position from the end of the array.   Argument 2: Optional. The number of items to be removed. If set to 0(zero), no items will be removed. And if not passed, all item(s) from provided index will be removed.   Argument 3…n: Optional. The new item(s) to be added to the array.          -5 -4 -3 -2 -1         |  |  |  |  | var array4=[16,17,18,19,20];          |  |  |  |  |          0  1  2  3  4    console.log(array4.splice(-2,1,\"me\")); // shows  [19]   console.log(array4); // shows [16, 17, 18, \"me\", 20]   The slice() method can take 2 arguments:   Argument 1: Required. An integer that specifies where to start the selection (The first element has an index of 0). Use negative numbers to select from the end of an array.   Argument 2: Optional. An integer that specifies where to end the selection. If omitted, all elements from the start position and to the end of the array will be selected. Use negative numbers to select from the end of an array.   var array=[1,2,3,4,5] console.log(array.slice(2)); // shows [3, 4, 5], returned selected element(s).   Observable is lazy   Remember that observables are lazy — if we want to pull a value out of an observable, we must subscribe().   mergeAll vs mergeMap in redux   mergeAll   When the inner observable emits, let me know by merging the value to the outer observable.   Under the hood, the mergeAll() operator basically does takes the inner observable, subscribes to it, and pushes the value to the observer. Here is one sample:   const click$ = Observable.fromEvent(button, ‘click’); const interval$ = Observable.interval(1000);  const observable$ = click$.map(event =&gt; {     return interval$; });  observable$.mergeAll().subscribe(num =&gt; console.log(num));  Because this is a common pattern in Rx, there is a shortcut that achieves the same behaviour — mergeMap().   const click$ = Observable.fromEvent(button, ‘click’); const interval$ = Observable.interval(1000);  const observable$ = click$.mergeMap(event =&gt; {     return interval$; });  observable$.subscribe(num =&gt; console.log(num));   more elegant, concise and flexible approach to check host string belongs to multiple value choices  checkStringAgainstMultipleLiteralValues.js  if (host.match(/[\"uat\" , \"beta\", \"lab\"].(api.)?yourdomain.(com.)?[\"au\",\"io\"]/)) {     console.log('matched') }  closure   闭包就是一个函数引用另外一个函数的变量，因为变量被引用着所以不会被回收，因此可以用来封装一个私有变量。这是优点也是缺点，不必要的闭包只会徒增内存消耗！另外使用闭包也要注意变量的值是否符合你的要求，因为他就像一个静态私有变量一样。   In JavaScript, if you use the function keyword inside another function, you are creating a closure.   Two one sentence summaries about closure      closure is the local variable for a function — kept alive after the function has returned, or   closure is a stack-frame which is not deallocated when the function returns (as if a ‘stack-frame’ were malloc’ed instead of being on the stack!).   In JavaScript, if you declare a function within another function, then the local variables can remain accessible after returning from the function you called.   Tips to redirect page   It’s better to use window.location.replace(\"httpxxx\")', rather than window.location.href=\"xxx\". Because replace` will not save the page in the session history, so users won’t get stufy in never-ending back-button fiasco.  ","categories": [],
        "tags": ["Coding","JavaScript"],
        "url": "/2016/06/12/JavaScript-tips.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Git commands notes",
        "excerpt":"hall-of-frame by commit numbers  git shortlog -s | sort -n -r   In short, shotlog will group and list all commits by author. By default, it will list each commits by author, while -s will only show the number of commit.   Or you can use even a simpler version to sort authers by commit number  git shortlog -s -n   How to to list of what branch for a given commit   $ git log --author todd --grep e2e $ git name-rev 78a4c4c340de70c726844e97233c58fa4738fea9  78a4c4c340de70c726844e97233c58fa4738fea9 remotes/origin/feature/term-deposit-protractors~1   git log tips   There are various parameters calling git log     git log –oneline   git log –stat   To show numer of lines added or remvoed   git log -p OR git log –patch  To show file changes contents   git log –graph –oneline   git log -S”Search_Keyworkd” –oneline   git log – file1.java file2.kt   git log –grep=”JIRA-123”   git log –author=”Tom|Jerry”   git log –after=”yesterday”   git log -3    to list last 3 commits   git show  When the show method is used, it displays all the default information but for only one commit. When a commit SHA is not specified the latest commit is displayed and the output is the same as that displayed by the patch -p command. git show   To list changed files and also changes number of lines  git show --stat  To search keywords in js files only  $ find . -name '*.js' | xargs grep -r 'SearchKeywords'   One line command to add and commit file  git status --short | awk '{split($0, a);print a[2]}' | xargs git add &amp;&amp; git commit -m 'summit status'   to show files commited but not pushed  git diff --stat --cached origin/feature/BRANCH_NAME   to view file content changed  git show PATH/FILE.sql   One line to fetch, checkout newly created branch  git fetch &amp;&amp; git for-each-ref | grep 'AO-106'  | awk '{split($0,a);print a[3]}' | awk \"{split($0,a, '/');print a[-1]}\"   To get changed files NOT contains keyword  $ git status | grep -v \"node_\"   -v is for reverse   Cherry picking  Cherry picking in Git is designed to apply some commit from one branch into another branch.  It can be done if you eg. made a mistake and committed a change into wrong branch, but do not want to merge the whole branch. You can just eg. revert the commit and cherry-pick it on another branch.   To use it, you just need git cherry-pick hash, where hash is a commit hash from other branch.   Difference among git merge and git rebase  TL;NR Git rebase and merge both integrate changes from one branch into another. Where they differ is how it’s done. Git rebase moves a feature branch into a master. Git merge adds a new commit, preserving the history.   Rebase  Git rebase compresses all the changes into a single “patch.” Then it integrates the patch onto the target branch. Unlike merging, rebasing flattens history. It transfers the completed work from one branch to another. In the process, unwanted history is eliminated.   Advocates of Git rebase like it because it simplifies their review process.   Merge  It takes the contents of a source branch and integrates it with a target branch.   Differences among Git and subversion   The differences between git and other source control (like subversion) is Git have to merge conflict before upload to server, while subversion will merge conflicts at server side   following lages are searchable in google      alice   gihub   Errors   WARN  No layout: index.html   Following errors when run hexo g   Unhandled rejection Error: ENOENT: no such file or directory, open '/Users/todzhang/dev/git/blogSrc/themes/next/layout/_scripts/schemes/.swig'     at Error (native)     at Object.fs.openSync (fs.js:549:18)     at Object.fs.readFileSync (fs.js:397:15)     at Object.ret.load (/Users/todzhang/dev/git/blogSrc/node_modules/hexo/node_modules/swig/lib/loaders/filesystem.js:55:15)     at compileFile (/Users/todzhang/dev/git/blogSrc/node_modules/hexo/node_modules/swig/lib/swig.js:695:31)     at Object.eval [as tpl] (eval at &lt;anonymous&gt; (/Users/todzhang/dev/git/blogSrc/node_modules/hexo/node_modules/swig/lib/swig.js:498:13), &lt;anonymous&gt;:338:18)     at compiled [as _compiledSync] (/Users/todzhang/dev/git/blogSrc/node_modules/hexo/node_modules/swig/lib/swig.js:619:18)     at tryCatcher (/Users/todzhang/dev/git/blogSrc/node_modules/hexo/node_modules/bluebird/js/release/util.js:16:23)     at null._compiled (/Users/todzhang/dev/git/blogSrc/node_modules/hexo/node_modules/bluebird/js/release/method.js:15:34)     at View.render (/Users/todzhang/dev/git/blogSrc/node_modules/hexo/lib/theme/view.js:29:15)     at /Users/todzhang/dev/git/blogSrc/node_modules/hexo/lib/hexo/index.js:387:25     at tryCatcher (/Users/todzhang/dev/git/blogSrc/node_modules/hexo/node_modules/bluebird/js/release/util.js:16:23)     at /Users/todzhang/dev/git/blogSrc/node_modules/hexo/node_modules/bluebird/js/release/method.js:15:34     at RouteStream._read (/Users/todzhang/dev/git/blogSrc/node_modules/hexo/lib/hexo/router.js:134:3)     at RouteStream.Readable.read (_stream_readable.js:336:10)     at resume_ (_stream_readable.js:733:12)     at nextTickCallbackWith2Args (node.js:442:9)     at process._tickCallback (node.js:356:17)   Solution:  That’s because one extra space required after semi colon in _config.yml of Next, as following config  # Duoshuo ShortName duoshuo_shortname: cloudsdocker   branch diverged  ~|master ⇒  git status On branch master Your branch and ‘origin/master’ have diverged, and have 1 and 6 different commits each, respectively.   (use “git pull” to merge the remote branch into yours)   The git pull command provides a shorthand way to fetch from origin and rebase local work on it:   $ git pull –rebase   To checkout and rebase master to current branch  git rebase HEAD master  undo a commit  git revert &lt;commit hash&gt;   Your branch is ahead of ‘origin/master’ by xx commits  Go to Intellij, select git view and chose log, then select previous version before those xx commits, then chose Reset current branch to here and with hard as option.   Git commit  List files of a given commit  git show commit_id  git show --pretty=\"\" --name-only bd61ad98       Updates were rejected because the tip of your current branch is behind  git push –set-upstream origin master To https://github.com/CloudsDocker/cloudsdocker.github.io.git  ! [rejected]          master -&gt; master (non-fast-forward) error: failed to push some refs to ‘https://github.com/CloudsDocker/cloudsdocker.github.io.git’ hint: Updates were rejected because the tip of your current branch is behind hint: its remote counterpart. Integrate the remote changes (e.g. hint: ‘git pull …’) before pushing again. hint: See the ‘Note about fast-forwards’ in ‘git push –help’ for details.   todds-MacBook-Pro:cloudsdocker.github.io todzhang$ git push To https://github.com/CloudsDocker/cloudsdocker.github.io.git  ! [rejected]          master -&gt; master (non-fast-forward) error: failed to push some refs to ‘https://github.com/CloudsDocker/cloudsdocker.github.io.git’ hint: Updates were rejected because the tip of your current branch is behind hint: its remote counterpart. Integrate the remote changes (e.g. hint: ‘git pull …’) before pushing again. hint: See the ‘Note about fast-forwards’ in ‘git push –help’ for details. todds-MacBook-Pro:cloudsdocker.github.io todzhang$ git pull fatal: refusing to merge unrelated histories   ","categories": [],
        "tags": ["Git","GitPages"],
        "url": "/2016/06/13/GIT.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "RenMinBi International",
        "excerpt":"RQFII  RQFII stands for Renminbi Qualified Foreign Institutional Investor. RQFII was introduced in 2011 to allow qualified foreign institutional investors to use RMB to buy securities such as bonds and shares and other investment instruments listed on the Chinese market. RQFII is an extension of the Qualified Foreign Institutional Investor (QFII) scheme established in 2002.   What is the difference between QFII and RQFII?   QFII (Qualified Foreign Institutional Investor) allows institutional investors to buy shares in mainland Chinese companies (A-shares) listed on stock exchanges in China. But they have to invest in their home currency, which is then converted. RQFII enables investors to buy into China using RMB. The government introduced it to facilitate the flow of offshore RMB into the country as the currency became more widely used globally.   Who can use these investment programmes?   Although China’s financial system is opening up, there are limits to who can invest. For a foreign institutional investor to qualify, it has to be licensed within its home country, demonstrate its capital strength, and have a track record in investments and asset management.   How much can be invested?   The Chinese government decides how much can be invested by allocating quotas to different countries. The limit is largely determined by demand. The RQFII quota for all countries is currently RMB970 billion (USD156 billion).   Which countries have been granted an RQFII quota?   Thirteen countries have been granted RMB quotas: Australia, Canada, Chile, France, Germany, Hong Kong, Hungary, Luxembourg, Qatar, South Korea, Singapore, Switzerland and the UK. The QFII programme is open to more than 60 countries.  ","categories": [],
        "tags": ["RMB","Finance","FX","Global Market"],
        "url": "/2016/06/13/RQFII.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Github page commands notes",
        "excerpt":"404 error for customized domain (such as godday)  404 There is not a GitHub Pages site here.     Go to github master branch for gitpages site, manually add CNAME file   following lages are searchable in google      alice   gihub   Travis errors:   Got following errors in Travis page:   branch not included or excluded   solution: that’s because your source branch, such as ‘blogSrc’ should be added in whitelist of .travis.yml, for instance   branches:   only:   - blogSrc   fatal: empty ident name  Because –global is requried when setting up travis , below is the sample  git config --global user.email abc@def.com  ","categories": [],
        "tags": ["Git","GitPages"],
        "url": "/2016/06/13/github-pages.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "HTTPS/2",
        "excerpt":"concepts      HTTP : 传统 HTTP 采用明文，完全开放的编码，缺少加解密功能，很容易遭受窃取、篡改等安全威胁。尤其涉及在线交易的网站，遭遇攻击如同家常便饭。   HTTPS : HTTPS 协议传输过程全程加解密，相当于增加了一层 SSL/TSL 分层协议的 HTTP，让访问更加安全。   HTTPS2 : HTTP/2 本身是 HTTP 协议自 1999 年发表后的第一次更新，于 2015 年 5 月正式发布。掐指一算，这个协议积累了 16 年才更新，憋了这么久，放的一定是大招，它的更新主要体现在多路复用，二进制传输，使用 HPACK 压缩头部信息，服务端 Push 。  ","categories": [],
        "tags": ["CTO","MobileInternet","HTTP"],
        "url": "/2016/06/14/HTTP-HTTPS2.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "SQL",
        "excerpt":"Differences between not in, not exists , and left join with null      It seems to me that you can do the same thing in a SQL query using either NOT EXISTS, NOT IN, or LEFT JOIN WHERE IS NULL. For example:    SELECT a FROM table1 WHERE a NOT IN (SELECT a FROM table2) SELECT a FROM table1 WHERE NOT EXISTS (SELECT * FROM table2 WHERE table1.a = table2.a) SELECT a FROM table1 LEFT JOIN table2 ON table1.a = table2.a WHERE table1.a IS NULL   I’m not sure if I got all the syntax correct, but these are the general techniques I’ve seen. Why would I choose to use one over the other? Does performance differ…? Which one of these is the fastest / most efficient? (If it depends on implementation, when would I use each one?)   answer   In a nutshell:   NOT IN is a little bit different: it never matches if there is but a single NULL in the list.   In MySQL, NOT EXISTS is a little bit less efficient   In SQL Server, LEFT JOIN / IS NULL is less efficient   In PostgreSQL, NOT IN is less efficient   In Oracle, all three methods are the same.  ","categories": [],
        "tags": ["Coding","SQL","DB"],
        "url": "/2016/06/14/SQL.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "equity trading",
        "excerpt":"Difference between mutal funds and hedge funds   Similarites:   Both are managed portfolios. There are manager (or managers) pick securities by certain strategy or algorithm.   Difference:      Headge fund tend to aggressive, they take speculative postions in derviaties such as options are able to short sell. This iwll typically increase leverage (and also the risks). On the other hand, mutal fund are not permitted to take these highly leveraged postions and more safer as a result.   Hedge funds are avaiable to a specific group of sophisticated investotrs with high net worth. While mutul fund are more accessibale with minimal amoutns of money.   Differences among money markets and capital markets   The money markets are used for the raising of short term finance, sometimes for loans that are expected to be paid back as early as overnight. Whereas the capital markets are used for the raising of long term finance, such as the purchase of shares, or for loans that are not expected to be fully paid back for at least a year.  ","categories": [],
        "tags": ["fund","finance","Equity"],
        "url": "/2016/06/23/Equity.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Something about authentication",
        "excerpt":"It’s annoying to keep on repeating typing same login and password when you access multiple systems within office or for systems in external Internet. There are bunch of  tools / technicles to cater for such. To my best knowledge, I’ll illustrate some as below.   SSO (Single Sign On)   After you successfully log into one system,  when you hop onto other systems, so you’ll no need to furhter re-enter your user name and password, and you’ll in ‘logged in status’. Under the scene, its sync up your login information among systems. The transferred details is typically called ‘tickets’. One of the implementation logic is ‘kerberos’, which is one protocol developed by MIT and is widely used in such domain. In general, kerberos is supported by various systems and software, for instance, you log onto your windows desktop, your user name and password will be validated in LDAP via either client or API, you   ","categories": [],
        "tags": [],
        "url": "/2016/06/29/SOMETHING-ABOUT-AUTHENTICATION.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "tips in as400 IBM Emulator",
        "excerpt":"Toggle crosshair      Open IBM personal communication client   Go to menu Edit-&gt;Preference-&gt;Apperance-&gt;Display settings-&gt;Ruler   change Rule style to crosshair or turn it off  ","categories": [],
        "tags": ["AS400","IBM Personal Communications"],
        "url": "/2016/06/29/TIPS-AS400-IBM-EMULATOR.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "heavy load web application",
        "excerpt":"Common solutions   With cache + hash, seems nothing is impossible.   hash   When involving big data, heavey load and concurrent, the ultimate solution is hash, such as random insert, time complexity O(1) etc, the only downside is waste of space, however, storage is cheap recently. hash is one must-have for concurrent systems.   cache   There are many mature cache mechanism, such as KV DB like memcache, redis, and they are supporting clusters, operatibility is good as well.   read-write segaration  In most cases, there are more read than write operation, so separate database to acccept write request to master DB, and redirect read request to slave DBs. At the same time, multiple salve DBs can be smoothly scale out, data replicate can leverage log bin.   Database separation and sharding  For DB sharding, leverage hash to direct to corresponding DB shard. To ease the load on single host.  ","categories": [],
        "tags": ["heavy load","mobile internet","DevOps"],
        "url": "/2016/06/29/heavy-load-web-app.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "git raise error filename too long when checkout",
        "excerpt":"Symptoms:   node_modules/grunt-contrib-imagemin/node_modules/pngquant-bin/node_modules/bin-wrapper/node_modules/download/node_modules/request/node_modules/form-data/node_modules/combined-stream/node_modules/delayed-stream/test/integration/test-handle-source-errors.js: Filename too long   solution      Execute following command     git config core.longpaths true           retry checkoutpap  ","categories": [],
        "tags": ["Git","DevOps"],
        "url": "/2016/07/02/file-name-too-long-when-git-checkout.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Useful bookmarks",
        "excerpt":"eBooks  list of various books  Node.js   install nodejs without admin access  ","categories": [],
        "tags": [],
        "url": "/2016/07/24/Bookmarks.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Data Structure",
        "excerpt":"Binary Tree     A binary tree is a tree in which no node can have more than two children.   A property of a binary tree that is sometimes important is that the depth of an average binary tree is considerably smaller than N. An analysis shows that the average depth is √O( N), and that for a special type of binary tree, namely the binary search tree, the average value of the depth is O(log N). Unfortunately, the depth can be as large as N − 1,   Cyclic List  Round Robin scheduling     using some form of an algorithm known as round-robin scheduling. A process is given a short turn to execute, known as a time slice, but it is interrupted when the slice ends, even if its job is not yet complete.   doubly linked list     a doubly linked list. These lists allow a greater variety of O(1)-time update operations, including insertions and deletions at arbitrary posi- tions within the list.   Header and Trailer Sentinels   a header node at the beginning of the list, and a trailer node at the end of the list. These “dummy” nodes are known as sentinels (or guards), and they do not store elements of the primary sequence.   Advantages using sentinel     Although we could implement a doubly linked list without sentinel nodes (as we did with our singly linked list in Section 3.2), the slight extra memory devoted to the sentinels greatly simplifies the logic of our operations. Most notably, the header and trailer nodes never change—only the nodes between them change. Furthermore, we can treat all insertions in a unified manner, because a new node will always be placed between a pair of existing nodes. In similar fashion, every element that is to be deleted is guaranteed to be stored in a node that has neighbors on each side.   Equivalence relation  equivalence relation in mathematics, satisfying the following properties:     Treatment of null: For any nonnull reference variable x, the call x.equals(null) should return false (that is, nothing equals null except null).   Reflexivity: Foranynonnullreferencevariablex,thecallx.equals(x)should return true (that is, an object should equal itself).   Symmetry: For any non null reference variables x and y,the calls x.equals(y) and y.equals(x) should return the same value.   Transitivity: For any nonnull reference variables x, y, and z, if both calls x.equals(y) and y.equals(z) return true, then call x.equals(z) must return true as well.   Ring Buffer  In other words, the circular buffer is well-suited as a FIFO buffer while a standard, non-circular buffer is well suited as a LIFO buffer. Circular buffering makes a good implementation strategy for a queue that has fixed maximum size. Ref to this wiki page   ","categories": [],
        "tags": ["programming","data strucutre"],
        "url": "/2016/07/25/Data-Structure.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Something about RESTful architect",
        "excerpt":"REST API must be hypertext driver  Roy’s interview   Key points for a REST structure  API designers, please note the following rules before calling your creation a REST API:      A REST API should not be dependent on any single communication protocol, though its successful mapping to a given protocol may be dependent on the availability of metadata, choice of methods, etc. In general, any protocol element that uses a URI for identification must allow any URI scheme to be used for the sake of that identification. [Failure here implies that identification is not separated from interaction.]   A REST API should not contain any changes to the communication protocols aside from filling-out or fixing the details of underspecified bits of standard protocols, such as HTTP’s PATCH method or Link header field. Workarounds for broken implementations (such as those browsers stupid enough to believe that HTML defines HTTP’s method set) should be defined separately, or at least in appendices, with an expectation that the workaround will eventually be obsolete. [Failure here implies that the resource interfaces are object-specific, not generic.]   A REST API should spend almost all of its descriptive effort in defining the media type(s) used for representing resources and driving application state, or in defining extended relation names and/or hypertext-enabled mark-up for existing standard media types. Any effort spent describing what methods to use on what URIs of interest should be entirely defined within the scope of the processing rules for a media type (and, in most cases, already defined by existing media types). [Failure here implies that out-of-band information is driving interaction instead of hypertext.]   A REST API must not define fixed resource names or hierarchies (an obvious coupling of client and server). Servers must have the freedom to control their own namespace. Instead, allow servers to instruct clients on how to construct appropriate URIs, such as is done in HTML forms and URI templates, by defining those instructions within media types and link relations. [Failure here implies that clients are assuming a resource structure due to out-of band information, such as a domain-specific standard, which is the data-oriented equivalent to RPC’s functional coupling].   A REST API should never have “typed” resources that are significant to the client. Specification authors may use resource types for describing server implementation behind the interface, but those types must be irrelevant and invisible to the client. The only types that are significant to a client are the current representation’s media type and standardized relation names. [ditto]   A REST API should be entered with no prior knowledge beyond the initial URI (bookmark) and set of standardized media types that are appropriate for the intended audience (i.e., expected to be understood by any client that might use the API). From that point on, all application state transitions must be driven by client selection of server-provided choices that are present in the received representations or implied by the user’s manipulation of those representations. The transitions may be determined (or limited by) the client’s knowledge of media types and resource communication mechanisms, both of which may be improved on-the-fly (e.g., code-on-demand). [Failure here implies that out-of-band information is driving interaction instead of hypertext.]   What’s hyptertext  I have a slide in my REST talk that explains what hypertext (and hypermedia) means.   Hypertext has many definitions: `  Ted Nelson’s original definition was focused on non-linear documents.   By ‘hypertext,’ I mean non-sequential writing — text that branches and allows choices to the reader, best read at an interactive screen. As popularly conceived, this is a series of text chunks connected by links which offer the reader different pathways. [Theodor H. Nelson]   Later, hypertext became associated with a particular user interface mechanism.   Hypertext is a computer-supported medium for information in which many interlinked documents are displayed with their links on a high-resolution computer screen. [Jeffrey Conklin]   When I say hypertext, I mean the simultaneous presentation of information and controls such that the information becomes the affordance through which the user (or automaton) obtains choices and selects actions. Hypermedia is just an expansion on what text means to include temporal anchors within a media stream; most researchers have dropped the distinction.   Hypertext does not need to be HTML on a browser. Machines can follow links when they understand the data format and relationship types.   Sample about web     Think of it in terms of the Web. How many Web browsers are aware of the distinction between an online-banking resource and a Wiki resource? None of them. They don’t need to be aware of the resource types. What they need to be aware of is the potential state transitions — the links and forms — and what semantics/actions are implied by traversing those links. A browser represents them as distinct UI controls so that a user can see potential transitions and anticipate the effect of chosen actions. A spider can follow them to the extent that the relationships are known to be safe. Typed relations, specific media types, and action-specific elements provide the guidance needed for automated agents.    Notworthy points           REST is intended for long-lived network-based applications that span multiple organizations. If you don’t see a need for the constraints, then don’t use them.            Don’t confuse application state (the state of the user’s application of computing to a given task) with resource state (the state of the world as exposed by a given service). They are not the same thing.       A Sample of implementing REST via a distributed queue  the usecase of a distributed queue? The queue should not care about the media types it is exchanging between senders and receivers, yet it can quite easily satisfy the addressability, uniform interface, and statelessness constraints of a RESTful architecture. Can’t a service that is media-type agnostic be a RESTful API? Such a service would require assigning semantic meaning to a specific HTTP method and URI pattern and would not receive state transition information from its exchanged media types (as the service doesn’t care or need to know about the media types). I hope I am explaining myself well.   For example: POST /{queue-name} sends a message to the queue GET /{queue-name}/top is the current message waiting to be processed. It returns Location: /messages/111 DELETE /messages/111 tells server you are done processing that message.   HATEOAS  Definitions  ","categories": [],
        "tags": ["RESTul","Architect"],
        "url": "/2016/08/02/RESTful.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Business Analysis",
        "excerpt":"Purpose of BA     带来一些商业价值（收益）   解决业务痛点   Best Parctices for BA/Architect     架构设计可以大而全，但实施过程最好是简化，化整为零，以最快速度先上线，通过迭代方式不断的完善。（船小好调头）   理想的架构应该像是App Store那样，设计成一个窗口，只要符合平台规范和协议就可以发布上去，关注是应用本身是否work，其他交给平台完成。   收益可以通过多种形式表现：提高产量、节约成本、挖掘用户价值、加强服务质量、优化用户体验、快速响应市场变化等   架构：将产品、技术、运营有机的结合起来   一个成功的互联网应用，背后一定是一群既懂得经营，又明白如何利用技术的业务和一群懂得驱动业务的技术大神们的故事。   任何一个产生生产力革命的行业一定是由业务和技术结合而产生的，比如网上订餐，叫车行业   Samples tips for online eCommerence     From API to CRS (中央预定系统 Centeral Reservation System)  ","categories": [],
        "tags": ["BA","Architect"],
        "url": "/2016/08/04/BA.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "iConnect",
        "excerpt":"UI  HTML5, AngularJS, BootStrap, REST API, JSON  Backend  Hadoop core (HDFS), Hive, HBase, MapReduce, Oozie, Pig, Solr  ","categories": [],
        "tags": [],
        "url": "/2016/08/04/iConnect.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Anatomy of ThreadLocal",
        "excerpt":"Design philosophies   fast-path  /**          * Get the entry associated with key.  This method          * itself handles only the fast path: a direct hit of existing          * key. It otherwise relays to getEntryAfterMiss.  This is          * designed to maximize performance for direct hits, in part          * by making this method readily inlinable.          *          * @param  key the thread local object          * @return the entry associated with key, or null if no such          */         private Entry getEntry(ThreadLocal key) {             int i = key.threadLocalHashCode &amp; (table.length - 1);             Entry e = table[i];             if (e != null &amp;&amp; e.get() == key)                 return e;             else                 return getEntryAfterMiss(key, i, e);         }   Magic number 0x61c88647  This is a golden ratio,  ","categories": [],
        "tags": ["java","source code"],
        "url": "/2016/08/05/Anatomy-of-ThreadLocal.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "kibana, view layer of elasticsearch",
        "excerpt":"  What’s Kibana  kibana is an open source data visualization plugin for Elasticsearch.  It provides visualization capabilities on top of the content indexed on an Elasticsearch cluster. Users can create bar, line and scatter plots, or pie charts and maps on top of large volumes of data.  Why Kibana     Flexible analytics and visualization platform   Real-time summary and charting of streaming data   Intuitive interface for a variety of users   Instant sharing and embedding of dashboards   Reference  digital ocean setup ELK  ","categories": [],
        "tags": ["elasticsearch","DevOps"],
        "url": "/2016/08/11/Kibana.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "kibana, view layer of elasticsearch",
        "excerpt":"  What’s Kibana  kibana is an open source data visualization plugin for Elasticsearch.  It provides visualization capabilities on top of the content indexed on an Elasticsearch cluster. Users can create bar, line and scatter plots, or pie charts and maps on top of large volumes of data.  Why Kibana     Flexible analytics and visualization platform   Real-time summary and charting of streaming data   Intuitive interface for a variety of users   Instant sharing and embedding of dashboards   Reference  digital ocean setup ELK  ","categories": [],
        "tags": ["elasticsearch"],
        "url": "/2016/08/11/Sentry.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "unmodifiableList, unmodifiableSet,unmodifiableMap",
        "excerpt":"What’s it  Returns an unmodifiable view of the specified set.  This method allows modules to provide users with “read-only” access to internal sets. Query operations on the returned set “read through” to the specified set, and attempts to modify the returned set, whether direct or via its iterator, result in an UnsupportedOperationException.&lt;p&gt;   Impelement logic   In general, it re-created a new Collection, and inherit read-only methods from superclass, while raise Exception for those modify functions. Here is the code snippet   static class UnmodifiableCollection&lt;E&gt; implements Collection&lt;E&gt;, Serializable {         private static final long serialVersionUID = 1820017752578914078L;          final Collection&lt;? extends E&gt; c;          UnmodifiableCollection(Collection&lt;? extends E&gt; c) {             if (c==null)                 throw new NullPointerException();             this.c = c;         }          public int size()                   {return c.size();}         public boolean isEmpty()            {return c.isEmpty();}         public boolean contains(Object o)   {return c.contains(o);}         public Object[] toArray()           {return c.toArray();}         public &lt;T&gt; T[] toArray(T[] a)       {return c.toArray(a);}         public String toString()            {return c.toString();}          public Iterator&lt;E&gt; iterator() {             return new Iterator&lt;E&gt;() {                 private final Iterator&lt;? extends E&gt; i = c.iterator();                  public boolean hasNext() {return i.hasNext();}                 public E next()          {return i.next();}                 public void remove() {                     throw new UnsupportedOperationException();                 }             };         }          public boolean add(E e) {             throw new UnsupportedOperationException();         }         public boolean remove(Object o) {             throw new UnsupportedOperationException();         }          public boolean containsAll(Collection&lt;?&gt; coll) {             return c.containsAll(coll);         }         public boolean addAll(Collection&lt;? extends E&gt; coll) {             throw new UnsupportedOperationException();         }         public boolean removeAll(Collection&lt;?&gt; coll) {             throw new UnsupportedOperationException();         }         public boolean retainAll(Collection&lt;?&gt; coll) {             throw new UnsupportedOperationException();         }         public void clear() {             throw new UnsupportedOperationException();         }     }  ","categories": [],
        "tags": ["java"],
        "url": "/2016/08/12/unmodifiableCollection.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "微服务",
        "excerpt":"可以想像一下，之前的传统应用系统，像是一个大办公室里面，有各个部门，销售部，采购部，财务部。办一件事情效率比较高。但是也有一些弊端，首先，各部门都在一个房间里。  ","categories": [],
        "tags": [],
        "url": "/2016/08/24/microservices.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Spark-vs-Storm",
        "excerpt":"The stark difference among Spark and Storm. Although both are claimed to process the streaming data in real time. But Spark processes it as micro-batches; whereas Storm processes per message - meaning if you intend to process things like social data, log data, etc.. you can actually apply CEP (Complex Event Processing) per tuples (i.e each social message in your example).  Spark, on other hand is good at processing small blocks of data, for instance if you are streaming a whole blobs of data (say some huge files of medical record, for example).   So, obviously, I would say, give your usecase, Storm may be a better fit for your needs.   And last, between distributions, I have found Hortonworks to be more easier in standing up and managing the cluster than other distribution (please take this comment with a grain of salt, as I may be slightly biased towards HWX given my comfort-zone of working around this distribution more than others)  ","categories": [],
        "tags": ["Spark","Storm","Apache","Haddop","BigData"],
        "url": "/2016/08/30/Spark-vs-Storm.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "http methods",
        "excerpt":"RFC origion http://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html#sec9.1.2)   part of Hypertext Transfer Protocol – HTTP/1.1 RFC 2616 Fielding, et al. 9 Method Definitions   The set of common methods for HTTP/1.1 is defined below. Although this set can be expanded, additional methods cannot be assumed to share the same semantics for separately extended clients and servers.   The Host request-header field (section 14.23) MUST accompany all HTTP/1.1 requests.   9.1 Safe and Idempotent Methods   9.1.1 Safe Methods   Implementors should be aware that the software represents the user in their interactions over the Internet, and should be careful to allow the user to be aware of any actions they might take which may have an unexpected significance to themselves or others.   In particular, the convention has been established that the GET and HEAD methods SHOULD NOT have the significance of taking an action other than retrieval. These methods ought to be considered “safe”. This allows user agents to represent other methods, such as POST, PUT and DELETE, in a special way, so that the user is made aware of the fact that a possibly unsafe action is being requested.   Naturally, it is not possible to ensure that the server does not generate side-effects as a result of performing a GET request; in fact, some dynamic resources consider that a feature. The important distinction here is that the user did not request the side-effects, so therefore cannot be held accountable for them.   9.1.2 Idempotent Methods   Methods can also have the property of “idempotence” in that (aside from error or expiration issues) the side-effects of N &gt; 0 identical requests is the same as for a single request. The methods GET, HEAD, PUT and DELETE share this property. Also, the methods OPTIONS and TRACE SHOULD NOT have side effects, and so are inherently idempotent.   However, it is possible that a sequence of several requests is non- idempotent, even if all of the methods executed in that sequence are idempotent. (A sequence is idempotent if a single execution of the entire sequence always yields a result that is not changed by a reexecution of all, or part, of that sequence.) For example, a sequence is non-idempotent if its result depends on a value that is later modified in the same sequence.   A sequence that never has side effects is idempotent, by definition (provided that no concurrent operations are being executed on the same set of resources).   9.2 OPTIONS   The OPTIONS method represents a request for information about the communication options available on the request/response chain identified by the Request-URI. This method allows the client to determine the options and/or requirements associated with a resource, or the capabilities of a server, without implying a resource action or initiating a resource retrieval.   Responses to this method are not cacheable.   If the OPTIONS request includes an entity-body (as indicated by the presence of Content-Length or Transfer-Encoding), then the media type MUST be indicated by a Content-Type field. Although this specification does not define any use for such a body, future extensions to HTTP might use the OPTIONS body to make more detailed queries on the server. A server that does not support such an extension MAY discard the request body.   If the Request-URI is an asterisk (“”), the OPTIONS request is intended to apply to the server in general rather than to a specific resource. Since a server’s communication options typically depend on the resource, the “” request is only useful as a “ping” or “no-op” type of method; it does nothing beyond allowing the client to test the capabilities of the server. For example, this can be used to test a proxy for HTTP/1.1 compliance (or lack thereof).   If the Request-URI is not an asterisk, the OPTIONS request applies only to the options that are available when communicating with that resource.   A 200 response SHOULD include any header fields that indicate optional features implemented by the server and applicable to that resource (e.g., Allow), possibly including extensions not defined by this specification. The response body, if any, SHOULD also include information about the communication options. The format for such a   body is not defined by this specification, but might be defined by future extensions to HTTP. Content negotiation MAY be used to select the appropriate response format. If no response body is included, the response MUST include a Content-Length field with a field-value of “0”.   The Max-Forwards request-header field MAY be used to target a specific proxy in the request chain. When a proxy receives an OPTIONS request on an absoluteURI for which request forwarding is permitted, the proxy MUST check for a Max-Forwards field. If the Max-Forwards field-value is zero (“0”), the proxy MUST NOT forward the message; instead, the proxy SHOULD respond with its own communication options. If the Max-Forwards field-value is an integer greater than zero, the proxy MUST decrement the field-value when it forwards the request. If no Max-Forwards field is present in the request, then the forwarded request MUST NOT include a Max-Forwards field.   9.3 GET   The GET method means retrieve whatever information (in the form of an entity) is identified by the Request-URI. If the Request-URI refers to a data-producing process, it is the produced data which shall be returned as the entity in the response and not the source text of the process, unless that text happens to be the output of the process.   The semantics of the GET method change to a “conditional GET” if the request message includes an If-Modified-Since, If-Unmodified-Since, If-Match, If-None-Match, or If-Range header field. A conditional GET method requests that the entity be transferred only under the circumstances described by the conditional header field(s). The conditional GET method is intended to reduce unnecessary network usage by allowing cached entities to be refreshed without requiring multiple requests or transferring data already held by the client.   The semantics of the GET method change to a “partial GET” if the request message includes a Range header field. A partial GET requests that only part of the entity be transferred, as described in section 14.35. The partial GET method is intended to reduce unnecessary network usage by allowing partially-retrieved entities to be completed without transferring data already held by the client.   The response to a GET request is cacheable if and only if it meets the requirements for HTTP caching described in section 13.   See section 15.1.3 for security considerations when used for forms.   9.4 HEAD   The HEAD method is identical to GET except that the server MUST NOT return a message-body in the response. The metainformation contained in the HTTP headers in response to a HEAD request SHOULD be identical to the information sent in response to a GET request. This method can be used for obtaining metainformation about the entity implied by the request without transferring the entity-body itself. This method is often used for testing hypertext links for validity, accessibility, and recent modification.   The response to a HEAD request MAY be cacheable in the sense that the information contained in the response MAY be used to update a previously cached entity from that resource. If the new field values indicate that the cached entity differs from the current entity (as would be indicated by a change in Content-Length, Content-MD5, ETag or Last-Modified), then the cache MUST treat the cache entry as stale.   9.5 POST   The POST method is used to request that the origin server accept the entity enclosed in the request as a new subordinate of the resource identified by the Request-URI in the Request-Line. POST is designed to allow a uniform method to cover the following functions:     - Annotation of existing resources;   - Posting a message to a bulletin board, newsgroup, mailing list,     or similar group of articles;   - Providing a block of data, such as the result of submitting a     form, to a data-handling process;   - Extending a database through an append operation. The actual function performed by the POST method is determined by the server and is usually dependent on the Request-URI. The posted entity is subordinate to that URI in the same way that a file is subordinate to a directory containing it, a news article is subordinate to a newsgroup to which it is posted, or a record is subordinate to a database.   The action performed by the POST method might not result in a resource that can be identified by a URI. In this case, either 200 (OK) or 204 (No Content) is the appropriate response status, depending on whether or not the response includes an entity that describes the result.   If a resource has been created on the origin server, the response SHOULD be 201 (Created) and contain an entity which describes the status of the request and refers to the new resource, and a Location header (see section 14.30).   Responses to this method are not cacheable, unless the response includes appropriate Cache-Control or Expires header fields. However, the 303 (See Other) response can be used to direct the user agent to retrieve a cacheable resource.   POST requests MUST obey the message transmission requirements set out in section 8.2.   See section 15.1.3 for security considerations.   9.6 PUT   The PUT method requests that the enclosed entity be stored under the supplied Request-URI. If the Request-URI refers to an already existing resource, the enclosed entity SHOULD be considered as a modified version of the one residing on the origin server. If the Request-URI does not point to an existing resource, and that URI is capable of being defined as a new resource by the requesting user agent, the origin server can create the resource with that URI. If a new resource is created, the origin server MUST inform the user agent via the 201 (Created) response. If an existing resource is modified, either the 200 (OK) or 204 (No Content) response codes SHOULD be sent to indicate successful completion of the request. If the resource could not be created or modified with the Request-URI, an appropriate error response SHOULD be given that reflects the nature of the problem. The recipient of the entity MUST NOT ignore any Content-* (e.g. Content-Range) headers that it does not understand or implement and MUST return a 501 (Not Implemented) response in such cases.   If the request passes through a cache and the Request-URI identifies one or more currently cached entities, those entries SHOULD be treated as stale. Responses to this method are not cacheable.   The fundamental difference between the POST and PUT requests is reflected in the different meaning of the Request-URI. The URI in a POST request identifies the resource that will handle the enclosed entity. That resource might be a data-accepting process, a gateway to some other protocol, or a separate entity that accepts annotations. In contrast, the URI in a PUT request identifies the entity enclosed with the request – the user agent knows what URI is intended and the server MUST NOT attempt to apply the request to some other resource. If the server desires that the request be applied to a different URI,   it MUST send a 301 (Moved Permanently) response; the user agent MAY then make its own decision regarding whether or not to redirect the request.   A single resource MAY be identified by many different URIs. For example, an article might have a URI for identifying “the current version” which is separate from the URI identifying each particular version. In this case, a PUT request on a general URI might result in several other URIs being defined by the origin server.   HTTP/1.1 does not define how a PUT method affects the state of an origin server.   PUT requests MUST obey the message transmission requirements set out in section 8.2.   Unless otherwise specified for a particular entity-header, the entity-headers in the PUT request SHOULD be applied to the resource created or modified by the PUT.   9.7 DELETE   The DELETE method requests that the origin server delete the resource identified by the Request-URI. This method MAY be overridden by human intervention (or other means) on the origin server. The client cannot be guaranteed that the operation has been carried out, even if the status code returned from the origin server indicates that the action has been completed successfully. However, the server SHOULD NOT indicate success unless, at the time the response is given, it intends to delete the resource or move it to an inaccessible location.   A successful response SHOULD be 200 (OK) if the response includes an entity describing the status, 202 (Accepted) if the action has not yet been enacted, or 204 (No Content) if the action has been enacted but the response does not include an entity.   If the request passes through a cache and the Request-URI identifies one or more currently cached entities, those entries SHOULD be treated as stale. Responses to this method are not cacheable.   9.8 TRACE   The TRACE method is used to invoke a remote, application-layer loop- back of the request message. The final recipient of the request SHOULD reflect the message received back to the client as the entity-body of a 200 (OK) response. The final recipient is either the   origin server or the first proxy or gateway to receive a Max-Forwards value of zero (0) in the request (see section 14.31). A TRACE request MUST NOT include an entity.   TRACE allows the client to see what is being received at the other end of the request chain and use that data for testing or diagnostic information. The value of the Via header field (section 14.45) is of particular interest, since it acts as a trace of the request chain. Use of the Max-Forwards header field allows the client to limit the length of the request chain, which is useful for testing a chain of proxies forwarding messages in an infinite loop.   If the request is valid, the response SHOULD contain the entire request message in the entity-body, with a Content-Type of “message/http”. Responses to this method MUST NOT be cached.   9.9 CONNECT   This specification reserves the method name CONNECT for use with a proxy that can dynamically switch to being a tunnel (e.g. SSL tunneling [44]).  ","categories": [],
        "tags": ["http","rfc"],
        "url": "/2016/09/01/HTTP-Methods-RFC.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Mac tips",
        "excerpt":"how to show full path in Finder window  Open and run following command in terminal window  defaults write com.apple.finder _FXShowPosixPathInTitle -bool true; killall Finder   how to switch differnt input  This can be configured in system proferenece-&gt;keyboard shortcuts-&gt;input sources-&gt;select next source in input Ctrl+Alt+Space   How to force an app to quit on your Mac  Press these three keys together: Option, Command, and Esc (Escape). This is similar to pressing Control-Alt-Delete on a PC. Or choose Force Quit from the Apple () menu in the upper-left corner of your screen.   To capture screenshot of Mac  Press Shift + Command + 5, then click an option, like Selection button to capture a still selection or Whole screen button to record your whole screen.   The screenshot tools appear in a small palette, which you can drag to reposition. The palette includes options for where to save the screenshot, whether to show the pointer, and more.   Save, edit, and share your shots  After you take a screenshot or video, a thumbnail appears in the corner of your screen. Drag it int   Forward delete (delete char next to cursor)  Fn+Delete   To hide and show dock  Just hit Command+Option+D at any time, and the dock will glide away (or back again).   Windows   Open a file     Cmd+Down arrow   Cmd+O   Switch between windows  While you have two or more documents open in your favorite word-processing software, simply press and hold the Command ⌘ key and then strike the ~ (Tilde) key on your keyboard.   navigate on page  fn + ←  Jump to top of document fn + →  Jump to bottom of document fn + ↓  Advance down one page fn + ↑  Advance up one page   Enter path in open/save window  Press Ctrl+Shift+G will open GoTo window   Switch running applications  Besides Command + Tab, you can swipe up on the touchpad with three fingers to view the windows of open apps, allowing you to quickly switch between programs. This view is called Mission Control, which also has its own dedicated keyboard shortcut (F3).   Capture screen shot   To capture the entire screen, press Command-Shift-3  The screen shot will be automatically saved as a PNG file on your desktop with the filename starting with “Picture” followed by a number, example Picture 1, Picture 2, and so on.   To capture a portion of the screen, press Command-Shift-4   Terminal  Page up /Down     Page up/down: Fn + up/down arrow, or Ctrl+f/b     Setup command line for sublime      ln -s \"/Applications/Sublime Text.app/Contents/SharedSupport/bin/subl\" /usr/local/bin/sublime           Open files from terminal  open abc.jpg   To make changed profile settings take effect right away  source .bash_profile   Applications   Safari   Sublime   Switch different sublime windows   'command + ~'   Path of customize build system  ~/Library/Application Support/Sublime Text 3/Packages/User/Nodejs.sublime-build   Open folders in Sublime  sublime .   Open and edit file with sublime  open -a 'Sublime Text' .bash_profile   Show full path in Sublime text 3  With Sublime Text 3, all that’s necessary is to edit your Sublime user preferences (Preferences -&gt; Settings - User) to include:  {   // ... other settings   \"show_full_path\": true }   Open files (go to anything) in sublime  Command + P   Go to matching brace  Ctrl + M   Go to previous/next position      Ctrl + “-“   Ctrl + Shift + “-“   Preview markdown   Command + Shift + P -&gt; Preview   Gestures   Three (3) fingures for windows switch      Up: Mission control, show all running applications   Left/Right: Swtich desktops   Thumb + three figures     Center to outside: show desktop   Outside to Center: show launch pad  ","categories": [],
        "tags": ["Mac","shortcut","Efficiency"],
        "url": "/2016/09/01/Mac-Tips.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "漫谈开发设计中的一些‘原则’及'设计哲学'",
        "excerpt":"在开发设计中有一些常用原则或者潜规则，根据笔者的经验，这里稍微总结一下最最常用的，以飨读者。   POLA  The principle of least astonishment (POLA) is: “If a necessary feature has a high astonishment factor, it may be necessary to redesign the feature.”   In general engineering design contexts, the principle means that a component of a system should behave in a way that users expect it to behave; that is, users should not be astonished by its behavior.   A textbook formulation is: “People are part of the system. The design should match the user’s experience, expectations, and mental models.”   In more practical terms, the principle aims to leverage the pre-existing knowledge of users to minimize the learning curve, for instance by designing interfaces that borrow heavily from “functionally similar or analogous programs with which your users are likely to be familiar”.   Example:  A web site could declare an input that should autofocus when the page is loaded,[8] such as a search field (e.g., Google.com), or the username field of a login form.   DRY  这里的DRY是Do Not Repeat Yourself的缩写。具体解释参见 ,严谨的定义是　Every piece of knowledge must have a single, unambiguous, authoritative representation within a system，意思是：任何一部分知识在系统中必须只有单一的，清晰并且权威的展示。？？？这是啥意思，没懂。简单说就是不要重复自己任何一部分的工作。比如说，有一段代码是用于清除字条串中的HTML符号，在多个程序中会用到此功能，如果每个地方都使用如下代码  html = html.replaceAll(\"\\\\&lt;.*?&gt;\",\"\")  html = html.replaceAll(\"&amp;nbsp;\",\"\"); html = html.replaceAll(\"&amp;amp;\".\"\");  如果是只有２，３个地方用到(Martin曾经提到过Rule of three,意思是一段代码如果被copy３次以上就应该重构到一个单独的子方法中)，你可能就直接复制过来用，但是想想是２，３百个地方用到呢？如果上面需要再做个修改(如下)，你是不是要去这个２，３百个地方去改代码。   html = html.replaceAll(\"&amp;lt;\".\"&lt;\"); html = html.replaceAll(\"&amp;gt;\".\"&gt;\");  所以这个DRY的规则就是推荐使用 子方法 的方式，这样只需要修改一次即可. 与之类似的编程思想有 DIE（Duplication is Evil）,SPoT(Single Point of Truth), SSOT (Singel Source of Truth) 。 题外话，和DRY对应的是WET,意思是 “write everything twice”（任何东西都写两遍）或者”we enjoy typing” （我们就是喜欢打字编码）。　:-)。   KISS  KISS 是 Keep it simple, stupid （或者Keep it short and simple ）的简称。意思是在设计时保持简约，通俗。这个很像是现在推畅的“极简风”。 使用KISS有什么好处呢？如下是其中的一些：     你可以更快的解决更多的问题   你可以使用更少的代码来解决复杂的问题   你可以写出更高质量的代码   你可以创建更大的系统，更好的去运维   你的代码将更加灵活，当有新需求时可以更好的支持扩展，修改或者重构   等等   在软件设计领域， 有一些技术具体实现这个精髓，比如 DDD (Domain Driven Design),TDD (Test Driven Develop),这个使代码集中在真正需要的功能上，而不需要其他额外的。另外一个建议是 不要试图通过注释来提高代码的可读性，而应该从代码本身提高。比如如下是不太好的变量定义  // i is for 'counter' and j means total sum int i, j;   而如下是好的设计   // more intuitive one int counter,sum;   与此相呼应的是称作 奥卡姆剃刀 或者 简约之法则：     Occam’s razor  The simplest (explanation|solution) is usually the best one. 往往最简单的解决方案是最好的解决方案    具体到以Java为例的程序设计，如下有一些实践KISS的建议     首先，认清自己，不要认为自己是个天才，这往往是你犯的第一个错。   把你的工作打散成几个子工作，每个部分不会耗费超过4-12个小时去完成   把一个问题分成几个小的子问题，每个问题可以通过1个或者只要几个类就能解决。   保持每个方法只做一件事，并且不要超过30-40行的代码量   保持每个类的体积不要太大。   不要害怕扔掉不用的代码。就像家里用不到的东西就及时扔掉一样。   New Jersey style （Worse is better）  新泽西风格，也叫做“Worse is better”。此原则指出，系统的质量不会因为新功能的增多而提高。 比如一个软件，只提供一些功能，但是用户很方便使用，有可能比一些提供了非常多令人眼花缭乱功能的“大杂烩”似的软件。比如像 Linux下面的 vi/vim, 浏览器中的Chrome.   SOLID  SOLID是几个编程哲学的总称，即 SOLID (Single responsibility, Open-closed, Liskov substitution, Interface segregation and Dependency inversion) ，下面我们分别解释一下：  Single responsibility （SRP）  单一功能原则。Robert描述这个为“A class should have only one reason to change.”，即修改一个类（或者模块）有（且只能有）一个理由。简单说就是 一个类或者模块只能负责一个功能。举个例子，有一个模块负责生成一个报表，可以想像可能有两个理由去修改此模块，第一，报表的内容要变，第二，报表的格式要改。这两个改动是出于不同的理由，一个是内容的一个美化版面的。 而 “单一职责” 规则认为这是两个不同的职责，因此应该分成两个不同的子模块。如果把两件事情放在一起，并且不同的改动是出于不同的原因，这种设计是不好的。此规则方便系统各模块间解耦合。  Open/closed principle （OCP）  开闭原则。Bertrand描述为“”software entities (classes, modules, functions, etc.) should be open for extension, but closed for modification”;”，也就是说对于一个实体（类，模块，方法等）允许在不修改源代码的前提下扩展它的功能行为。即，你可以把 新代码放到新的类或者方法中，新类通过继承来重用已有代码及功能。而 已有的代码只有在修bug时才去修改。 此原则主要用于降低在添加新加功能时引入新的bug的风险。  The Liskov Substitution Principle （LSP）  里氏替换原则. 原文是 “Derived classes must be substitutable for their base classes.”，意思是，派生类（子类）对象能够替换其基类（超类）对象被使用。 比如说，如果 S 是T 的子类， 那么任何T类的具体实现对象都可以替换S的实现对象出现的地方，具体的调用者也不知道具体是父类还是子类，还不会出现任何错误。比如下图，调用者可以2来替换1的地方 。   Interface segregation principle （ISP）  接口隔离。原文是 many client-specific interfaces are better than one general-purpose interface. 意思是多个特定客户端接口要好于一个宽泛用途的接口。Make fine grained interfaces that are client specific. 应该定义粒度合适的一系列接口(像下图)，以便于每个客户去实现具体的功能请求。换句话说是，客户（client）不应该必须去依赖于它用不到的功能方法。此原则的目的是系统解开耦合，从而容易重构，更改和重新部署。    Dependency inversion principle (DIP)  依赖反转原则. 原文是 One should “Depend upon Abstractions. Do not depend upon concretions.” 意思是 一个方法应该遵从“依赖于抽象而不是一个实例”。该原则规定：      高层次的模块不应该依赖于低层次的模块，两者都应该依赖于抽象接口。   抽象接口不应该依赖于具体实现。而具体实现则应该依赖于抽象接口。 这个就像是设计模式中的Adaptor适配器模式。 下图解释了这个原理。  图1中，高层对象A依赖于底层对象B的实现；图2中，把高层对象A对底层对象的需求抽象为一个接口A，底层对象B实现了接口A，这就是依赖反转。   SOC  Separation of concerns,?即关注点分离。 是处理复杂性的一个原则。由于关注点混杂在一起会导致复杂性大大增加，所以能够把不同的关注点分离开来，分别处理就是处理复杂性的一个原则，一种方法。这个与SOLID中的 SRP很类似。   YANGI  是”You aren’t gonna need it”的缩写，直译是“你将来用不到它的”。这个是极限编程的一个编程思想。意思是说,永远不要因为 预计你会用到某个功能就去写一段代码去实现，总是只有问题出现了，真的需要这个功能时才去写。   参考     https://en.wikipedia.org/wiki/Principle_of_least_astonishment   DRY    六大设计原则–里氏替换原则【Liskov Substitution Principle】   SOLID   how to keep code simple   奥卡姆剃刀   Apache KISS   Worse is better   ","categories": [],
        "tags": ["development","desgin","principals","MyBlog"],
        "url": "/2016/09/02/Design-Principals.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Kubernetes 与 Docker Swarm的对比",
        "excerpt":"Kubernetes 和Docker Swarm 可能是使用最广泛的工具，用于在集群环境中部署容器。但是这两个工具还是有很大的差别。   Kubernetes   Google根据其在Linux上容器管理经验，改造到docker管理上，就是kubernetes。他的在许多方面表现良好。最重要的是构造于Google多年的宝贵经验只上。   如果你从docker1.0以上开始使用kubernetes，你得到的用户体验会非常良好。比如你会发现kubernetes解决一些docker自身的问题。例如你可以mount（绑定）持久化存储卷（volume），以便于在迁移docker时不至于丢失数据。   kubernetes使用flannel（一个使用go写的虚拟网络的开源系统）构造容器间的网络通信。它还内置有负载均衡。除此之外，它的“服务发现”使用了etcd（一个使用golang编写的开源虚拟网络系统）。然而，使用kubernetes是有代价的。首先，它用一个不同的命令行接口，不同的编程接口及不同的YAML文件定义等。换言之，你不能使用docker命令行接口也不能用docker compose来定义容器。为了使用kubernetes，所有所有的东西都需要从头开始。这就好像这个工具并不是为了docker写的一样（这个在某种程度上确实是）。kubernetes把集群带到了一个全新的高度，代价是学习曲线比较陡。   Docker Swarm   docker-swarm 使用了一个不同的方式。它是docker原生的集群工具。   最方便的部分是它暴露了docker标准的编程接口，意味着你之前一直在使用的任何与docker沟通的工具（docker命令行接口，docker compose，dokku，krane等等），都可以无缝的在docker swarm上使用。   这个其实是个双刃剑，毁誉参半。一直可以使用自己得心应手熟悉的工具，这再好不过了，然而，这样意味着我们又被docker紧紧的“耦合”了（而非业界一直追求的松耦合”）。如果你需要的都能满足，那皆大欢喜。可是如果不是呢，要是你有特殊需求这些API满足不了怎么办？这是就不得不去耍一些“小聪明”。   我们来仔细剖析下这两个工具，主要会从如何在集群中运行容器，这两个工具分别如何安装以及他们提供的功能。   安装设置   安装设置swarm非常简单，简单明了并且很灵活。我们需要做的就是安装一个服务发现工具，然后在所有的节点上安装swarm容器。因为它自己就是以一个docker容器来部署的，因此它在不同的操作系统上运行方式都是没有差别的。我们运行swarm容器，开放一个端口，然后告知服务发现模块的地址。这不能再简单了。我们甚至可以在没有任何服务发现模块的情况下开始使用，来看看我们是否喜欢它，当开始真正认真的要使用时再去添加etcd，consul或者其他支持的工具。   相比较而言，kubernetes的安装就有点复杂晦涩了。不同的操作系统上安装都不同。每个操作系统都有自己的独立安装指令和运维团队。比如说，如果你选择使用vagrant来试用，然后在Fedora这里遇到问题卡住了，但这不是意味着其他的（比如Ubuntu或者coreos）也不能用。你可以，不过要开始在kubernetes官方以外到处搜索. 你所需要的很可能在某个社区论坛里提供的解决方案，但是你需要时间去查询，运气好的话可以在第一次尝试就能用。一个严重的问题是安装kubernetes依赖于bash脚本。如果我们是处于配置管理不是必须的情况下是，这个本身可能不是大问题。我们可能不希望运行一个脚本，而是希望kubernetes成为puppet，chef或者ansible的一部分。同样，这个也是可以解决的。你可以找到ansible 的安装手册来动行kubernetes或者你自己去写。跟swarm比这些都不是什么大问题,只是一点点的小痛苦而已。使用刀砍请不要期待任何的安装文档，除非都可使用docker命令行的时候运行的参数而已。我们打算去运行容器，swarm可以实现这个，但kubernetes 没有。有些人可能并不在意具体是使用哪个服务发现的工具。我喜欢swarm背后提倡的那种极简风格，以及他背后的逻辑，内置电池，拆留由已。任何东西都是拆箱可用但是我们还提供了选项让你去替换其中的任一个模块。   与swarm不同的是，kubernetes 是一个可配置的工具。你需要跟kubernetes提供的各个选项来共生存。例如，如果你打算使用kubernets,你必须要使用etcd.我不是说etcd不好(实际上正好相反)，但是如果你习惯于，比如你在非常复杂的业务场景下使用consul，如果要使用服务发现，必须特别针对kubernets使用一个，而剩下的其他部分使用其他一个产品。另外一个对使用Kubernets觉得不方便的地方就是你需要在事先了解各种事情。比如，你需要告诉他要用到的所有节点的地址，每个节点的职责是什么,这个集群有多少个“小黄人” (minion,是kubernet对于一个集群中机器之前叫的名字)，等等。 而使用Swarm，我们只需要启动一个节点，告诉他加入网络，就可以了。我们不需要提前告诉关于集群其他的信息，因为这些信息会通过各个节点间的 “八卦”（gossip protocol）来传输。   配置本身也许并不是这些工具之间最大的差别。不管你使用哪个工具，或早或晚，所有都会配置好并运行起来，到时候你们就会忘掉在配置时的痛苦经历。你可能会说我们之所以选择某个工具就是因为它安装配置简单吧。很公平的。我们继续往下讨论如何定义容器及之上的这些工具。   运行容器   如果使用Swarm来运行Docker容器，你如何去定义所有的参数呢？ 实际上你根本不需要！你所做的跟使用Swarm之前没有什么不同。比如，你习惯于使用Docker CLI（命令行接口），你可以继续使用几乎相同的命令。如果你习惯于使用Docker Componse来运行容器，你可以继续在Swarm集群中使用。不管你之前习惯于怎么使用容器，你仍旧可以使用，只是在更大级别的集群中使用。   Kubernetes要求你去学习它自己的CLI（命令行接口）和配置。你不能使用你之前创建的docker-compose.yml配置，你必须要去新建与Kubernetes对应的配置。你也不能使用之前学习的Docker CLI（命令行接口）。你必须要去学习 Kubernetes CLI（命令行接口），并且很有可能，确保你整个公司机构也要去学习。   不管你选择哪个工具来部署集群，你都要先熟悉Docker。你可能已经习惯于使用 使用Docker Compose来定义你运行容器的参数。如果你曾经使用它超过几个小时，你可能就会直接使用它而扔掉Docker CLI。你可以使用它运行容器，跟踪日志变化，等等。另外一方面，你可能是Docker的  “死忠”，看不上 Docker Compose，而是任何事都使用Docker CLI，或者你甚至自己写bash脚本来运行容器。不管哪种方式，这些都可以在Docker Swarm上使用。   如果你选择Kubernetes，请先准备好同一件事需要有多个定义配置。你需要使用 Docker Compose来运行Kubernetes范围外的容器。开发人员需要继续在他们的笔记本电脑上运行容器，你的测试环境可能也不是一个大集群，等等。换言之，如果你选择了Docker，那么 Docker Compose 和 Docker CLI将是不可避免的。你肯定会在某些地方或者以某种方式用到它们。一旦你开始使用 Kubernetes，你就会发现你所有的 Docker Compose的配置都要翻译成 Kubernetes的方式，从这个时候，你也要开始维护这两个版本了。使用 Kubernetes，这些重复的事情意味着维护成本的提高。重复只是一个问题，另外的是你在集群中运行的命令将于集群外使用的命令不一样了。你之前学习并喜爱的Docker的所有命令在 Kubernetes集群内将是完全行不通了。   Kubernetes的开发团队强制你去按照他们的办事方式行事，其实不是为了让你过的那么苦。如此巨大差别的主要原因是 Swarm 和 Kubernetes针对同一问题采取了不同的处理手段。 Swarm团队决定使用跟Docker相同的API接口，因此我们看到这些之前有如此好的兼容性。结果就是，你可以使用几乎所有之前的东西，只是在更大级别的集群中使用而已。没有新的东西要去做，不需要去重复配置参数，也不需要去新学习什么。不管你是直接使用Docker CLIgipj使用Swarm，这些API接口是基本上一致的。不好的方面是如果你想Swarm去做某件事，但是这个在Docker API中没有，这样你就悲催了。简单来说，如果你在找一个工具，可以部署使用Docker API的容器到集群中，那么 Swarm就是解决方案。另一方面，如果你想要一个工具，可以解决Docker API办不到的事情，你就应该去试试 Kubernetes了。这就是功能性（ Kubernetes）VS. 简易性（Swarm）。   这里唯一还没有回答的问题就是这些限制是什么。主要的限制中有两个，网络配置和持续化存储卷。走到Swarm1.0，我们不能连接运行于不同服务器上的容器。事实上，现在我们仍然不能连接他们，但是我们可能通过跨主机网络来协助连接运行于不同服务器上的容器。这是个非常强大的功能。而 Kubernetes使用 flannel（一个使用go写的虚拟网络的开源系统）来实现网络路由。目前自从Docker1.0开始， 这个功能也成为Docker CLI的一部分了。   另外一个问题是持续化存储卷。Docker在1.9版本中引入此功能。直到最近，如果你持久化一个数据卷，这个容器就绑定到这个持续化卷所在的服务器上了。它不能再次移动，除非你使用一些恶心的小花招，比如在不同的服务器间复制这些数据卷文件。这些本身是一些比较慢的操作，这与Swarm等工具的初衷也是相违背的。即便你有时间去复制，你也不知道从哪里去复制，因为集群工具会认为你整个数据中心是一个实体。你的容器会部署到它认为最合适的地方（比如，运行最少容器，CPU或者内容使用率最低，等等）。现在已经有Docker内置的持久化卷了。网络和持久化卷缺失曾经是许多人放弃Swarm而去选择 Kubernetes。自从Docker1.9，这此已经成为过去式。   选择   当需要在Docker Swarm 和 Kubernetes做出选择时，可以考虑如下几点。你是否想依赖于Docker自己来解决集群的问题。如果是，选择Swarm。如果某些功能在Docker中不支持，那它也非常可能在Swarm中找不到，因为Swarm是依赖于Docker API的。另外一方面，如果你想要一个工具可以解决Docker的限制，Kubernetes将是不错的选择。Kubernetes不是基于Docker，而是基于Google多年对于管理容器的经验。它是按照自己的方式来行事。   真正的问题是Kubernetes这种自我的方式（与Docker非常的不同）相比于它提供的优点是否值得。或者，我们是不是应该押宝在Docker本身上，期待Docker将来会解决这些难题。在回答这些问题之前，请先看一下Docker1.9之后的版本。它已经有个网络配置及持久化卷。也有了所谓的“除非死掉 才去重启”的策略，这次方便去管理那些讨厌的错误。现在Kubernetes 和 Swarm之间的差异又少了3个。实际上，现如今，Kubernetes 相对于 Swarm的优势很少了。另一方面，Swarm使用了Docker API意味着你可以共用命令的配置。个人认为，我倾向于押宝于Docker引擎变得越来越好，这样Docker Swarm也会受益。这两者之间的差异已经非常小。两个都是可用于生产环境，但是Swarm更易于去配置，易于使用，并且可以重用在上集群之前的配置，不需要在集群和非集群环境下重复工作。   我个人的建议是使用Docker Swarm。而 Kubernetes太“任性”了，不易于配置，与Docker CLI，API差别太大，并且在Docker1.0之后，相对于Swarm来说没有太多的优势。他们之间其他的差距影响真的是不太大。但是Docker Swarm更易于配置。   其他      使用kubernetes的好处是在其前后基于Google对container管理几十年的经验，比如Borg。   使用某个特定容器其实并不是特别重要的事，最主要的还是集群技术。kubernetes类似于数据库操作领域的hibernate/ibates，即解耦合。在需要的时候我们可以使用Rocket替换Docker，对使用者透明。kubernetes这个功能在swarm中是找不到的。在生产环境中，如果需要，对于上层是透明的，没有任何影响。   不得不承认，Swarm更多的是一个部署工具，而Kubernetes是用于HA（高可用）架构的大规模的编配平台。   Kubernetes是介于中间的产品，没有Swarm那么简单友好，也没有Mesos那么功能强大。因此很有可能是在Mesos上层使用Docker，而非与Kubernetes集成。   人们使用Mesos是因为它本身不是为了Docker或者容器设计的，它只是个集群抽象层。人们用就是因为它是唯一一个既支持部署应用程序又可以同时管理hadoop。（将来有可能不一定）   如果我是个开发人员，我会选择 Compose+Swarm,它简单易用。但是如果我是CTO，我会选择k8s，因为这样我就不用关于Docker API的兼容性问题了。   原文地址 ： https://technologyconversations.com/2015/11/04/docker-clustering-tools-compared-kubernetes-vs-docker-swarm/   ","categories": [],
        "tags": ["Mac","shortcut","DevOps"],
        "url": "/2016/09/04/Kubernets-vs-Swarm.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Head First Blockchina 1",
        "excerpt":"深入浅出区块链系统：第一章.  what you should know about blockchain   考虑的大家现在很多都是碎片化阅读，不知道大家如何，反正如果我是在在只有很短一段时间里，不太容易切换状态静下心来读一篇洋洋洒洒的长文章。因此此系统会保持短小精悍，把整套分成一系列小文章，每个文章有分成若干个段（sections）。既KISS原则，查看这里什么是KISS.   目前区块链（blockchain）应该是在最近比较火的新技术之一了，这个不光在银行金融行业，也在其他诸多行业逐渐成为热点。区块链基于被认为是继互联网之后最重要的技术发明之一。看到过很多关于区块链的文章，要么是太过于学术，要么又局限于某个行业领域。对于一般人来说不太容易去理解其所以然。因此，笔者提笔自己写一个关于区块链的系列文章，以方便大家。   比特币  说到区块链就不得不提比特币。在进入讨论blockchain之前，先上张图片。    这个是在说雷曼兄弟公司的破产，背景是发生于2008年金融危机。当时被媒体及世人厌恶的贪婪，低效的传统金融体系垮塌，与此同时，不知道是不是巧合，比特币（bitcoin）诞生了（其实电子货币已经有几十年的历史了）。      比特币，就像是美元，人民币一样是个货币，只是这个是数字化的货币，没有一个具体的国家或者机构管理。既然是货币就要拿来用，要流通。当有任何变动，就会产生所谓的 money in money out， balance changed。即这些比特币的变动，最新的余额是多少，等都要记录下来。在现实世界中，这些记录在具体你的每个银行账户中，由一些监管机构监视并确保准确性。   但是比特币是个电子货币，没有一个具体的营业监管机构。怎么破？   这时区块链就被发明出来（大约是在2009年），区块链起源于比特币，就是当比特币从一个人转移到另一个人时，用于记录这些变动。换句话说，区块链（Blockchain） 就是比特币（bitcoin）的不可变动（immutable）的记账系统。   记账  有人可能会觉得“记账系统”太过于笼统，不太明白，因此首先这里说一下什么是记账，举个例子，你在淘宝上买东西，整个过程需要有多个记账操作，包括可能需要在购物车里添加一条记录，你买好了需要在商家那里记一笔账，然后支付时需要记下一笔，快递再需要记录一笔。如果你买的不是一般的小东西，比如是一个房子，那样还要在相关管理部门还要记录一笔。 有没有发现这个是非常低效的，需要花费很多的重复资源的过程？换句话说，这些低效都是最终都要转嫁到我们消费者头上。区块链却在设计之初很好的解决上面这些弊端。   什么是区块链   区块链是专门针对比特币设计开发的记账系统，用于所有比特币的记账。因为区块链本身良好的设计，区块链被服务于比特币仅仅只是一个用例和开端，其还可以用于很多地方。   记住这几个词可以帮助理解什么是区块链     chain/链。 像下面这个图，数据的组织是由一个一个大小相同的块(block)组成一个链条(chain)，就像是DNA里每个基因有机的组织在一起。   .   下面使用一个例子来解释一下。当有如下三个操作时就会在区块链中添加三条记录。               去中心化，或者说是“分布式” ， 也就是具体的这些记账数据是分布式的散落在各个节点，而且每个节点都存有一份所有的交易。这样有个好处，就是每个交易有多份副本，互相之间可以对比查对，那些欺诈，篡改数据就没有可能了。其实，传统金融业有一个问题就是各个金融机构间互相的不信任，想像一下在2008年金融危机时，各个金融机构竞相抢购那些credit default derivatives等产品，就是因为大家对对方的不信任所引起的。            挖矿。 “矿工”，其实就是链中一个个能够保存对账信息的节点的别名. 当有新的对账或者交易时，应该就是把数据写到某个节点，然后再需要加入到区块链中时。但问题是“链”只有一个，节点有很多，到底由哪个节点来完成这个任务呢？解决办法就是“打”，看谁厉害。其实就是许多的矿工节点会互相竞争，使用一些非常耗费计算资源，后台使用复杂的算法，最后使用一种叫做 PoW （Proof Of Work,是一种快速断定工作量的技术，比如你安排工人来给把一车箱子从仓库搬到车间，你并不需要从一开始紧盯着他去搬每一个箱子，只要看到最终的所有的箱子都已经在车间，即可证明他做完了工作，可以给相应的报酬了。这个我们在后续章节详细解释）的机制来决定最终哪个节点获胜，由它有资格来写这个区块，并加入到区块链中，同时这个获胜的节点可以得到相应财务上的奖励，即若干的比特币，这也正是不断激励人们投入更加强大计算能力的机器来挖矿的原因。这个过程被比特币平台很好的控制节奏，也就是大概每10分钟左右产生一个新的“区块”以添加到区块链路中。       可以参考https://blockchain.info/?currency=CNY， 这里是以人民币滚动显示当前所有挖矿的更新，下图是此屏幕截图。    区块链的应用领域   金融业  对于金融业来说，在进行远程转账时一直在使用的所谓“关系银行”，比如你想要通过中国工商银行给朋友在澳洲的汇丰银行的账户转账，这时在中间可能要经过在香港的汇丰以及悉尼的银行等多个第三方机构来中转，不光要多花手续费，真正拿到手时可能已经1周时间过去了。如果使用区块链，转账其实就是添加一个“对账”信息块并加入到区块链中即可，对方银行可以立即在链中发现此交易。这样此过程就流水化（streamline）了，就跟发个电子邮件似的。 相对于之前，区块链会有3个优势，（1）不需要经过第三方 （2）快 （3）省钱      一般商业公司  设想一下普通办公场景，一般业务处理都会涉及到许多纸制的表格，文件，请多文件的复印件等等。一是方便文件信息的传递，记录，另外一个原因是为了应付内部外部的审计。如果使用区块链，这些问题都得到很好的解决。比如，由于区块链的系统架构，其本身数据就是自动审计的。简言之，在这个领域，区块链有3大优势，（1）数据透明（2）数据安全性验证（3）审计。   对于零售行业  比如说你想知道这个食品是不是有机食品，而每个环节的数据都是散落在各个地方，不便于统一追踪。另外，数据的来源又是多种多样，又容易被篡改，比如作为一个钻石加工商，我是无法确定这批钻石是不是血钻。而“区块链”本身的特性保证了可以跟踪产品生命周期的每个阶段详细信息，而且区块链的“只能添加”的属性也确保了数据数据的准确性，不容易被后期篡改。因此区块链也可以用以政府类的投票，这样就更加具有合法性，不可能被人为篡改结果。   小结  以上的案例都涉及到一个关键词 “信任”。 你不信任供应链路，你不信任 “相关银行”， 等。但是你可以信任 “区块链”，它是允许多个不同的机构一起协同工作，但是不需要他们之间相互信任。   FAQ （常问的问题）   这些 “区块” 具体是什么样子的？  首先每个区块包含有一个时间戳，包含一个哈希码，指向其前面链接的区块，然后就是对账交易数据本身。每个区块都有一个唯一编号，生成这个编号是需要大量的计算工作及验证。我们在后续章节详细介绍此功能。   什么样新的块才能够加入  当需要添加新的块时，需要有所有节点中50%认为正确同意才可以。这样可以保证恶意的数据被加入到链中。   每个节点都存一份不也是有额外的成本吗  其实在2010年，1P (Peta byte)数据存储一个月是 $80,000/month,预计在2020年，也就是10年后，同样的1P的数据存储一个月只要 $4/month. 可见，存储本身的成本几乎可以越来越忽略不计了。   总之，BlockChain的出现，是由于人们的预期， 技术的进步。   所有上面提到的东西，包括此文章的markdown源代码，mindmap思维导图等等都可以在我的github上找到。此文章是我在GitBook上此系统的第一篇，链接。如果有任何建议或者想法，请联系我。   联系我：     phray.zhang@gmail.com (email/邮件，whatsapp, linkedin)   helloworld_2000 (wechat/微信)   github   [简书 jianshu]（http://www.jianshu.com/users/a9e7b971aafc）   微信公众号：vibex  ","categories": [],
        "tags": ["blockchain","hyperledger","MyBlog"],
        "url": "/2016/09/11/head-first-blockchian-1.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "JetBrains/IntelliJ tips",
        "excerpt":"Shortcuts   Open files by name   To navigate to a class, file or symbol with the specified name:   On the main menu, point to Navigate, and then choose Class, File, or Symbol respectively, or use the following shortcuts: Class: ⌘O File (directory): ⇧⌘O Symbol: ⌥⌘O   Search keywords in files   Shift+Command+F: Fine in path   Re-open files   Command +E  ","categories": [],
        "tags": ["Coding","JetBrains","Java"],
        "url": "/2016/09/14/JetBrains-Tips.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "浅谈软件单元测试中的“断言” (assert)，从石器时代进步到黄金时代。",
        "excerpt":"大家都知道，在软件测试特别是在单元测试时,必用的一个功能就是“断言”（Assert)，可能有些人觉得不就一个Assert语句，没啥花头，也有很多人用起来也是懵懵懂懂，认为只要是Assert开头的方法，拿过来就用。一个偶然的机会跟人聊到此功能，觉得还是有必要在此整理一下如何使用以及对“断言”的理解。希望可以帮助大家对此有一个系统的理解，也趁机聊聊“断言”发展一路过来的心路历程。   基础知识  首先稍微介绍一下断言相关知识，对于有经验的程序员请移步到下面的“断言”进化史部分。   什么是断言  在单元测试时，程序员预计在程序运行到某个节点位置，需要判断某些逻辑条件必须满足，这样下面的一些业务逻辑才可以进行下去，如果不满足，程序就会”报错”甚至是”崩溃”。比如说，一段程序是负责“转账”，在真正开始转账操作前首先需要“断言”这个账户是一个“合法”的账户，比如账户不是null。当出现些状况时，程序开发人员就可以在第一时间知道这个问题，可以去debug除错，而非等到交付给用户后才发现问题。其实这个功能是TDD (Test Driven Develop)的基石之一。   “断言” vs “异常”或者错误， 即 Assert vs. Exception/Error     “断言”通常是给程序开发人员自己使用，并且在开发测试期间使用。而异常等在程序运行期间触发   通常“断言”触发后程序“崩溃”退出，不需要从错误中恢复。而“异常”通常会使用try/catch等结构从错误中恢复并继续运行程序。   “断言”进化史   “石器时代”   一开始的一些单元测试框架（比如JUnit）提供的断言语句，这样在程序某个地方确保某个逻辑关系肯定返回是true,如果不是true,这个单元测试就是没有测试通过。如下就是一个例子,如果程序运行到此行时返回false程序就会抛出一个错误（如下图一）并停止运行，开发人员可以去检查下为什么出现此问题。非常的简单粗爆。   assert(x=y);      “青铜时代”   上面这种断言除了简单之外，是有一个问题，就是当断言被触发时显示出来的错误消息不是很友好。如上图一，只是知道出错了，但是并没有太多有用的信息，比如最好是能显示出x与y的值来，这样好更快的理解为啥出错。后来，支持断言的单元测试框架升级版本出现了，它们提供了一系列的高级”断言“语句，添加了一些更加友好的程序接口，同时还提供比较亲民的错误消息，比如下面的例子使用了两个单独的断言语句。   int x=111; int y=222;       assertEquals(x, y); assertNotEquals(x, y);   执行的结果如下图二，你可以看到这个错误结果相对于上面“石器时代”已经包括了不少有用的信息，比如除了知道断言失败外还显示了期望的值以及实际值。      “黄金时代”   但是上面这种方式有一个弊端，就是需要大量的预置断言方法（比如判断相等一个方法，判断不相等一个方法等），去支持各种场景。接下来又出现了新的解决方案，其中的明星就是Hamcrest (其实这个词是使用一种叫做angram的文字游戏，即把一个原来单词中的字母顺序改变，这个Hamcrest就是从Matchers的变形)框架。是使用一种assertThat组合上Matcher来使用。   这个有多个好处，     首先是支持了在Java8中才迟迟引入的流式编程(Stream)，即每个Matcher执行完后会再返回一个Matcher，这样可以一个套一个组成一个Matcher链   另外Hamcrest还使用了非常接近于人类自然语言以及使用and/or/not等逻辑判断的方式来写测试方法，比如当你看到下面的测试语句肯定会一目了然：   assertThat(actual, is(not(equalTo(expected)));      还有一个好处是输出的断言消息更加易读。   另外还有一个好处即Hamcrest框架支持泛型TypeSafe，即在编译时就会找到类型不匹配的错误。比如下面第一个是传统的断言，在编译期不会报错，但是运行时会失败，而第二个会在编译时报错，就不用等到运行期。   assertEquals(\"abc\", 123); // 1 assertThat(123, is(\"abc\")); // 2         使用Hamcrest的最后一个好处是对测试框架的“解耦合”，即，使用此框架你可以现在使用Junit后面可以转到TestNG。甚至你自己去扩展自己实现。   总结   上面说了这么多，是不是感觉平时经常使用的一个看似简简单单的Assert还有不少的东西可以深挖一下滴。这个只是抛砖引玉，如果大家还有什么点子或建议请使用下面的方式。      联系我：         phray.zhang@gmail.com (email/邮件，whatsapp, linkedin)     helloworld_2000 (wechat/微信)     blog on github pages     简书 jianshu     github     微信公众号：vibex      Reference:      benefit of assertThat   ","categories": [],
        "tags": ["java","testing","MyBlog"],
        "url": "/2016/09/17/Assert-In-Unit-Test-CN.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "How to customize Sublime syntax highlights",
        "excerpt":"Reference     Sublime Scope Naming   Syntax Guide  ","categories": [],
        "tags": ["tag","efficiency"],
        "url": "/2016/10/06/Sublime-Syntax-Highlights.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "【原创】深入浅出区块链系统：第二章",
        "excerpt":"   使用Solidity创建以太坊(Ethereum)智能合约(Smart Contract)    引言  前面第一章 (位于微博上的链接)主要介绍了区块链的概念，我们知道区块链分为两大类，一是以公有链为代表的无权限控制区块链，第二是有权限控制的区块链，这个又包括了私有链(Private Blockchain,以Overstock为代表)和联盟链(Consortium Blockchain,以R3为代表)，相对于公有链来说，这些链一般都是没有电子货币，因为他们不需要像公有链那样要靠电子货币作为挖矿的奖励来激励参与，所以速度也是比较快的。   上一章都是讲的抽象的概括，下面我们就深入讲一些具体的东西，这样以便于大家有一个形象的概念，方便理解。我们这一章主要讲讲公有链，以方便讲解以及大家去继续研究，尝试，这里选择在公有链领域社区最为活跃以太坊（Ethereum），对于中国用户来说，其于2016年9月19号刚刚在中国上海举办了DevCon 2区块链峰会，很多人可能有所印象。第一步，这个东西怎么读啊？其实这是新构造的一个单词，而非一个已有的英语单词，其读作[i’θi:’riəm]。接下来我们会一起过一下涉及的一些概念，后面我会介绍几个如何进行太坊开发的技术工具，以及两个比较好用的应用框架。   大家都知道，学习一个新技术最好的方式就是亲自动手试一把，几乎学习所有新编程语言上来都会写个HelloWorld并运行一把，在这一章最后一部分我会手把手的带领大家创建并运行一个智能合约。   概念   Ethereum:   Ethereum (官方链接) ，是个区块链公有链解决方案，如果比特币的区块链称作区块链1.0的话，那Ethereum可以称为区块链2.0 。 其主要特色就是支持可编程的智能合约。这个开源的系统相当于计算机中的操作系统一样，其是一个平台，提供了API及接口，以供其上运行不同的程序共享使用。同时因为它本质上是去中心化的区块链，因此号称是零宕机，零审查，以及不会有欺诈与人为篡改。就像所有的公有链需要激励机制的“代币”一样，它除了底层的区块链外，还有自己的加密电子货币，Ether，即以太币，在国内有些人戏称为“姨太”。目前一个“姨太”大约11美元，实时的价格趋势可以参见 这个交易所链接      智能合约   智能合约 (解释链接 )，其实这个概念本身是远远早于区块链产生的（早在1994年就出现了）。智能合约，说白了就是自己写的一段代码放到区块链上，在这里可以添加自己需要的业务逻辑等，只是这段代码在创建后不像传统应用是部署到服务器上运行，而是放到区块链上，并且自动执行(其运行部署都会消耗Gas(气，也就是若干的以太币))。各个参与方不需要像以前需要一个或若干个中心节点/服务器，大家都各自在自己那里完全按照“合约”执行，中间没有人可以去篡改或者停止，此设计会大大提高flexibility(灵活性）以及互相不信任的问题。比如有一个智能合约定义的逻辑是：当A收到钱后，B就会收到货物，这些操作都是按照合约自动执行，中间不再会有违约或者被人为修改的风险。   这些智能合约是以DAPP (Decentralized  Application)的形式存在。智能合约是部署在区块链上，由于区块链的透明性，这些合约对任何人都是可见的，当然这个有利有弊。如果其有bug或者漏洞，就有可能被人抓住并利用，比如2016年6月的The DAO攻击，就造成相当于5千万美元的以太币丢失，这也直接导致了以太坊后面的一次更分叉，这块笔者后面会撰文详解。   Web 3.0  大家可能听说过web 1.0， 其是指之前传统的网页技术，比如HTML，传统的JavaScript,VBScript,CSS。而web 2.0 是使用所谓的DHMTL,HTML5, Ajax,等众多的JavaScript技术，来创作类似于桌面程序效果般的网页应用。Web 2.0这些技术有个问题，就是过于依赖中心化的服务器/第三方机构，比如除了其应该做的提供网页访问服务外，还有验证，用户行为记录分析等。 而这里提出的web 3.0是有如下几个特性，首先是去中心化，比如通用的后台端，使用Swarm与bzz来作为内容寻址的存储系统，基于区块链的共识形成机制，基于Whisper的异步消息机制等，这样具体的业务逻辑都会分发到每个客户端去执行，而非位于昂贵且易于出问题的少数中心节点。   这是刚刚提到的架构图      Solidity  上面说到的这些智能合约一般来说是使用一种特殊的编程语言来创建的，即Solidity，这个语言是以太坊提出并创造的，面向对象的DSL特定领域编程语言(Domain Specific Language),它是以太坊支持的4种语言（另外三个是Serpent, LLL 和 Mutan），只不过其是最流行的一个语言。从技术上来讲，solidify源代码会编译成字节码，然后运行于EVM（Ethereum Virtual Machine）上面。如果你看到源代码后就会觉得其实Solidity是与JavaScript十分类似的语言，如下是一段代码：      Gavin Wood (Solidity之父)说Solidity就是根据ECMAScript（是JavaScript,ActionScript等的标准祖先）所创建的，这样对于大多数开发人员来说学习曲线会很平滑。   开发工具   由于发展时间不是很长，目前市面上可用的开发环境IDE不太多。下面介绍下稍微比较成熟可靠的开发工具。   Microsoft Visual Studio Ethereum 插件   没错，就是那个市面上已经非常常见的visual studio，也就是dot net的开发工具，不是一个全新的开发工具。此开发集成环境只需要安装solidify插件即可。 这个也从侧面可以看到微软对于以太坊以及区块链的野心。   安装此插件后在微软的Visual Studio后就可以在新建项目时的模板里看到这个Solidity 选项：      当选择此模板后，visual studio他会自动构造出一个应用的基本文件结构。这样你可以省去一些每次开发一个智能合约都要重复的工作。你就可以集中时间精力到真正业务代码上。   如下就是这个IDE自动生成的代码      Ethereum Studio   除了背靠微软这个大旗的visual studio集成开发环境外，还有一个方便大家使用的免费的IDE。这个是基于Cloud9平台的一个在线IDE，其完全运行于浏览器中，不用安装，可以用于任何的操作系统。如下就是这个在线集成开发环境的样子。这个还是比较推荐的开发环境：      智能合约应用开发框架   目前比较常用的智能合约构架有如下几个，都是开源并且免费的。这里我们来手把手的创建并运行一个智能合约，来体会一下。   Embark   首先推荐的是这个叫做Embark的框架，他是一个让你可以轻松开发部署Dapps的平台，它支持的功能包括，在JS代码中部署智能合约，智能合约的热部署，可以集成grunt等构造工具。支持TDD（即测试驱动的开发）比如支持mocha等测试框架，可以方便的使用IPFS等去中心化的系统，支持增量，智能的部署修改过的智能合约等。这个工具是使用nodejs写的，因此你需要先安装nodejs的环境。这个平台会在你本地启动一个区块链服务模拟器，这样你就可以完全在本地开发测试，大大提高了工作效率。如下是启动后的截图。Embark的安装及源代码位于Gitub这里   首先你需要来安装Embark以及区块链模拟器。   # 安装Embark npm -g install embark-framework  # 安装区块链模拟器 npm -g install ethersim  # 启动RPC模拟器 embark simulator   启动的模拟器是下面这个样子    然后我们去创建一个新的智能合约：   # 创建一个叫做demo的智能合约基础结构 embark demo # 进入这个目录，下面含有配置文件 embark.yaml cd embark_demo # 启动应用 embark run  启动后,首先在后台看，Emark帮忙使用coffee script等构造并部署了合约。      你可以使用浏览器试验一下，比如打开http://localhost:8000，然后你可以试着输入个数值，去试试看看它是不是已经能够响应处理你的输入了：      是不是很神奇，短短两三分钟，已经从零开始构造出一个可以运行的以太坊DAPP ，并运行于区块链之上。 接下来我们介绍另外一个框架选择方案。   Truffle   Truffle,是跟前面提到的 Ethereum Studio 同一个公司（ConsenSys）开发的一个框架， 这个跟前面的embark类似，也是可以提供一个智能合约的开发测试平台,他的一个特色就是它可以集成nodejs里面强大的测试功能，比如Mocha, Chai等等. 像Embark一样，你需要另外安装运行其他软件，来启动以太坊客户端模拟器，最常用的是EthereumJS TestRPC  Github link, 它会在内存中启动一个Ethereum的客户端， 这样可以快速测试你开发的应用。   因为这个也是使用nodejs创建的应用，因此使用如下命令来安装此程序，安装好了启动此应用   #安装以太坊模拟器 npm install -g ethereumjs-testrpc # 启动模拟器 testrpc   启动后是这样子的    模拟器启动好了，接下来执行下面的命令来初始化truffle应用。  mkdir firstApp cd firstApp truffle init  上面最后一个命令就会自动帮你构造好的程序框架，包括一些最基本的JavaScript文件，几个智能合约源代码，主应用程序的HTML代码及配套的CSS等文件 。如下是这个基本框架：    接下来你可以添加自己的代码到contracts目录下的智能合约文件，也可以什么都不动，因为truffle已经自动生成了最基本的框架。   #这个命令会把智能合约源代码编译成字节码 truffle compile   编译好的代码需要部署到区块链上才可以执行，在truffle中这个工作是由migrates目录下定义的migrate作业执行的，我们去修改文件2_deploy_contracts.js为如下：  module.exports = function(deployer) {   // deployer.deploy(ConvertLib);   // deployer.autolink();   // deployer.deploy(MetaCoin);   deployer.deploy(HelloEthereum); };   然后执行如下命令去执行代码部署，它除了把你的智能合约发布到区块链之外，还会做一些相关工作，比如link用到的library等。deployer可以使用promise的方式 (e.g. .then(function(xx)))来执行其他额外的工作等，比如创建一个其他的合约并调用，等。这个便于你来灵活的扩展应用。  truffle migrate truffle build   结语   好了，至此我们已经了解了什么是以太坊已经其上运行的智能合约，DAPP等概念。后面又介绍了开发智能合约的工具已经可复用的框架，最后又手把手亲自做了一个智能合约。这样大家应该对区块链以及以太坊等公有链有了一个形象具体的感觉了吧。如果这里有什么问题或者建议，欢迎通过下面的联系方式与我沟通。   Referece     Introduction to Smart Contracts   Installing Solidity   以太坊官方教程   Swarm   Truffle Official Doc   联系我：     phray.zhang@gmail.com (email/邮件，whatsapp, linkedin)   helloworld_2000 (wechat/微信)   github   [简书 jianshu]（http://www.jianshu.com/users/a9e7b971aafc）   微信公众号：vibex   webo/微博: cloudsdocker  ","categories": [],
        "tags": ["blockchain","ethereum","MyBlog"],
        "url": "/2016/10/13/head-first-blockchian-2.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Head First Blockchina 3",
        "excerpt":"   Hyperledger Fabric for Mortals    HyperLedger, from Arvind Krishna, Director of IBM research lab, meant to cross borders. e.g. for credit ledger, the importer and exporter in China need to make preparations few months beforehand, this is similiar to Marco Polo went to China 300 years ago.   Another usage in stock market is stock loan, by using blockchain, we are going to know whose the stock came from. And the borrowers need to know who can borrow, and I need to prepare collaterials, need to check whether it comply with the processes, when I pay it back, etc. there are lots of things. right now they are handled by people manually, so going forward, if I have some stocks and I want to borrow it to earn some money, those can be programmed in blockchain. they’ll be processed by machines automatically.  this will help to show there are no inside tradings, etc.   but initially as blockchian are annoymous, we are not sure whthere there are any money laundering, regulators may have concerns. So we need permisisoned blockchains. we need to one identity put aside of the transactions, so if reuiqred, can be recorded and checked, e.g. whether this is dealing with corrupted governments, etc. in addtion, it’s not requried that only one player or government to provide identity, there are many entities can provide identity service, but off couse need to follow some standards agreed.   JPMorgan Chase &amp; Co has announced its second entrant into the blockchain fray, this time utilizing the smart contract enabled ethereum blockchain in creating a system called Quorum.   前面第一章讲过了基本概念，可能有此抽象。接下来我们来看一些目前已经存在的真实项目和案例，好有一些形象的理解，以便为后面深入一些概念准备。      JPMorgan 的 Juno   Ripple 全球化交割网络   R3 DEV   以态坊   万象Lab   All of the advantages derived from basic blockchain technology can be boiled down to only two benefits; corruption resistance and redundancy.   All of the advantages derived from basic blockchain technology can be boiled down to only two benefits; corruption resistance and redundancy.   The crux of the issue comes down to whether or not private blockchains can ever be made secure enough to use for large amounts of value. No hacker is going to bother to attack your blockchain if it’s only being used for bingo night at a retirement home. However, the moment the world finds out that your blockchain has millions of dollars worth of payments flowing across it, you’ve basically just launched the latest hackathon, complete with a multi-million dollar, winner-take-all grand prize.   For instance, a public blockchain is a transparency engine. In Vitalik Buterin’s blog post ‘On Public and Private Blockchains’ ‘On Public and Private Blockchains’ written last August, he pointed out that public blockchains “protect the users of an application from the developers, establishing that there are certain things that even the developers of an application have no authority to do.” A good example of this is a user of a social network or some other membership site where the owner changing   http://www.youtube.com/watch?v=810aKcfM__Q   The main takeaway is that permissioned blockchains create an environment where malware has an advantage,so security problems are constant and sometimes completely overcome your network.   First of all, the transaction speed of a privately-run blockchain can be faster than any other blockchain solution, approaching even the speeds of a normal database that isn’t a blockchain. This is because there are few nodes all with high trust levels. No need for every node to verify a transaction, in fact, they’re all mostly trusted so there is no need to do all of the meticulous work.   Complete agreement between nodes isn’t required, so fewer nodes need to do the work for any one transaction. Lastly, and perhaps most importantly in the current environment of banks embracing private blockchains so readily, choosing a private blockchain can help protect their underlying product from disruption.   Lastly, there is the desktop route, deploying a private blockchain on your desktop computer, even in a windows environment, with Multichain. It allows rapid design, deployment and operation of private blockchains to your custom specification.   http://r3members.com/   like unique keys and check constraints cannot protect a database against malicious modifications. The bottom line is this: We need a whole bunch of new stuff for shared write databases to work, and it just so happens that blockchains provide them.   Some key elements include regular peer-to-peer techniques, grouping transactions into blocks, one-way cryptographic hash functions, a multi-party consensus algorithm, Some key elements include regular peer-to-peer techniques, grouping transactions into blocks, one-way cryptographic hash functions, a multi-party consensus algorithm,   These types of rules can be expressed as bitcoin-style transaction constraints or Ethereum-style enforced stored procedures (“smart contracts”), each of which has advantages and disadvantages.   One key difference is that private blockchains don’t need proof of work mining,since blocks are created by a closed set of identified participants.   First, fear of the loss of raw power they would sustain if they went with public blockchains. Second, the fear of being supplanted by a new rising, bitcoin-centric financial industry. In fact, they would prefer that blockchain technology had never been invented.   区块链都足以产生影响和改变金融业的力量。这是由于区块链具有以下特点：（1）去中心化：区块链技术不依赖额外的第三方管理机构或硬件设施，没有中心管制，除了自成一体的区块链本身，通过分布式核算和存储，各个节点实现了信息自我验证、传递和管理。（2）开放性：区块链技术基础是开源的，除了交易各方的私有信息被加密外，区块链的数据对所有人开放，任何人都可以通过公开的接口查询区块链数据和开发相关应用，因此整个系统信息高度透明。（3）独立性：基于协商一致的规范和协议（类似比特币采用的哈希算法等各种数学算法），整个区块链系统不依赖其他第三方，所有节点能够在系统内自动安全地验证、交换数据，不需要任何人为的干预。（4）安全性：只要不能掌控全部数据节点的51%，就无法肆意操控修改网络数据，这是区块链本身变得相对安全，避免了主观人为的数据变更。（5）匿名 区块链都足以产生影响和改变金融业的力量。这是由于区块链具有以下特点：（1）去中心化：区块链技术不依赖额外的第三方管理机构或硬件设施，没有中心管制，除了自成一体的区块链本身，通过分布式核算和存储，各个节点实现了信息自我验证、传递和管理。（2）开放性：区块链技术基础是开源的，除了交易各方的私有信息被加密外，区块链的数据对所有人开放，任何人都可以通过公开的接口查询区块链数据和开发相关应用，因此整个系统信息高度透明。（3）独立性：基于协商一致的规范和协议（类似比特币采用的哈希算法等各种数学算法），整个区块链系统不依赖其他第三方，所有节点能够在系统内自动安全地验证、交换数据，不需要任何人为的干预。（4）安全性：只要不能掌控全部数据节点的51%，就无法肆意操控修改网络数据，这是区块链本身变得相对安全，避免了主观人为的数据变更。（5）匿名.  正是这些优势特点，决定区块链技术能够提高系统的追责性，降低系统的信任风险，对优化金融机构业务流程、提高金融机构的竞争力具有相当重要的意义；通过使用区块链技术，金融信息和金融价值能够得到更加严格的保护，能够实现更加高效、更低成本的流动，从而实现价值和信息的共享。   区块链技术可通过程序化记录、存储、传递、核实、分析信息数据，从而形成信用，可以大量省去人力成本、中介成本，信用记录完整、难以造假，同时摧毁某些节点对系统没有影响。目前最大区块链比特币链存在费用增加、容量限制、确认时间变长、能耗走高的缺点。但例如Ripple、以太坊等另类区块链，以及公共、私有、联盟链等多种形式的涌现将区块链在金融领域造成颠覆式创新变成可能。   https://ripple.com/   只要诚实的节点所控制的计算能力的总和，大于有合作关系的(cooperating)攻击者的计算能力的总和，该系统就是安全的   Retail Giant Overstock to Issue its Own Stock on Blockchain Platform   Referece     Hyperledger Whitepaper   About blockchain   IBM Blockchain for developers   Getting started with IBM Blockchain   IBM Blockchain 101: Quick-start guide for developers   ","categories": [],
        "tags": ["blockchain","hyperledger"],
        "url": "/2016/10/19/head-first-blockchian-3.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Simpler chronicle of CI(Continuous Integration) “乱弹系列”之持续集成工具",
        "excerpt":"引言  有句话说有人的地方就有江湖，同样，有江湖的地方就有恩怨。在软件行业历史长河（虽然相对于其他行业来说，软件行业的历史实在太短了，但是确是充满了智慧的碰撞也是十分的精彩）中有一些恩怨情愁，分分合合的小故事，比如类似的有，从一套代码发展出来后面由于合同到期就分道扬镳，然后各自发展成独门产品的Sybase DB和微软的SQL Server；另外一个例子是，当时JBPM的两个主要开发的小伙伴离开当时的RedHat，在JBPM基础上自立门户新创建的Java工作流管理软件Activiti，等等。在持续集成工具龙头老大这个宝座，也曾经发生过合作合并，吵架分家，再对着干的事情，今天分享一下这前前后后有趣的故事。   DevOps  首先，防止__先入为主__,以为大家都知道这个那个的。先普及下相关背景知识，如果已经了解的同学可以跳过。目前在软件工程领域已经火了好几年的DevOps领域，核心的模块就是CI与’CD’，即Continuous Integration与Continuous Deployment,也就是持续集成与持续部署，这个对于处于敏捷开发环境下尤其是互联网等需要高速迭代是个核心的功能，可以说没有CI，就不可能达到像Google或者Facebook这些一天有多个release的情况。   CI  CI(Continuous Integration) 持续集成起源于 XP(极限编程)与 TDD (Test Driven Develop)也就是__以测试驱动__的开发模式，是防止出现所谓的’集成地狱’,即防止程序员在正常编码工作中，需要写新的业务逻辑，添加新的代码，但是同时也新引入了bug。CI会持续的（重复的）进行一些小的工作，比如不断的跑测试用例，去扫描代码等工作。以减轻或者很大程度上避免这个个新引入的bug对软件交付质量引起的负面影响。目前，市场上有很多的CI解决方案及工具，常用的如下几个，        CI 的进化史  世界上本来没有CI,用的人多了也就成就了CI。本来软件工程里是没有这个概念的。最开始，就像下图中描述的帝国时代里，整个社会节奏平稳而缓慢，每个程序员自己做自己的开发，然后各自把自己的工作上次（提交），整个团队把代码放在一起，然后整个人过来，启动make/build，后面有个人去把编译好的代码放到测试机器上，每个程序员自己或者单独的测试团队去测试程序，如果没有问题，另外的人去发布到生产环境上。这些都是或多或少由人手工去做的。      但是就像很多人类的发明就是为了人类”偷懒”一样，CI慢慢在一些想偷懒的牛人脑子里形成。这其中就有Kent Beck （多说一句，这个现在工作于Facebook的牛人，还发明创造了很多到现在还在流行的东西，比如Agile敏捷开发，以JUnit为代码的xUnit测试理念，TDD测试驱动开发等等），在上个世纪最后几年，Kent Beck创造了XP（注意这个不是Bill的那个XP操作系统），是eXtreme Programming，即极限编程。虽然现在看起来极限编程有很多很诡异不太现实的方式，比如两个程序员坐在一起，使用一台电脑一起写一段程序等天马行空的想法。但是其中一个理念就是“持续集成”（CI)。以此理念，后面出现了使用各种语言写的CI的工具，其中的老大是CruiseControl。这个就像是上图中那个跑车一样，在当时整个缓慢的大环境下其提升工作效率的效果十分的吸眼。      到了2005年，当时就职于Sun(没错，就是创造了Java的那家公司)的一个叫川口浩介（Kohsuke Kawaguchi）的日本人，就是上图这位“霓虹金”，敢于冒险，重新“发明轮子”，不顾如日中天的CruiseControl，设计并开发了一个新的持续集成的软件，起名叫做Hudson。它提供了很多强大的功能，比如提供插件机制，这样就使其几乎集成了市面上所有的源代码管理工具，比如CVS, Subversion, Git, Perforce等。除此之外，它还提供了界面的扩展能力，另外还支持基于Apache Ant 和 Apache Maven的项目，除了xNix,还支持Windows环境等一众强大功能。听起来这么牛逼的工具，很快，在大约2007年的时候Hudson已经超越CruiseControl。然后在2008年5月的JavaOne大会上，Hudson获得了开发解决方案类的Duke’s Choice奖项。从此，小弟翻身做大哥，Hudson成为CI的代名词。其主要开发者 Kohsuke Kawaguchi 还获得了Google-O’Reilly Open Source Award。他后来也不用自己苦逼的写代码了，只要到处受邀去演讲做是如何受什么启发创造并发明了这么好的工具，造福大批程序员。再后来他还离职创立了公司CloudBees，出任CEO，迎娶白富美，走上人生新巅峰。（也难怪上图中他笑的如此开心）   一切看起来都是那么美好。但是，天有不测风云，在2009年6月，Oracle收购Sun，所有人都蒙逼了，是不是写反了？一个传统数据库的公司收购了在Java及开源老大的Sun？！！这个消息公布之后，两个公司内部各个产品及项目就被整合，调整，Hudson也不例外。这也就算了，反正谁给钱不是干活哪，但是在2010年9月，Oracle竟然暗戳啜的把Hudson变成了注册商标。2010年11月，Hudson社区的核心开发人员发现了这个事情，他们觉得这对于一个一直标榜自己是开源CI领域“诚实可靠小郎君”的Hudson来说是个玷污。双方进行了会谈，过程不太友好，然后就不出意料的谈崩了。2011年圣诞节过后，几个秃顶的大叔觉得不要再跟Oracle的律师在这里瞎扯淡了，他们决定自立门户，自己起个新的名字叫Jenkins。然后凑钱注册网址，买服务器，列出下面的清单，统统改名，     hudson-labs.org -&gt; jenkins-ci.org   @hudsonci -&gt; @jenkinsci   http://github.com/hudson -&gt; http://github.com/jenkinsci   hudson-dev -&gt; jenkins-dev   hudson-users -&gt; jenkins-users   hudson-commits -&gt; jenkins-commits   hudson-issues -&gt; jenkins-issues   然后把代码fork出一份来（这里好笑的是Hudson与Jenkins都声称对方是自己这里的子分叉，都跟孩子斗气似的），即便分出来了，但是绝大部分还是基于之前的核心代码，所以你可以通过下图看到Hudson与Jenkins的界面都十分类似。   Jenkins的界面     Hudson的界面     但是有一个值得注意的地方就是两个系统的logo，其中Hudson是一个高傲的老头子，而Jenkins是一个谦卑为你服务的老头子。       分家之后，Hudson有Oracle和Sonatype’s corporate的支持和Hudson的注册商标，而Jenkins拥有的是大多数的核心开发者，社区，和后续更多的commit。比如下图是分家之后两个软件的对比。两个软件的活跃程度十分明显，Jenkins遥遥领先。      CI持续集成的工作原理   上面讲完了主流CI工具的江湖故事后，我们来看下这类工具本身的技术情况。其实这类工具的工作原理大同小异，比如下图，一个典型的用例是     程序员在本地开发完成后把代码提交到VCS (Version Control System)比如SVN, Git, Perforce, RTS等   CI工具发现有新的check in 自动启动去抓取最新的代码。当然这里有很多不同的配置，比如除了主动监视VSC外，还可以使用CRON等配置按时启动，比如每隔一个小时启动一次，或者每两次check in 启动一次，等等很多的策略。   CI可以配置使用集群的编译机器，去选择最合适的机器（有不同的策略，比如找到最清闲或者离代码文件距离最近的机器等）来编译源代码   根据不同的配置，CI有可能会调用配置好的测试用例，如果测试失败，根据策略（比如少于几个错误就先忽略）要么通知用户，要么继续跑测试用例   根据配置，CI可能会去执行其他操作，比如静态源代码分析，如代码有没有不符合公司安全要求，把连接密码写在代码里面等等，还有比如生成文档，测试报告，等。   如果所有定义好的jobs跑完，去生成最终报告并送给用户   生成一些分析报表，比如最近成功率，最近哪些程序员造成的错误最多等等。   一些高级的CI,比如Jenkinsg还支持自定义扩展，也会去按配置去执行。      这其中如果任何一步出现了错误，比如某个程序员在提交代码时忘记同时提交一个新写的类，造成失败，首先在CI（比如Jekins，或者Travis）上会显示错误 （比如下图），同时还可以配置CI工具会发出邮件提醒，甚至可以根据提交信息智能的显示出来是哪个程序员搞砸的。      总而言之，这个自动化的过程就像是一个可以配置的流水线，在其上可以添加任意个不同类型的节点，在每个节点可以通过灵活的配置来设置需要完成的工作，还提供了统计及报表，邮件通知等功能，方便团队高效的管理软件的持续集成。   发展及未来   目前的CI也在处于高速发展期，比如最新的Jenkins 2 可以支持使用Groovy编写插件，pipeline等。同时也出现了像是开源的__Travis__之类的持续集成service，即你不用自己去安装调试Jenkins，直接写个YAML文件 （.travis.yaml）放到云上，自动就可以使用其提供的服务了。   另外，持续集成也在跟其他新兴技术相结合使用，比如结合云计算及分布式处理，可以提高CI的运行速度和容错能力，比如下图中的各个服务器可以分别使用cluster(集群)而非一台机器，这样就可以避免所谓的SPOF (Single Point of Failure)单点故障。      如果有什么问题或者想要跟我讨论，请通过如下方式找到我。   联系我：     phray.zhang@gmail.com (email/邮件，whatsapp, linkedin)   helloworld_2000 (wechat/微信)   github   [简书 jianshu]（http://www.jianshu.com/users/a9e7b971aafc）   微信公众号：vibex   webo/微博: cloudsdocker   Reference     CI in wikipedia   Slide share   ","categories": [],
        "tags": ["CI","DevOps","MyBlog"],
        "url": "/2016/10/26/Continuous-Integretaion-Simpler-Chronicle.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Python Scraphy",
        "excerpt":"Python Scraphy   ‘https://www.seek.com.au/jobs-in-information-communication-technology?highpay=True&amp;salaryrange=150000-999999&amp;salarytype=annual’   scrapy shell httxxxx scrapy extrac                 response.css(‘title::text’).re(r’Quotes.*’) [‘Quotes to Scrape’]                           response.css(‘title::text’)[0].extract() ‘Quotes to Scrape’                           response.xpath(‘//title’)            view(response)   Reference     https://doc.scrapy.org/en/latest/intro/tutorial.html     ","categories": [],
        "tags": ["python","scraphy"],
        "url": "/2016/11/04/Python-Scrapy.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "JSON lines",
        "excerpt":"JSON lines   The JSON Lines format is useful because it’s stream-like, you can easily append new records to it. It doesn’t have the same problem of JSON when you run twice. Also, as each record is a separate line, you can process big files without having to fit everything in memory, there are tools like JQ to help doing that at the command-line.   Reference     http://jsonlines.org   https://en.wikipedia.org/wiki/JSON_Streaming     ","categories": [],
        "tags": [],
        "url": "/2016/11/05/JSON-Lines.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Eslastic Search",
        "excerpt":"Eslastic Search   `Elastic Search notes   List indices  http://localhost:9200/_cat/indices?v   pretty-print JSON message (if any)  curl -XPUT 'localhost:9200/customer?pretty'   如果有任何建议或者想法，请联系我。   联系我：     phray.zhang@gmail.com (email/邮件，whatsapp, linkedin)   helloworld_2000 (wechat/微信)   github   [简书 jianshu]（http://www.jianshu.com/users/a9e7b971aafc）   微信公众号：vibex   微博: cloudsdocker  ","categories": [],
        "tags": [],
        "url": "/2016/11/12/elastic.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "用10几行代码自己写个人脸识别程序",
        "excerpt":"用10几行代码自己写个人脸识别程序   CV (Computer Vision)  最近在研究CV的一些开源库(OpenCV)，有一个体会就是在此领域，除了一些非常学术的_机器学习_, 深度学习_等概念外，其实还有一些很有趣的_现实的_应用场景。比如之前很流行的微软的 https://how-old.net, 你使用自己指定或者上传的照片进行面部识别_猜年龄。 如下图所示：    细想一下这个很吸引眼球的程序，其实技术本身打散了就包括两大块，一是从图片中扫描并进行面部识别，二是对找到的人脸根据算法去猜个年龄。大家可以猜猜实现第一个功能需要多少核心代码量？其实不用上万行，在这里我就使用短短几行代码（去除空格换行什么的，有效代码只要10行）就实现一个_高大上_面部识别的功能。在此文容我细述一下具体实现代码以及我对机器识别图像领域技术的理解。   面部识别,刷脸  _人脸识别_技术大家应该都不陌生，之前大家使用的数码相机，或者现在很多手机自带的相机都有人脸识别的功能。其效果就像是下图这样。近的看，_剁手节_刚刚过了没有多久 , 背后的马老板一直在力推的刷脸支付也是一个此领域的所谓“黑科技”。比如在德国汉诺威电子展上，马云用支付宝“刷脸”买了一套纪念邮票。人脸识别应用市场也从爆发。随后，各大互联网巨头也纷纷推出了刷脸相关的应用。      如果要加个定义，人脸识别又叫做人像识别、面部识别，是一种通过用摄像机或摄像头采集含有人脸的图像或视频流，并自动在图像中检测和跟踪人脸，进而对检测到的人脸进行脸部的一系列相关技术。   我的十行代码程序   OK，长话短说，先上 干货 ，下面就是此程序的_带注释_ 版本，完整的程序以及相关配套文件可以在 这个github库 https://github.com/CloudsDocker/pyFacialRecognition 中找到，有兴趣可以_fork_ 下来玩玩。下面是整个程序的代码样子，后面我会逐行去解释分析。      就这短短的十行代码代码？seriously？“有图有真相”，我们先来看下运行的效果：   首先是原始的图片     运行程序后识别出面部并高亮显示的结果  请注意 K歌二人组 的脸上的红色框框，这就是上面十行代码的成果。    代码解析  准备工作  因为此程序使用是的Python,因此你需要去安装Python。这里就不赘述了。除此之外，还需要安装 OpenCV (http://opencv.org/downloads.html)。 多说一句,这个 OpenCV正如其名，是一个开源的机器识别的深度学习框架。这是Intel（英特尔）实验室里的一个俄罗斯团队创造的，目前在开源社区非常的活跃。   特别提一下，对于Mac的用户，推荐使用brew去安装 （下面第一条语句可能会执行报错，我当时也是搞了好久。如果遇到第一条命令不过可以通过文尾的方式联系作者）  brew tap homebrew/science brew install opencv   安装完成之后,在python的命令行中输入如下代码验证，如果没有报错就说明安装好了。  &gt;&gt;&gt; import cv2   程序代码“庖丁解牛”   # -*- coding: utf-8 -*- import cv2,sys     由于这里注释及窗口标题中使用了中文，因此加上utf-8字符集的支持   引入Opencv库以及Python的sys内建库，用于解析输入的图片参数   inputImageFile=sys.argv[1]     在运行程序时将需要测试的照片文件名作为一个参数传进来   faceClassifier=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')      加载OpenCV中自带预先培训好的人脸识别层级分类器 HAAR Casscade Classifier，这个会用来对我们输入的图片进行人脸判断。   这里有几个在深度学习及机器图像识别领域中的几个概念，稍微分析一下，至于深入的知识，大家可以自行搜索或者联系作者。   Classifer  在机器深度学习领域，针对识别不同物体都有不同的classifier,比如有的classifier来识别洗车，还有识别飞机的classifier，有classifier来识别照片中的笑容，眼睛等等。而我们这个例子是需要去做人脸识别，因此需要一个面部识别的classifier。   物体识别的原理  一般来说，比如想要机器学习着去识别“人脸”，就会使用大量的样本图片来事先培训，这些图片分为两大类，positive和negative的，也就是分为包“含有人脸”的图片和“不包含人脸”的图片，这样当使用程序去一张一张的分析这些图片，然后分析判断并对这些图片“分类” (classify),即合格的图片与不合格的图片，这也就其为什么叫做 classifier ， 这样学习过程中积累的”知识”，比如一些判断时的到底临界值多少才能判断是positive还是negative什么的，都会存储在一个个XML文件中，这样使用这些前人经验（这里我们使用了 哈尔 分类器）来对新的图片进行‘专家判断’分析，是否是人脸或者不是人脸。   Cascade  这里的 Cascade是 层级分类器 的意思。为什么要 分层 呢？刚才提到在进行机器分析照片时，其实是对整个图片从上到下，从左到右，一个像素一个像素的分析，这些分析又会涉及很多的 特征分析 ，比如对于人脸分析就包含识别眼睛，嘴巴等等，一般为了提高分析的准确度都需要有成千上万个特征，这样对于每个像素要进行成千上万的分析，对于整个图片都是百万甚至千万像素，这样总体的计算量会是个天文数字。但是，科学家很聪明，就想到分级的理念，即把这些特征分层，这样分层次去验证图片，如果前面层次的特征没有通过，对于这个图片就不用判断后面的特征了。这有点像是系统架构中的 FF (Fail Fast),这样就提高了处理的速度与效率。   objImage=cv2.imread(inputImageFile)     使用OpenCV库来加载我们传入的测试图片   cvtImage=cv2.cvtColor(objImage,cv2.COLOR_BGR2GRAY)     首先将图片进行灰度化处理，以便于进行图片分析。这种方法在图像识别领域非常常见，比如在进行验证码的机器识别时就会先灰度化，去除不相关的背景噪音图像，然后再分析每个像素，以便抽取出真实的数据。不对针对此，你就看到非常多的验证码后面特意添加了很多的噪音点，线，就是为了防止这种程序来灰度化图片进行分析破解。   foundFaces=faceClassifier.detectMultiScale(cvtImage,scaleFactor=1.3,minNeighbors=9,minSize=(50,50),flags = cv2.cv.CV_HAAR_SCALE_IMAGE)     执行detectMultiScale方法来识别物体，因为我们这里使用的是人脸的cascade classifier分类器，因此调用这个方法会来进行面部识别。后面这几个参数来设置进行识别时的配置，比如   scaleFactor: 因为在拍照，尤其现在很多都是自拍，这样照片中有的人脸大一些因为离镜头近，而有些离镜头远就会小一些，因为这个参数用于设置这个因素，如果你在使用不同的照片时如果人脸远近不同，就可以修改此参数，请注意此参数必须要大于1.0   minNeighbors: 因为在识别物体时是使用一个移动的小窗口来逐步判断的，这个参数就是决定是不是确定找到物体之前需要判断多少个周边的物体   minSize：刚才提到识别物体时是合作小窗口来逐步判断的，这个参数就是设置这个小窗口的大小   print(\" 在图片中找到了 {} 个人脸\".format(len(foundFaces)))     显示出查找到多少张人脸，需要提到的识别物体的方法返回的一个找到的物体的位置信息的列表，因此使用 len 来打印出找到了多少物体。   for (x,y,w,h) in foundFaces:     cv2.rectangle(objImage,(x,y),(x+w,y+h),(0,0,255),2)     遍历发现的“人脸”，需要说明的返回的是由4部分组成的位置数据，即这个“人脸”的横轴，纵轴坐标，宽度与高度。   然后使用 OpenCV 提供的方法在原始图片上画出个矩形。其中 (0,0,255) 是使用的颜色，这里使用的是R/G/B的颜色表示方法，比如 (0,0,0)表示黑色，(255,255,255)表示白色，有些网页编程经验的程序员应该不陌生。   cv2.imshow(u'面部识别的结果已经高度框出来了。按任意键退出'.encode('gb2312'), objImage) cv2.waitKey(0)     接下来是使用 opencv 提供的imshow方法来显示这个图片，其中包括我们刚刚画的红色的识别的结果   最后一个语句是让用户按下键盘任意一个键来退出此图片显示窗口   总结  好了，上面是这个程序的详细解释以及相关的知识的讲解。其实这个只是个_抛砖引玉_的作用，还用非常多的应用场景，比如程序解析网页上的图片验证码，雅虎前几个月开源的 NSFW, Not Suitable for Work (NSFW)，即判断那些不适合工作场所的图片，内容你懂的。 :-)   最后，再提一下，所有这些源代码及相关文件都开源在 https://github.com/CloudsDocker/pyFacialRecognition ，在fork并下载到本地后执行下面代码来测试运行  git clone https://github.com/CloudsDocker/pyFacialRecognition.git cd pyFacialRecognition ./run.sh   如果有任何建议或者想法，请联系我。   联系我：     phray.zhang@gmail.com (email/邮件，whatsapp, linkedin)   helloworld_2000 (wechat/微信)   微博: cloudsdocker   github   [简书 jianshu]（http://www.jianshu.com/users/a9e7b971aafc）   微信公众号：vibex   Reference     OpenCV   HAAR 哈尔特征   Face Detection using Haar Cascades   NSFW  ","categories": [],
        "tags": ["DeepLearning","FacialRecognition","MyBlog"],
        "url": "/2016/11/22/Facial-Recognition.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Java new features",
        "excerpt":"JDK Versions     JDK 1.5 in 2005   JDK 1.6 in 2006   JDK 1.7 in 2011   JDK 1.8 in 2014 Sun之前风光无限，但是在2010年1月27号被Oracle收购。 在被Oracle收购后对外承诺要回到每2年一个realse的节奏。但是2014年就公布由于Java Security的问题delay一年   Java新功能一览     Generics   Concurrent Framework   Numberic Literal   Lambda   JDK New release vs. Stock Price   最近在做一个Java项目的相关工作，发现很多程序员还在使用一些最基本的Java语言特性，其实可以理解为很多人入门Java要么在学校里要么通过某些书籍，而这些在国内的学习资料更新比较慢，很多都还是基于JDK1.4或者更老。其实Java这个语言一直在进步。下面我就以Java最近几个主版本引入的新功能来梳理一下这些“黑科技”   Java 1.5  这个版本个人认为在Java最近10年的历史中是一个最重要的升级。JDK1.5，内部版本号是Tiger，引入了一些颠覆性的功能，列在如下： Generics Conncurrecnt Framework，应该是继Collection Framework后又一个块头的“框架”了。   Java 1.6   所有上面提到的东西，包括此文章的markdown源代码，mindmap思维导图等等都可以在我的github上找到。联系我：     phray.zhang@gmail.com (email/邮件，whatsapp, linkedin)   helloworld_2000 (wechat/微信)   github   Reference     JDK History  ","categories": [],
        "tags": ["java history","JDK"],
        "url": "/2016/12/01/Java-New-Features-Chronicle.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Random number in java",
        "excerpt":"ThreadLocalRandom, SecureRandm, java.util.Random, java.math.Random   Instances of java.util.Random are threadsafe. However, the concurrent use of the same java.util.Random instance across threads may encounter contention and consequent poor performance. Consider instead using ThreadLocalRandom in multithreaded designs.   The Java Math library function Math.random() generates a double value in the range [0,1). Notice this range does not include the 1.   int rand = ThreadLocalRandom.current().nextInt(x,y);   Reference     How to generate a range of random integers in Java   Random JavaDoc   SecureRandom JavaDoc   ThreadLocalRandom JavaDoc   Apache Common Math   LCG wikipedia   Blog about this topic   ImportNew  ","categories": [],
        "tags": ["MyBlog","Java"],
        "url": "/2016/12/09/HeadsFirst-RandomNumber-In-Java.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Angulary Misc",
        "excerpt":"Dependency Injection  Angular doesn’t automatically know how you want to create instances of your services or the injector to create your service. You must configure it by specifying providers for every service.   Providers tell the injector how to create the service. Without a provider, the injector would not know that it is responsible for injecting the service nor be able to create the service.   What is difference between declarations, providers and import in NgModule      imports: is used to import supporting modules likes FormsModule, RouterModule, CommonModule, or any other custom-made feature module. makes the exported declarations of other modules available in the current module   declarations are to make directives (including components and pipes) from the current module available to other directives in the current module. Selectors of directives, components or pipes are only matched against the HTML if they are declared or imported. declaration is used to declare components, directives, pipes that belongs to the current module. Everything inside declarations knows each other. For example, if we have a component, say UsernameComponent, which display list of the usernames, and we also have a pipe, say toupperPipe, which transform string to uppercase letter string. Now If we want to show usernames in uppercase letters in our UsernameComponent, we can use the toupperPipe which we had created before but how UsernameComponent know that the toupperPipe exist and how we can access and use it, here comes the declarations, we can declare UsernameComponent and toupperPipe.   providers are to make services and values known to DI. They are added to the root scope and they are injected to other services or directives that have them as dependency.provider is used to inject the services required by components, directives, pipes in our module.   CLI to create new component  ng generate component components/xxx-table -m deposit.module.ts --spec ng generate component components/comp1 -m core.module.ts --spec Error: Specified module does not exist Specified module does not exist   $ ng generate component core/components/comp1 -m core/core.module --spec Error: dryRunSink.commit(...).ignoreElements is not a function dryRunSink.commit(...).ignoreElements is not a function   ng generate component app/core/components/comp1 -m app/core/core.module --spec   Angular JS notes   AngularJS applications are built around a design pattern called Model-View-Controller (MVC), which places an emphasis on creating applications that are      Extendable: It is easy to figure out how even a complex AngularJS app works once you understand the basics—and that means you can easily enhance applications to create new and useful features for your users.   Maintainable: AngularJS apps are easy to debug and fix, which means that long-term maintenance is simplified.   Testable: AngularJS has good support for unit and end-to-end testing, meaning that you can find and fix defects before your users do.   Standardized: AngularJS builds on the innate capabilities of the web browser without getting in your way, allowing you to create standards-compliant web apps that take advantage of the latest features (such as HTML5 APIs) and popular tools and frameworks.  ","categories": [],
        "tags": ["Angular","JavaScript"],
        "url": "/2016/12/12/Angular.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Apache Tips",
        "excerpt":"Apache   sudo su - apachectl start apachectl stop   Open a browser to access http://localhost/  ","categories": [],
        "tags": ["Apache","DevOps"],
        "url": "/2016/12/24/Apache-Tips.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Hash Code Misc",
        "excerpt":"contract of hashCode :     Whenever it is invoked on the same object more than once during an execution of a Java application, the hashCode method must consistently return the same integer, provided no information used in equals comparisons on the object is modified. This integer need not remain consistent from one execution of an application to another execution of the same application.   If two objects are equal according to the equals(Object) method, then calling the hashCode method on each of the two objects must produce the same integer result.   It is not required that if two objects are unequal according to the equals(java.lang.Object) method, then calling the hashCode method on each of the two objects must produce distinct integer results. However, the programmer should be aware that producing distinct integer results for unequal objects may improve the performance of hash tables.   As much as is reasonably practical, the hashCode method defined by class Object does return distinct integers for distinct objects. (This is typically implemented by converting the internal address of the object into an integer, but this implementation technique is not required by the JavaTM programming language.)   Hashcode in Java Collections  Hashcode vs. equals  Equals  For usage in java collections framework, equals() used in following scenarios:     contains or not   to remove one item   Below is one implementation of a Equals  public class Employee{     public boolean equals(Object o){         if(o==null) return false;         if(!(o instanceof Employee)) return false;          Employee other=(Employee)o;         return this.employeeID==other.employeeID;     } }   Hashcode  When insert data into HashTable, HashMap, HashSet, the hashcode used to determine where to store/search the value in list/bucket. The hashcode only point to an area in list/bucket of data. The hashtable then iterates this area (all keys with the same hash code) and uses the key’s equals() method to find the right key. Once the right key is found, the object stored for that key is returned.   After you override the hashcode, you are still be able to get origional hashcode via calling int originalHashCode = System.identityHashCode(emp1);   Rules  In this regard there is a rule of thumb that if you are going to overide the one of the methods (equals, hashcode), you have to override both, othersise it’s a violation of contract .   HashMap tips     Since searching inlined list is O(n) operation, in worst case hash collision reduce a map to linked list. This issue is recently addressed in Java 8 by replacing linked list to the tree to search in O(logN)   HashMap works on the principle of hashing.   HashMap  stores both Key and Value in LinkedList node or as Map   using immutable, final object with proper equals() and hashcode() implementation would act as perfect Java HashMap  keys and improve the performance of Java HashMap  by reducing collision. Immutability also allows caching their hashcode of different keys which makes overall retrieval process very fast and suggest that String and various wrapper classes e.g. Integer very good keys in Java HashMap.   if the load factor is .75 it will act to re-size the map once it filled 75%. Java HashMap re-size itself by creating a new bucket array of size twice of the previous size of HashMap and then start putting every old element into that new bucket array. This process is called rehashing because it also applies the hash function to find new bucket location.   there is potential race condition exists while resizing HashMap in Java, if two thread at the same time found that now HashMap needs resizing and they both try to resizing. on the process of resizing of HashMap in Java, the element in the bucket which is stored in linked list get reversed in order during their migration to new bucket because Java HashMap  doesn’t append the new element at tail instead it append new element at the head to avoid tail traversing.   If race condition happens then you will end up with an infinite loop.   Why String, Integer and other wrapper classes are considered good keys?      String, Integer and other wrapper classes are natural candidates of HashMap key, and String is most frequently used key as well because String is immutable and final, and overrides equals and hashcode() method.   Immutability is best as it offers other advantages as well like thread-safety, If you can  keep your hashCode same by only making certain fields final, then you go for that as well.   ConcurrentHashMap     ConcurrentHashMap provides better concurrency by only locking portion of map determined by concurrency level.   but Hashtable provides stronger thread-safety than ConcurrentHashMap   ConcurrentHashMap performs better than earlier two because it only locks a portion of Map, instead of whole Map, which is the case with Hashtable and synchronized Map.   provided all functions supported by Hashtable with an additional feature called “concurrency level”, which allows ConcurrentHashMap to partition Map.   ConcurrentHashMap allows multiple readers to read concurrently without any blocking. This is achieved by partitioning Map into different parts based on concurrency level and locking only a portion of Map during updates.   Default concurrency level is 16, and accordingly Map is divided into 16 part and each part is governed with a different lock. This means, 16 thread can operate on Map simultaneously until they are operating on different part of Map.   Since update operations like put(), remove(), putAll() or clear() is not synchronized, concurrent retrieval may not reflect most recent change on Map.   ConcurrentHashMap also uses ReentrantLock to internally lock its segments.   In hashmap and hashtable, you can check one item first and add it if not present. Though this code will work fine in HashMap and Hashtable, This won’t work in ConcurrentHashMap; because, during put operation whole map is not locked, and while one thread is putting value, other thread’s get() call can still return null which result in one thread overriding value inserted by other thread. Ofcourse, you can wrap whole code in synchronized block and make it thread-safe but that will only make your code single threaded. ConcurrentHashMap provides putIfAbsent(key, value) which does same thing but atomically and thus eliminates above race condition   ConcurrentHashMap is best suited when you have multiple readers and few writers. If writers outnumber reader, or writer is equal to reader, than performance of ConcurrentHashMap effectively reduces to synchronized map or Hashtable. Performance of CHM drops, because you got to lock all portion of Map, and effectively each reader will wait for another writer, operating on that portion of Map. ConcurrentHashMap is a good choice for caches, which can be initialized during application start up and later accessed my many request processing threads.   ConcurrentHashMap allows concurrent read and thread-safe update operation.   Iterator returned by ConcurrentHashMap is weekly consistent, fail-safe and never throw ConcurrentModificationException.   In Java.ConcurrentHashMap doesn’t allow null as key or value.   How null work in HashMap  How null key is handled in HashMap? Since equals() and hashCode() are used to store and retrieve values, how does it work in case of the null key?   The null key is handled specially in HashMap, there are two separate methods for that putForNullKey(V value) and getForNullKey() . Later is offloaded version of get() to look up null keys.  Null keys always map to index 0.  This null case is split out into separate methods for the sake of performance in the two most commonly used operations (get and put), but incorporated with conditionals in others. In short, equals() and hashcode() method are not used in case of null keys in HashMap.   Performance changes in JDK 1.7 and 1.8     There is some performance improvement done on HashMap and ArrayList from JDK 1.7, which reduce memory consumption. Due to this empty Map are lazily initialized and will cost you less memory. Earlier, when you create HashMap e.g. new HashMap() it automatically creates an array of default length e.g. 16. After some research, Java team found that most of this Map are temporary and never use that many elements, and only end up wasting memory. Also, From JDK 1.8 onwards HashMap has introduced an improved strategy to deal with high collision rate. Since a poor hash function e.g. which always return location of same bucket, can turn a HashMap into linked list, i.e. converting get() method to perform in O(n) instead of O(1) and someone can take advantage of this fact, Java now internally replace linked list to a binary true once certain threshold is breached. This ensures performance or order O(log(n)) even in the worst case where a hash function is not distributing keys properly.   such change is creating empty ArrayList and HashMap with size zero in JDK 1.7.0_40 update.   If you are running on Java 1.6 or earlier version of Java 1.7, you can open code of java.util.ArrayList and check that, currently empty ArrayList is initialized with Object array of size 10. If you create several temporary list in your program, which remains uninitialized, due to any reason then you are not only losing memory but also losing performance by giving your garbage collector more work.   Same is true for empty HashMap, which was initialized by default initial capacity of 16. This changes are result of observation made by Nathan Reynolds, and Architect at Oracle, which apparently analysed 670 Java heap dumps from different Java programs to find out memory hogs.   By the way, it’s not just memory, it’s also extra work-load for Garbage collector   Hashtable     One of the major differences between HashMap and Hashtable is that HashMap is non-synchronized whereas Hashtable is synchronized   Another difference is HashMap allows one null key and null values but **Hashtable doesn’t allow null ** key or values.   Another significant difference between HashMap vs Hashtable is that Iterator in the HashMap is  a fail-fast ** iterator  while the enumerator for the **Hashtable is not and throw ConcurrentModificationException if any other Thread modifies the map structurally  by adding or removing any element except Iterator’s own remove() method   ConcurrentHashMap vs Hashtable vs Synchronized Map     Though all three collection classes are thread-safe and can be used in multi-threaded, concurrent Java application, there is a significant difference between them, which arise from the fact that how they achieve their thread-safety.   Hashtable is a legacy class from JDK 1.1 itself, which uses synchronized methods to achieve thread-safety. All methods of Hashtable are synchronized which makes them quite slow due to contention if a number of thread increases.   Synchronized Map is also not very different than Hashtable and provides similar performance in concurrent Java programs. The only difference between Hashtable and Synchronized Map is that later is not a legacy and you can wrap any Map to create it’s synchronized version by using Collections.synchronizedMap()   Unlike Hashtable and Synchronized Map, it never locks whole Map, instead, it divides the map into segments and locking is done on those. Though it performs better if a number of reader threads are greater than the number of writer threads.   ConcurrentHashMap and CopyOnWriteArrayList implementations provide much higher concurrency while preserving thread safety, with some minor compromises in their promises to callers. ConcurrentHashMap and CopyOnWriteArrayList are not necessarily useful everywhere you might use HashMap or ArrayList, but are designed to optimize specific common situations.   ConcurrentHashMap does not allow null keys or null values while synchronized HashMap allows one null key.   Synchronized List          CopyOnWriteArrayList and CopyOnWriteArraySet CopyOnWriteArrayList is a concurrent alternative of synchronized List. CopyOnWriteArrayList provides better concurrency than synchronized List by allowing multiple concurrent reader and replacing the whole list on write operation. Yes, write operation is costly on CopyOnWriteArrayList but it performs better when there are multiple reader and requirement of iteration is more than writing. Since CopyOnWriteArrayList Iterator also don’t throw ConcurrencModificationException it eliminates need to lock the collection during iteration. Remember both ConcurrentHashMap and CopyOnWriteArrayList doesn’t provides same level of locking as Synchronized Collection and achieves thread-safety by there locking and mutability strategy. So they perform better if requirements suits there nature. Similarly, CopyOnWriteArraySet is a concurrent replacement to Synchronized Set. See What is CopyOnWriteArrayList in Java for more details            BlockingQueue BlockingQueue is also one of better known collection class in Java 5. BlockingQueue makes it easy to implement producer-consumer design pattern by providing inbuilt blocking support for put() and take() method. put() method will block if Queue is full while take() method will block if Queue is empty. Java 5 API provides two concrete implementation of BlockingQueue in form of ArrayBlockingQueue and LinkedBlockingQueue, both of them implement FIFO ordering of element. ArrayBlockingQueue is backed by Array and its bounded in nature while LinkedBlockingQueue is optionally bounded. Consider using BlockingQueue to solve producer Consumer problem in Java instead of writing your won wait-notify code. Java 5 also provides PriorityBlockingQueue, another implementation of BlockingQueue which is ordered on priority and useful if you want to process elements on order other than FIFO.            Deque interface  is added in Java 6 and it extends Queue interface to support insertion and removal from both end of Queue referred as head and tail. Java6 also provides concurrent implementation of Deque like ArrayDeque and LinkedBlockingDeque. Deque Can be used efficiently to increase parallelism in program by allowing set of worker thread **to help each other by taking some of work load from other thread by utilizing Deque double end consumption property. So if all Thread has there **own set of task Queue and they are consuming from head; helper thread can also share some work load via consumption from tail.            ConcurrentSkipListMap and ConcurrentSkipListSet Just like ConcurrentHashMap provides a concurrent alternative of synchronized HashMap. ConcurrentSkipListMap and ConcurrentSkipListSet provide concurrent alternative for synchronized version of SortedMap and SortedSet. For example instead of using TreeMap or TreeSet wrapped inside synchronized Collection, You can consider using ConcurrentSkipListMap or ConcurrentSkipListSet from java.util.concurrent package. They also implement NavigableMap and NavigableSet to add additional navigation method we have seen in our post How to use NavigableMap in Java.       To sort hashmap by key and value     Why can’t we use TreeMap in place of HashMap is the question appears in most Java programmer’s mind when they asked to sort HashMap in Java. Well, TreeMap is way slower than HashMap because it runs sorting operation with each insertion, update and removal and sometimes you don’t really need an all time sorted Map, What you need is an ability to sort any Map implementation based upon its key and value.     Sort by Key       As I said Map or HashMap in Java can be sorted either on keys or values. Sorting Map on keys is rather easy than sorting on values because Map allows duplicate values but doesn’t allow duplicates keys. You can sort Map, be it HashMap or Hashtable by copying keys into List than sorting List by using Collections.sort() method, here you can use either Comparator or Comparable based upon whether you want to sort on a custom order or natural order.  -Once List of keys is sorted, we can create another Map, particularly LinkedHashMap to insert keys in sorted order. LinkedHashMap will maintain the order on which keys are inserted, the result is a sorted Map based on keys. This is shown in the following example by writing a generic parameterized method to sort Map based on keys. You can also sort Map in Java by using TreeMap and Google Collection API (Guava). The advantage of using Guava is that you get some flexibility on specifying ordering.     Sorting Map in Java - By Value       To implement Collection.sort(map, new Comparator&lt;Map.Entry&lt;K,V»(){ o1.getValue().compareTo(o2.getValue())   But need to take care of null and duplication   Reference     http://javarevisited.blogspot.in/2011/02/how-hashmap-works-in-java.html   http://javarevisited.blogspot.in/2011/04/difference-between-concurrenthashmap.html   http://javarevisited.blogspot.com/2013/02/concurrent-collections-from-jdk-56-java-example-tutorial.html#ixzz4WBYzKelD   http://javarevisited.blogspot.com/2012/12/how-to-sort-hashmap-java-by-key-and-value.html#ixzz4WBgfeGVM  ","categories": [],
        "tags": ["java","hashcode"],
        "url": "/2016/12/30/HashCode-Contract.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Java GC notes",
        "excerpt":"verbose:gc  verbose:gc prints right after each gc collection and prints details about each generation memory details. Here is blog on how to read verbose gc   If you are trying to look for memory leak, verbose:gc may not be enough. Use some visualization tools like jhat (or) visualvm etc.,   4416K-&gt;512K(4928K), 0.0081170 secs   Before GC used memory is 4416K After GC used memory is 512K Total allocated memory is 4928K   -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:C:/Users/tzhang17/temp/gc/gc.log   a typical ratio of YoungGen vs. OldGen is 1:3 or 33%.   Minimizing the frequency of major GC collections is a key aspect for optimal performance so it is very important that you understand and estimate how much memory you need during your peak volume.   Again, your type of application and data will dictate how much memory you need. Shopping cart type of applications (long lived objects) involving large and non-serialized session data typically need large Java Heap and lot of OldGen space. Stateless and XML processing heavy applications (lot of short lived objects) require proper YoungGen space in order to minimize frequency of major collections.   Generational collection  According to the generational hypothesis [21], most objects die young and consequently older objects tend to live longer. Generational collection capitalises on the generational hypothesis by dividing the available memory space into multiple regions called generations. Garbage collector passes are less frequent as the generations grow older and objects are always allocated into the newest generation. If the object survives a garbage collection, it is promoted to an older generation. Each generation can have a separate garbage collection strategy.   Reference counting  Reference counting uses a counter per object to record the number of references to the object. The pointer is incremented each time a reference towards the object is created. The object is reclaimed when its reference count drops to zero. Reference counting is being extensively used by scripting languages such as Perl.   GC strategy  Mark-Sweep garbage collection is most times followed by a compaction phase in order to avoid memory fragmentation. The compaction phase requires moving the objects to adjacent memory locations, thus making Mark-Sweep quite an expensive algorithm for large memory multiprocessor environments, unless a multithreaded heap compactor is employed. Simple reference counting is also unsuitable for high throughput environments because it requires objects to be reclaimed on pointer updates; if a pointer is removed and the reference count of the pointed object drops to zero, the runtime system is required to collect both that object and the objects it references. Furthermore, a major drawback of reference counting is its inability to collect circular data structures, such as doubly linked lists. Despite its drawbacks, the simplicity in the implementation of reference counting made it the preferred garbage collection strategy in runtime environments with a limited lifetime, such as scripting languages.   mutable memory  Memory in a typical JVM is organised in a series of mutable (garbage collected) and immutable zones. Class code is usually loaded in immutable space1 and remains there until the JVM is stopped. Also, the code emitted from the JIT compiler is temporarily stored in immutable space. The actual allocations take place in the heap, which is a contiguous memory area.   Apart from the class member values, each object also contains additional data such as a pointer to the respective class methods and flags related to locking and garbage collection. In most virtual machines, object headers take up to 8–12 bytes of additional storage space for each object, and can therefore sadle a program with significant performance and space overhead. A lot of work has been put into compacting the object header [6], which, in some cases, resulted in space savings of up to 20%.   A failure to allocate space for an object triggers a garbage collection cycle. The root set is determined by conservatively scanning the stacks of running or suspended threads and the current values of the processor registers for potential pointers to the heap. Root set acquisition can also be a performance bottleneck in the case when a large number of threads is executed concurrently, though these costs can be amortised using clever co-operation of the garbage collector with the JIT.   JDK 1.5  Sun’s JVM is an implementation of the 1.5 version of the Java language specification. It features an adaptive optimising JIT compiler, the well-known Hotspot engine, and a choice of three garbage collectors [2, 12]. Sun’s JVM is based upon a generational copying garbage collector that utilises two generations  Figure 1 presents the heap organisation, which is shared among all collectors. Allocations initially occur in the eden space and survivors are promoted to one of the survivor spaces in a copying fashion. Optionally, portions of the heap space can be allocated to individual threads (Thread-Local Heaps (TLHs)), in order to speed up allocations on large-heap multithreaded environments. Objects that reach a certain age threshold, usually measured in minor garbage collection cycles, are copied to the tenured generation where they are left untouched until a major collection occurs. A mark-compact garbage collector is used for the tenured generation.   Tuning advise     Unless you have specific hardware constraints, devote as much memory as you can to the virtual machine. A big heap size offers the opportunity for less frequent, albeit more time consuming, full heap collections. In a throughput-oriented environment sacrificing pause times to allow more CPU time for the executed application is often a good compromise. Do not allow the virtual machine to be swapped out to disk, as this is catastrophic for performance. In an application server that only runs a single virtual machine, you could devote about 90% of its available RAM to it and turn off paging, without risking the failure of either the virtual machine or the operating system.   Calculate the memory allocation rate for your application. It is a significant measurement that you should perform by exposing the application to full workload. Its impact varies depending on the underlying hardware. As a rule of thumb, on a multiprocessor machine, each processor could easily generate more than 150MB of garbage per second. High allocation rates can be efficiently dealt with by using parallel collectors or large eden heap sizes.   Committed heap  A MemoryUsage object represents a snapshot of memory usage. Instances of the MemoryUsage class are usually constructed by methods that are used to obtain memory usage information about individual memory pool of the Java virtual machine or the heap or non-heap memory of the Java virtual machine as a whole. A MemoryUsage object contains four values:   init\trepresents the initial amount of memory (in bytes) that the Java virtual machine requests from the operating system for memory management during startup. The Java virtual machine may request additional memory from the operating system and may also release memory to the system over time. The value of init may be undefined. used\trepresents the amount of memory currently used (in bytes). committed\trepresents the amount of memory (in bytes) that is guaranteed to be available for use by the Java virtual machine. The amount of committed memory may change over time (increase or decrease). The Java virtual machine may release memory to the system and committed could be less than init. committed will always be greater than or equal to used. max\trepresents the maximum amount of memory (in bytes) that can be used for memory management. Its value may be undefined. The maximum amount of memory may change over time if defined. The amount of used and committed memory will always be less than or equal to max if max is defined. A memory allocation may fail if it attempts to increase the used memory such that used &gt; committed even if used &lt;= max would still be true (for example, when the system is low on virtual memory). Below is a picture showing an example of a memory pool:         +———————————————-+         +////////////////           |                  +         +////////////////           |                  +         +———————————————-+       |--------|        init     |---------------|            used     |---------------------------|               committed     |----------------------------------------------|   Garbage Collection for JVM   Interpreting vs compile     The HotSpot JVM (and other modern JVMs) uses a combination of bytecode interpretation and dynamic compilation. When a class is first loaded, the JVM executes it by interpreting the bytecode. At some point, if a method is run often enough, the dynamic compiler kicks in and converts it to machine code; when compilation completes, it switches from interpretation to direct execution.   Code may also be decompiled (reverting to interpreted execution) and recompiled for various reasons, such as loading a class that invalidates assumptions made by prior compilations, or gathering sufficient profiling data to decide that a code path should be recompiled with different optimizations.   One of the challenges of writing good benchmarks (in any language) is that optimizing compilers are adept at spotting and eliminating dead code—code that has no effect on the outcome. Since benchmarks often don’t compute anything, they are an easy target for the optimizer. Most of the time, it is a good thing when the optimizer prunes dead code from a program, but for a benchmark this is a big problem because then you are measuring less execution than you think.   Many microbenchmarks perform much “better” when run with HotSpot’s -server compiler than with -client, not just because the server compiler can produce more efficient code, but also because it is more adept at optimizing dead code.   Writing effective performance tests requires tricking the optimizer into not optimizing away your benchmark as dead code. This requires every computed result to be used somehow by your program—in a way that does not require synchronization or substantial computation.   We happen to need it to verify the correctness of the algorithm, but you can ensure that a value is used by printing it out. However, you should avoid doing I/O while the test is actually running, so as not to distort the run time measurement.   A cheap trick for preventing a calculation from being optimized away without introducing too much overhead is to compute the hashCode of the field of some derived object, compare it to an arbitrary value such as the current value of System. nanoTime, and print a useless and ignorable message if they happen to match:     if(foo.x.hashCode()==System.nanoTime())   System.out.println(\" \");          The comparison will rarely succeed, and if it does, its only effect will be to insert a harmless space character into the output. (The print method buffers output until println is called, so in the rare case that hashCode and System.nanoTime are equal no I/O is actually performed.)       Not only should every computed result be used, but results should also be unguessable. Otherwise, a smart dynamic optimizing compiler is allowed to replace actions with precomputed results.   Java GC   Java’s GC considers objects “garbage” if they aren’t reachable through a chain starting at a garbage collection root, so these objects will be collected. Even though objects may point to each other to form a cycle, they’re still garbage if they’re cut off from the root.   See the section on unreachable objects in Appendix A: The Truth About Garbage Collection in Java Platform Performance: Strategies and Tactics (free ebook, also available on Safari) for the gory details.   Java Garbage collector handles circular-reference!   How?   There are special objects called called garbage-collection roots (GC roots). These are always reachable and so is any object that has them at its own root.   A simple Java application has the following GC roots:   Local variables in the main method The main thread Static variables of the main class   To determine which objects are no longer in use, the JVM intermittently runs what is very aptly called a mark-and-sweep algorithm. It works as follows   The algorithm traverses all object references, starting with the GC roots, and marks every object found as alive. All of the heap memory that is not occupied by marked objects is reclaimed. It is simply marked as free, essentially swept free of unused objects.   So if any object is not reachable from the GC roots(even if it is self-referenced or cyclic-referenced) it will be subjected to garbage collection. Ofcourse sometimes this may led to memory leak if programmer forgets to dereference an object.    The actual answer to this is implementation dependent. The Sun JVM keeps track of some set of root objects (threads and the like), and when it needs to do a garbage collection, traces out which objects are reachable from those and saves them, discarding the rest. It’s actually more complicated than that to allow for some optimizations, but that is the basic principle. This version does not care about circular references: as long as no live object holds a reference to a dead one, it can be GCed.   Other JVMs can use a method known as reference counting. When a reference is created to the object, some counter is incremented, and when the reference goes out of scope, the counter is decremented. If the counter reaches zero, the object is finalized and garbage collected. This version, however, does allow for the possibility of circular references that would never be garbage collected. As a safeguard, many such JVMs include a backup method to determine which objects actually are dead which it runs periodically to resolve self-references and defrag the heap.     A garbage collector starts from some “root” set of places that are always considered “reachable”, such as the CPU registers, stack, and global variables. It works by finding any pointers in those areas, and recursively finding everything they point at. Once it’s found all that, everything else is garbage.   There are, of course, quite a few variations, mostly for the sake of speed. For example, most modern garbage collectors are “generational”, meaning that they divide objects into generations, and as an object gets older, the garbage collector goes longer and longer between times that it tries to figure out whether that object is still valid or not – it just starts to assume that if it has lived a long time, chances are pretty good that it’ll continue to live even longer.   Nonetheless, the basic idea remains the same: it’s all based on starting from some root set of things that it takes for granted could still be used, and then chasing all the pointers to find what else could be in use.   Interesting aside: may people are often surprised by the degree of similarity between this part of a garbage collector and code for marshaling objects for things like remote procedure calls. In each case, you’re starting from some root set of objects, and chasing pointers to find all the other objects those refer to…     How Garbage Collection Really Works   Many people think garbage collection collects and discards dead objects. In reality, Java garbage collection is doing the opposite! Live objects are tracked and everything else designated garbage. As you’ll see, this fundamental misunderstanding can lead to many performance problems.   Garbage-Collection Roots—The Source of All Object Trees   Every object tree must have one or more root objects. As long as the application can reach those roots, the whole tree is reachable. But when are those root objects considered reachable? Special objects called garbage-collection roots (GC roots; see Figure 2.2) are always reachable and so is any object that has a garbage-collection root at its own root.   There are four kinds of GC roots in Java:      Local variables are kept alive by the stack of a thread. This is not a real object virtual reference and thus is not visible. For all intents and purposes, local variables are GC roots.   Active Java threads are always considered live objects and are therefore GC roots. This is especially important for thread local variables.   Static variables are referenced by their classes. This fact makes them de facto GC roots. Classes themselves can be garbage-collected, which would remove all referenced static variables. This is of special importance when we use application servers, OSGi containers or class loaders in general. We will discuss the related problems in the Problem Patterns section.   JNI References are Java objects that the native code has created as part of a JNI call. Objects thus created are treated specially because the JVM does not know if it is being referenced by the native code or not. Such objects represent a very special form of GC root, which we will examine in more detail in the Problem Patterns section below.   Therefore, a simple Java application has the following GC roots:      Local variables in the main method   The main thread   Static variables of the main class   Marking and Sweeping Away Garbage   To determine which objects are no longer in use, the JVM intermittently runs what is very aptly called a mark-and-sweep algorithm. As you might intuit, it’s a straightforward, two-step process:      The algorithm traverses all object references, starting with the GC roots, and marks every object found as alive.   All of the heap memory that is not occupied by marked objects is reclaimed. It is simply marked as free, essentially swept free of unused objects.    Garbage collectors which rely solely on reference counting are generally vulnerable to failing to collection self-referential structures such as this. These GCs rely on a count of the number of references to the object in order to calculate whether a given object is reachable.   Non-reference counting approaches apply a more comprehensive reachability test to determine whether an object is eligible to be collected. These systems define an object (or set of objects) which are always assumed to be reachable. Any object for which references are available from this object graph is considered ineligible for collection. Any object not directly accessible from this object is not. Thus, cycles do not end up affecting reachability, and can be collected.    Tracing collector vs. countering collector  There are two primary types of garbage collectors, although often a hybrid approach is found between these to suit particular needs. The first type, the one which might be the most intuitive, is a reference counting collector. The second one, which is most similar to what we described above, is a tracing collector.   Reference Counting Collector  When a new memory object is allocated by the GC, it is given an integer count field. Every time a pointer is made to that object, a reference, the count is increased. So long as the count is a positive non-zero integer, the object is actively being referenced and is still alive. When a reference to the object is removed, the count is decremented. When the count reaches zero, the object is dead and can be immediately reclaimed. There are a number of points to remember about Reference Counting collectors:     Circular references will never be reclaimed, even if the entire set of objects is dead.   Reference counting is pervasive: The entire program must be made aware of the system, and every pointer reference or dereference must be accompanied by an appropriate increment or decrement. Failing to maintain the count, even once in a large program, will create memory problems for your program.   Reference counting can be costly, because counts must be manipulated for every pointer operation, and the count must be tested against zero on ever decrement. These operations can, if used often enough, create a performance penalty for your program.   These types of collectors are often called cooperative collectors because they require cooperation from the rest of the system to maintain the counts.  Tracing Collector  Tracing collectors are entirely dissimilar from reference counting collectors, and have opposite strengths and weaknesses. When the Tracing GC allocates a new memory chunk, the GC does not create a counter, but it does create a flag to determine when the item has been marked, and a pointer to the object that the GC keeps. The flags are not manipulated by the program itself, but are only manipulated by the GC when it performs a run.   During a GC run, the program execution typically halts. This can cause intermittent pauses in the program, pauses which can be quite long if there are many memory objects to trace.   The GC selects a set of root objects which are available to the current program scope and parent scopes. Starting from these objects, the GC identifies all pointers within the objects, called children. The object itself is marked as being alive, and then the collector moves to each child and marks it in the same way. The memory objects form a sort of tree structure, and the GC traverses this tree using recursive or stack-based methods.   At the end of the GC run, when there are no more children to be marked, all unmarked objects are considered unreachable and therefore dead. All dead objects are collected.   A few points to remember about Tracing GCs:     Tracing GCs can be used to find cycles, memory objects whose pointers form circular structures. Reference Counting schemes cannot do this.   Tracing GCs cause pauses in the program, and these pauses can become unbearably long in some complex programs that use many small memory objects.   Dead objects are not reclaimed immediately. Reclamation only occurs after a GC run. This causes a certain inefficiency in memory usage.   Tracing collectors do not require the program to account explicitly for memory counts or memory status updates. All memory tracking logic is stored inside the GC itself. This makes it easier to write extensions for these systems, and also makes it easier to install a Tracing GC in an existing system then to install a Reference Counting one.   Tracing GCs are often called uncooperative collectors because they do not require cooperation from the rest of the system to function properly. Hybrid Collectors   Sometimes, reference counting schemes will utilize Tracing systems to find cyclical garbage. Tracing systems may employ reference counts on very large objects to ensure they are reclaimed quickly. These are just two examples of hybridized garbage collectors that are more common then either of the two “pure” types described above.   In later chapters, we will discuss garbage collectors and their algorithms in more detail.   Java runtime data area  There are 5 areas     Heap   Java Stack   Method Area   Native method area   PC/Register   Java GC   Java’s GC considers objects “garbage” if they aren’t reachable through a chain starting at a garbage collection root, so these objects will be collected. Even though objects may point to each other to form a cycle, they’re still garbage if they’re cut off from the root.   See the section on unreachable objects in Appendix A: The Truth About Garbage Collection in Java Platform Performance: Strategies and Tactics (free ebook, also available on Safari) for the gory details.   Java Garbage collector handles circular-reference!   How?   There are special objects called called garbage-collection roots (GC roots). These are always reachable and so is any object that has them at its own root.   A simple Java application has the following GC roots:   Local variables in the main method The main thread Static variables of the main class   To determine which objects are no longer in use, the JVM intermittently runs what is very aptly called a mark-and-sweep algorithm. It works as follows   The algorithm traverses all object references, starting with the GC roots, and marks every object found as alive. All of the heap memory that is not occupied by marked objects is reclaimed. It is simply marked as free, essentially swept free of unused objects.   So if any object is not reachable from the GC roots(even if it is self-referenced or cyclic-referenced) it will be subjected to garbage collection. Ofcourse sometimes this may led to memory leak if programmer forgets to dereference an object.    The actual answer to this is implementation dependent. The Sun JVM keeps track of some set of root objects (threads and the like), and when it needs to do a garbage collection, traces out which objects are reachable from those and saves them, discarding the rest. It’s actually more complicated than that to allow for some optimizations, but that is the basic principle. This version does not care about circular references: as long as no live object holds a reference to a dead one, it can be GCed.   Other JVMs can use a method known as reference counting. When a reference is created to the object, some counter is incremented, and when the reference goes out of scope, the counter is decremented. If the counter reaches zero, the object is finalized and garbage collected. This version, however, does allow for the possibility of circular references that would never be garbage collected. As a safeguard, many such JVMs include a backup method to determine which objects actually are dead which it runs periodically to resolve self-references and defrag the heap.     A garbage collector starts from some “root” set of places that are always considered “reachable”, such as the CPU registers, stack, and global variables. It works by finding any pointers in those areas, and recursively finding everything they point at. Once it’s found all that, everything else is garbage.   There are, of course, quite a few variations, mostly for the sake of speed. For example, most modern garbage collectors are “generational”, meaning that they divide objects into generations, and as an object gets older, the garbage collector goes longer and longer between times that it tries to figure out whether that object is still valid or not – it just starts to assume that if it has lived a long time, chances are pretty good that it’ll continue to live even longer.   Nonetheless, the basic idea remains the same: it’s all based on starting from some root set of things that it takes for granted could still be used, and then chasing all the pointers to find what else could be in use.   Interesting aside: may people are often surprised by the degree of similarity between this part of a garbage collector and code for marshaling objects for things like remote procedure calls. In each case, you’re starting from some root set of objects, and chasing pointers to find all the other objects those refer to…     How Garbage Collection Really Works   Many people think garbage collection collects and discards dead objects. In reality, Java garbage collection is doing the opposite! Live objects are tracked and everything else designated garbage. As you’ll see, this fundamental misunderstanding can lead to many performance problems.   Garbage-Collection Roots—The Source of All Object Trees   Every object tree must have one or more root objects. As long as the application can reach those roots, the whole tree is reachable. But when are those root objects considered reachable? Special objects called garbage-collection roots (GC roots; see Figure 2.2) are always reachable and so is any object that has a garbage-collection root at its own root.   There are four kinds of GC roots in Java:      Local variables are kept alive by the stack of a thread. This is not a real object virtual reference and thus is not visible. For all intents and purposes, local variables are GC roots.   Active Java threads are always considered live objects and are therefore GC roots. This is especially important for thread local variables.   Static variables are referenced by their classes. This fact makes them de facto GC roots. Classes themselves can be garbage-collected, which would remove all referenced static variables. This is of special importance when we use application servers, OSGi containers or class loaders in general. We will discuss the related problems in the Problem Patterns section.   JNI References are Java objects that the native code has created as part of a JNI call. Objects thus created are treated specially because the JVM does not know if it is being referenced by the native code or not. Such objects represent a very special form of GC root, which we will examine in more detail in the Problem Patterns section below.   Therefore, a simple Java application has the following GC roots:      Local variables in the main method   The main thread   Static variables of the main class   Marking and Sweeping Away Garbage   To determine which objects are no longer in use, the JVM intermittently runs what is very aptly called a mark-and-sweep algorithm. As you might intuit, it’s a straightforward, two-step process:      The algorithm traverses all object references, starting with the GC roots, and marks every object found as alive.   All of the heap memory that is not occupied by marked objects is reclaimed. It is simply marked as free, essentially swept free of unused objects.    Garbage collectors which rely solely on reference counting are generally vulnerable to failing to collection self-referential structures such as this. These GCs rely on a count of the number of references to the object in order to calculate whether a given object is reachable.   Non-reference counting approaches apply a more comprehensive reachability test to determine whether an object is eligible to be collected. These systems define an object (or set of objects) which are always assumed to be reachable. Any object for which references are available from this object graph is considered ineligible for collection. Any object not directly accessible from this object is not. Thus, cycles do not end up affecting reachability, and can be collected.    Tracing collector vs. countering collector  There are two primary types of garbage collectors, although often a hybrid approach is found between these to suit particular needs. The first type, the one which might be the most intuitive, is a reference counting collector. The second one, which is most similar to what we described above, is a tracing collector.   Reference Counting Collector  When a new memory object is allocated by the GC, it is given an integer count field. Every time a pointer is made to that object, a reference, the count is increased. So long as the count is a positive non-zero integer, the object is actively being referenced and is still alive. When a reference to the object is removed, the count is decremented. When the count reaches zero, the object is dead and can be immediately reclaimed. There are a number of points to remember about Reference Counting collectors:     Circular references will never be reclaimed, even if the entire set of objects is dead.   Reference counting is pervasive: The entire program must be made aware of the system, and every pointer reference or dereference must be accompanied by an appropriate increment or decrement. Failing to maintain the count, even once in a large program, will create memory problems for your program.   Reference counting can be costly, because counts must be manipulated for every pointer operation, and the count must be tested against zero on ever decrement. These operations can, if used often enough, create a performance penalty for your program.   These types of collectors are often called cooperative collectors because they require cooperation from the rest of the system to maintain the counts.   Tracing Collector  Tracing collectors are entirely dissimilar from reference counting collectors, and have opposite strengths and weaknesses. When the Tracing GC allocates a new memory chunk, the GC does not create a counter, but it does create a flag to determine when the item has been marked, and a pointer to the object that the GC keeps. The flags are not manipulated by the program itself, but are only manipulated by the GC when it performs a run.   During a GC run, the program execution typically halts. This can cause intermittent pauses in the program, pauses which can be quite long if there are many memory objects to trace.   The GC selects a set of root objects which are available to the current program scope and parent scopes. Starting from these objects, the GC identifies all pointers within the objects, called children. The object itself is marked as being alive, and then the collector moves to each child and marks it in the same way. The memory objects form a sort of tree structure, and the GC traverses this tree using recursive or stack-based methods.   At the end of the GC run, when there are no more children to be marked, all unmarked objects are considered unreachable and therefore dead. All dead objects are collected.   A few points to remember about Tracing GCs:     Tracing GCs can be used to find cycles, memory objects whose pointers form circular structures. Reference Counting schemes cannot do this.   Tracing GCs cause pauses in the program, and these pauses can become unbearably long in some complex programs that use many small memory objects.   Dead objects are not reclaimed immediately. Reclamation only occurs after a GC run. This causes a certain inefficiency in memory usage.   Tracing collectors do not require the program to account explicitly for memory counts or memory status updates. All memory tracking logic is stored inside the GC itself. This makes it easier to write extensions for these systems, and also makes it easier to install a Tracing GC in an existing system then to install a Reference Counting one.   Tracing GCs are often called uncooperative collectors because they do not require cooperation from the rest of the system to function properly. Hybrid Collectors   Sometimes, reference counting schemes will utilize Tracing systems to find cyclical garbage. Tracing systems may employ reference counts on very large objects to ensure they are reclaimed quickly. These are just two examples of hybridized garbage collectors that are more common then either of the two “pure” types described above.   In later chapters, we will discuss garbage collectors and their algorithms in more detail.   to be callibrated  G1 is a concurrent collector that operates on discrete regions within the heap. Each region (there are by default around 2,048 of them) can belong to either the old or new generation, and the generational regions need not be contiguous. The idea behind having regions in the old generation is that when the concurrent background threads look for unreferenced objects, some regions will contain more garbage than other regions. The actual collection of a region still requires that application threads be stopped, but G1 can focus on the regions that are mostly garbage and only spend a little bit of time emptying those regions. This approach—clearing out only the mostly garbage regions—is what gives G1 its name: Garbage First. That doesn’t apply to the regions in the young generation: during a young GC, the entire young generation is either freed or promoted (to a survivor space or to the old generation). Still, the young generation is defined in terms of regions, in part because it makes resizing the generations much easier if the regions are predefined. G1 has four main operations: A young collection A background, concurrent cycle A mixed collection If necessary, a full GC We’ll look at each of those in turn, starting with the G1 young collection shown in Figure 6-6.   Reference     http://www.ibm.com/developerworks/java/library/j-jtp10283/   https://blogs.oracle.com/jonthecollector/entry/our_collectors   https://en.wikipedia.org/wiki/Garbage_collection_%28computer_science%29#Tracing_garbage_collectors   http://users.cecs.anu.edu.au/~steveb/pubs/papers/urc-oopsla-2003.pdf   https://www.dynatrace.com/resources/ebooks/javabook/   https://en.wikipedia.org/wiki/Tracing_garbage_collection   https://en.wikibooks.org/wiki/Memory_Management/Garbage_Collection   http://flyingfrogblog.blogspot.com/2013/09/how-do-reference-counting-and-tracing.html   https://www.dynatrace.com/resources/ebooks/javabook/how-garbage-collection-works/   http://stackoverflow.com/questions/1910194/how-does-java-garbage-collection-work-with-circular-references   http://www.java-books.us/j2ee_0003.php   http://www.ibm.com/developerworks/java/library/j-jtp10283/   https://blogs.oracle.com/jonthecollector/entry/our_collectors   https://en.wikipedia.org/wiki/Garbage_collection_%28computer_science%29#Tracing_garbage_collectors   http://users.cecs.anu.edu.au/~steveb/pubs/papers/urc-oopsla-2003.pdf   https://www.dynatrace.com/resources/ebooks/javabook/   https://en.wikipedia.org/wiki/Tracing_garbage_collection   https://en.wikibooks.org/wiki/Memory_Management/Garbage_Collection   http://flyingfrogblog.blogspot.com/2013/09/how-do-reference-counting-and-tracing.html   https://www.dynatrace.com/resources/ebooks/javabook/how-garbage-collection-works/   http://stackoverflow.com/questions/1910194/how-does-java-garbage-collection-work-with-circular-references   http://www.java-books.us/j2ee_0003.php  ","categories": [],
        "tags": ["java","GC"],
        "url": "/2016/12/30/Java-GC.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Java Enum Misc",
        "excerpt":"Enum Misc      All enums implicitely extends java.lang.Enum   Enum in Java are type-safe   You can specify values of enum constants at the creation time. Enum.values() returns an array of MyEnum’s values.   Enum constants are implicitly static and final and can not be changed once created.   Enum can be safely compare using:            Switch-Case Statement       == Operator       .equals() method           You can not create instance of enums by using new operator in Java because constructor of Enum in Java can only be private and Enums constants can only be created inside Enums itself.   Instance of Enum in Java is created when any Enum constants are first called or referenced in code.   An enum can be declared outside or inside a class, but NOT in a method.   An enum declared outside a class must NOT be marked static, final , abstract, protected , or private   Enums can contain constructors, methods, variables, and constant class bodies.   enum constructors can have arguments, and can be overloaded.   enum constructors can NEVER be invoked directly in code. They are always called automatically when an enum is initialized.   The semicolon at the end of an enum declaration is optional.   These are legal:      enum Foo { ONE, TWO, THREE}     enum Foo { ONE, TWO, THREE};   type safe  The advantage of this style of enumeration over the C/C++-style enum or constants is that they are type-safe, meaning that, for example, if you define a method  public void setSuit(Suit suit) { ... }  the caller cannot pass in a value that does not correspond to an enumeration value   Language level features     Since enumeration instances are all effectively singletons, they can be compared for equality using identity (“==”).   In Java 5.0, the enum keyword is introduced as a special type of class that always extends java.lang.Enum.   Note that the enumeration values are still static class members, though not declared as such.   Enum is implemented using Arrays and common operations result in constant time. So if you are thinking of an high-performance Map, EnumMap could be a decent choice for enumeration data. We have already seen many examples of Java enum in our article 10 Examples of enum in Java  and using Enum as thread-safe Singleton. In this Java tutorial, we will see simple examples of using EnumMap in Java.   All keys used in EnumMap must be  from same Enum type which is specified while creating EnumMap in Java. For example if you can not use different enum instances from two different enum.   EnumMap is ordered collection and they are maintained in the natural order of their keys( natural order of keys means  the order on which enum constant are declared inside enum type ). you can verify this while Iterating over an EnumMap in Java.   Iterators of EnumMap are fail-fast Iterator , much like of ConcurrentHashMap and doesn’t throw ConcurrentModificationException and may not show effect of any modification on EnumMap during Iteration process.   You can not insert null keys inside EnumMap in Java.  EnumMap doesn’t allow null key and throw NullPointerException, at same time null values are permitted.   EnumMap is not synchronized and it has to be synchronized manually before using it in a concurrent or multi-threaded environment. like synchronized Map in Java  you can also make EnumMap synchronized by using Collections.synchronizedMap() method and as per javadoc this should be done while creating EnumMap in java to avoid accidental non synchronized access.   The constructor of enum in java must be private any other access modifier will result in compilation error. Now to get the value associated with each coin you can define a public getValue() method inside Java enum like any normal Java class. Also, the semicolon in the first line is optional.   public enum Currency {         PENNY(1), NICKLE(5), DIME(10), QUARTER(25);         private int value;          private Currency(int value) {                 this.value = value;         } };       Since constants defined inside Enum in Java are final you can safely compare them using “==”, the equality operator as shown in following example of  Java Enum:   Java compiler automatically generates static values() method for every enum in java. Values() method returns array of Enum constants in the same order they have listed in Enum and you can use values() to iterate over values of Enum  in Java as shown in below example. Notice the order is exactly the same as defined order in the Enum.   for(Currency coin: Currency.values()){    System.out.println(\"coin: \" + coin); }     In Java, Enum can override methods also. Let’s see an example of overriding toString() method inside Enum in Java to provide a meaningful description for enums constants.   public enum Currency {   ........          @Override   public String toString() {        switch (this) {          case PENNY:               System.out.println(\"Penny: \" + value);               break;          case NICKLE:               System.out.println(\"Nickle: \" + value);               break;          case DIME:               System.out.println(\"Dime: \" + value);               break;          case QUARTER:               System.out.println(\"Quarter: \" + value);         }   return super.toString();  } };             EnumSet doesn’t have any public constructor instead it provides factory methods to create instance e.g. EnumSet.of() methods. This design allows EnumSet to internally choose between two different implementations depending upon the size of Enum constants.   If Enum has less than 64 constants than EnumSet uses RegularEnumSet class which internally uses a long variable to store those 64 Enum constants and if Enum has more keys than 64 then it uses JumboEnumSet. See my article the difference between RegularEnumSet and JumboEnumSet for more details.     An instance of Enum in Java is created when any Enum constants are first called or referenced in code.   Enum in Java can **implement the interface and override any method **like normal class It’s also worth noting that Enum in java implicitly implements both Serializable and Comparable interface. Let’s see and example of how to implement interface using Java Enum:   public enum Currency implements Runnable{   PENNY(1), NICKLE(5), DIME(10), QUARTER(25);   private int value;   ............            @Override   public void run() {   System.out.println(\"Enum in Java implement interfaces\");                     } }     You can define abstract methods inside Enum in Java and can also provide a different implementation for different instances of enum in java.  Let’s see an example of using abstract method inside enum in java    public enum Currency {         PENNY(1) {             @Override             public String color() {                 return \"copper\";             }         },         NICKLE(5) {             @Override             public String color() {                 return \"bronze\";             }         },         DIME(10) {             @Override             public String color() {                 return \"silver\";             }         },         QUARTER(25) {             @Override             public String color() {                 return \"silver\";             }         };         private int value;          public abstract String color();          private Currency(int value) {             this.value = value;         }    }        In this example since every coin will have the different color we made the color() method abstract and let each instance of Enum to define  their own color. You can get color of any coin by just calling the color() method as shown in below example of Java Enum:   System.out.println(“Color: “ + Currency.DIME.color());   So that was the comprehensive list of properties, behavior and capabilities of Enumeration type in Java. I know, it’s not easy to remember all those powerful features and that’s why I have prepared this small Microsoft powerpoint slide containing all important properties of Enum in Java. You can always come back and check this slide to revise important features of Java Enum.   RegularEnumSet vs. JumboEnumSet      Since Enum always has fixed number of instances, data-structure which is used to store Enum can be optimized depending upon number of instances and that’s why we have two different implementation of EnumSet in Java. We will take a closer look on this concept in next paragraph.   How EnumSet is implemented in Java EnumSet is an abstract class and it provides two concrete implementations, java.util.RegularEnumSet and java.util.JumboEnumSet. Main difference between RegularEnumSet and JumboEnumSet is that former uses a long variable to store elements while later uses a long[] to store its element. Since RegularEnumSet uses long variable, which is a 64 bit data type, it can only hold that much of element. That’s why when an empty EnumSet is created using EnumSet.noneOf() method, it choose RegularEnumSet if key universe (number of enum instances in Key Enum) is less than or equal to 64 and JumboEnumSet if key universe is more than 64. Here is the code which does that :   public static &lt;E extends Enum&lt;E&gt;&gt; EnumSet&lt;E&gt; noneOf(Class&lt;E&gt; elementType) {            .. ............          if (universe.length &lt;= 64)             return new RegularEnumSet&lt;E&gt;(elementType, universe);         else             return new JumboEnumSet&lt;E&gt;(elementType, universe);     }     EnumSet is not thread-safe, which means if it needs to be externally synchronized, when multiple thread access it and one of them modifies the Collection.   EnumSet can not be used to store any other object except Enum, at the same time you can not store instances of two different Enum.   EnumSet doesn’t allow Null elements.   EnumSet Iterators are fail-safe in nature.   Difference between EnumMap vs HashMap     Internally EnumMap is represented using Array and provides constant time performance for common methods e.g. get() or put(). Now let’s see few differences between EnumMap vs HashMap :   As said earlier, first and foremost difference between EnumMap and HashMap is that EnumMap is optimized for enum keys while HashMap is a general purpose Map implementation similar to Hashtable. you can not use any type other than Enum as key in EnumMap but you can use both Enum and any other Object as key in HashMap.   Another difference between EnumMap and HashMap is performance. as discussed in the previous point, due to specialized optimization done for Enum keys, EnumMap is likely to perform better than HashMap when using enum as key object.   One more thing which can be considered as the difference between HashMap and EnumMap is the probability of Collision. Since Enum is internally maintained as array ** and they are **stored in their natural order using ordinal(), as shown in following code which is taken from put() method of EnumMap. Since EnumMap doesn’t call hashCode method on keys, there is no chance of collision.   int index = ((Enum)key).ordinal();     Object oldValue = vals[index];     vals[index] = maskNull(value);     you can use both == and equals() method to compare Enum, they will produce same result because equals() method of Java.lang.Enum internally uses == to compare enum in Java. Since every Enum in Java implicitly extends java.lang.Enum ,and since equals() method is declared final, there is no chance of overriding equals method in user defined enum        But there are still some slight difference, because  ==  (equality operator) being operator and equals() being method. – Using == for comparing Enum can prevent NullPointerException –  == method provides type safety during compile time       A fail-fast system is nothing but immediately report any failure that is likely to lead to failure. When a problem occurs, a fail-fast system fails immediately. In Java, we can find this behavior with iterators. Incase, you have called iterator on a collection object, and another thread tries to modify the collection object, then concurrent modification exception will be thrown. This is called fail-fast. - See more at:   Real world Examples of Enum in Java     Enum as Thread Safe Singleton   Strategy Pattern using Enum. to implement the Strategy interface and define individual strategy   Enum as replacement of Enum String or int pattern. There is now no need to use String or integer constant to represent fixed set of things e.g. status of object like ON and OFF for a button or START, IN PROGRESS and DONE for a Task. Enum is much better suited for those needs as it provide compile time type safety and better debugging assistent than String or Integer.   Enum as State Machine   Enum Java valueOf example. “You could also include valueOf() method of enum in java which is added by compiler in any enum along with values() method. Enum valueOf() is a static method which takes a string argument and can be used to convert a String into an enum. One think though you would like to keep in mind is that valueOf(String) method of enum will throw “Exception in thread “main” java.lang.IllegalArgumentException: No enum const class” if you supply any string other than enum values.   Reverse lookup  Often in your object model it is common to have data that is naturally “associated” with an enumeration. Since an enum is a class, it is easy to represent this associated information as class fields. Often it is desirable to “lookup” the associated enumeration using the field value. This is easy to do using a static java.util.Map. Take, for example, a Status enum that has an associated status code.  public enum Status {      WAITING(0),      READY(1),      SKIPPED(-1),      COMPLETED(5);       private static final Map&lt;Integer,Status&gt; lookup           = new HashMap&lt;Integer,Status&gt;();       static {           for(Status s : EnumSet.allOf(Status.class))                lookup.put(s.getCode(), s);      }       private int code;       private Status(int code) {           this.code = code;      }       public int getCode() { return code; }       public static Status get(int code) {           return lookup.get(code);      } }  Sleek EnumMap  Why would I use an EnumMap rather than a HashMap?  The primary reasons boil down to some inherent advantages of Java’s enum as stated in the Javadoc documentation for EnumMap: “Enum maps are represented internally as arrays. This representation is extremely compact and efficient.” Later in the same Javadoc documentation, there is an “Implementation Note” that states: “All basic operations execute in constant time. They are likely (though not guaranteed) to be faster than their HashMap counterparts.”   The Javadoc documentation states similar advantages for the EnumSet over the HashSet:      Enum sets are represented internally as bit vectors. This representation is extremely compact and efficient. The space and time performance of this class should be good enough to allow its use as a high-quality, typesafe alternative to traditional int-based ‘bit flags.’  … Implementation note: All basic operations execute in constant time. They are likely (though not guaranteed) to be much faster than their HashSet counterparts. Even bulk operations execute in constant time if their argument is also an enum set.    Reference     http://crunchify.com/why-and-for-what-should-i-use-enum-java-enum-examples/   http://www.ajaxonomy.com/2007/java/making-the-most-of-java-50-enum-tricks   http://www.javaworld.com/article/2073430/the-sleek-enummap-and-enumset.html   http://javarevisited.blogspot.com/2011/08/enum-in-java-example-tutorial.html#ixzz4W6bC2mUV   http://javarevisited.blogspot.in/2011/08/enum-in-java-example-tutorial.html   http://www.java67.com/2013/11/difference-between-regularenumset-and-jumboenumset-java.html#ixzz4WANO22q5   http://javarevisited.blogspot.com/2011/08/enum-in-java-example-tutorial.html#ixzz4WAP43bUB   http://javarevisited.blogspot.com/2012/09/difference-between-enummap-and-hashmap-in-java-vs.html#ixzz4WAQcrjkC   http://javarevisited.blogspot.com/2013/04/how-to-compare-two-enum-in-java-equals.html#ixzz4WARs9slQ  ","categories": [],
        "tags": ["java","enum"],
        "url": "/2017/01/02/Enum-Misc.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "IntelliJ Tips",
        "excerpt":"Shortcuts  Expand/collapse method body in code editor  Cmd + +/- to expand and collapse a method body  Show java doc          Ctrl+J: To show JavaDoc       Cmd+Alt+B: To show interface implementations        Alt+Enter: when cursor on class declare line, press Alt+Enter can quickly create a unit test against this test class       Ctrl+Shift+A: Action window, just like sublime, search IDE actions   F11: add bookmark,   Shift+F11: show bookmarks   Shift + Escape: close bottom source pannel or left side barGo to settings, Editor-&gt;General-&gt;Mouse-&gt; change font size (Zoom)   Alt + F: Git refresh : Assign it in Intellij key map   Ctrl + T: swich different editorsAlt + F1: Show in , then preses ‘1’ to show file in project viewer, like “scroll from source”use Ctrl+Q to show quick documentation for the element at caret. to show javadocAlt + F12: to open terminal window, then ‘mvn package’   Ctrl+I: implement methodsAlt+A: code completion, this is customized in keymap, for ‘basic’Press Tab / Shift+Tab. To change indentation of a text fragmentCtrl+Alt+I To fix indentationCtrl+D  Duplicate Line or SelectionAlt + Insert: generate code (getter or setter, toString)Ctrl+F12: List all opened files, press Enter to selectAlt+1: to show the project tree window, press Alt+1 again will close the project structureAlt+7: show structureGo to interface implementations, the shortcut In PC, it is CTRL + ALT + B: JetBrains navigation documentation.   maximize edito pane: Ctrl + Shift + F12 (Default keymap).- Ctrl+R: repalce- F6: move file, Shift+F6: rename file- Ctrl+Alt+ &lt;-: go back to previous position- Ctrl+Alt+S: show settings- bookmakr: F11 toggle for annonymous bookmark, Shift+F11: show all bookmarks. Ctrl+F11: then press 0,1,2,3, etc. then Ctrl +1, Ctrl +3, to go to that bookmarkmaster password: hello123- copy file name: focus file in “project view”, press Ctrl+C- Incremental search: Ctrl+F, then use up/down arrow keys to navigate- Ctrl+Shift+N: open files by file name pattern- Press Ctrl+Shift+B . Press the Ctrl+Shift keys and hover your mouse pointer over the symbol. When the symbol turns to a hyperlink, click it without releasing Ctrl+Shift keys. The type declaration of the symbol opens in the editor.        Ctrl+Shift+F7: highlight all references (of selected method) put carpet on a name, Ctrl+B will show the definition Ctrl+Shift+Alt+F: copy current file name – go to matching braceCtrl+} will move to the close bracket.Ctrl+{ will move to the open bracket – split editorgo to menu “window” -&gt; “editor tabs” -&gt; split Firslty open wealth-access-ui project, which only contains files at root, then right click in left pane, and chose import wealth-acecss-ui, it will load module.Use F2/Shift+F2 keys to jump between highlighted syntax errors.Use Ctrl+Alt+Up/Ctrl+Alt+Down shortcuts to jump between compiler error messages or search operation results.Use Ctrl+J to complete any valid Live Template abbreviation if you don’t remember it. For example, type it and press Ctrl+J to see what happens.       Navigating to the declaration of a symbol Place the caret at the desired symbol in the editor.Do one of the following:On the main menu, choose Navigate | Declaration.Press Ctrl+B.Click the middle mouse button.Keeping Ctrl pressed, point to the symbol, and click, when it turns to a hyperlink. You can also see declaration at the tooltip while keeping Ctrl pressed.   Ctrl+Shift+A: Action window, just like sublime, search IDE actions F11: add bookmark,  Shift+F11: show bookmarks Shift + Escape: close bottom source pannel or left side bar Go to settings, Editor-&gt;General-&gt;Mouse-&gt; change font size (Zoom) Alt + F: Git refresh : Assign it in Intellij key map Ctrl + T: swich different editors Alt + F1: Show in , then preses ‘1’ to show file in project viewer, like “scroll from source” use Ctrl+Q to show quick documentation for the element at caret. to show javadoc Alt + F12: to open terminal window, then ‘mvn package’ Ctrl+I: implement methods Alt+A: code completion, this is customized in keymap, for ‘basic’ Press Tab / Shift+Tab. To change indentation of a text fragment Ctrl+Alt+I To fix indentation Ctrl+D  Duplicate Line or Selection Alt + Insert: generate code (getter or setter, toString) Ctrl+F12: List all opened files, press Enter to select Alt+1: to show the project tree window, press Alt+1 again will close the project structure Alt+7: show structure Go to interface implementations, the shortcut In PC, it is CTRL + ALT + B: JetBrains navigation documentation.      maximize edito pane: Ctrl + Shift + F12 (Default keymap).   Ctrl+R: repalce   F6: move file, Shift+F6: rename file   Ctrl+Alt+ &lt;-: go back to previous position   Ctrl+Alt+S: show settings   bookmakr: F11 toggle for annonymous bookmark, Shift+F11: show all bookmarks. Ctrl+F11: then press 0,1,2,3, etc. then Ctrl +1, Ctrl +3, to go to that bookmark master password: hello123   copy file name: focus file in “project view”, press Ctrl+C   Incremental search: Ctrl+F, then use up/down arrow keys to navigate   Ctrl+Shift+N: open files by file name pattern        Press Ctrl+Shift+B . Press the Ctrl+Shift keys and hover your mouse pointer over the symbol. When the symbol turns to a hyperlink, click it without releasing Ctrl+Shift keys. The type declaration of the symbol opens in the editor.       Ctrl+Shift+F7: highlight all references (of selected method)   put carpet on a name, Ctrl+B will show the definition   Ctrl+Shift+Alt+F: copy current file name   – go to matching brace Ctrl+} will move to the close bracket. Ctrl+{ will move to the open bracket   – split editor go to menu “window” -&gt; “editor tabs” -&gt; split   Firslty open wealth-access-ui project, which only contains files at root, then right click in left pane, and chose import wealth-acecss-ui, it will load module. Use F2/Shift+F2 keys to jump between highlighted syntax errors. Use Ctrl+Alt+Up/Ctrl+Alt+Down shortcuts to jump between compiler error messages or search operation results. Use Ctrl+J to complete any valid Live Template abbreviation if you don’t remember it. For example, type it and press Ctrl+J to see what happens.      Navigating to the declaration of a symbol   Place the caret at the desired symbol in the editor. Do one of the following: On the main menu, choose Navigate | Declaration. Press Ctrl+B. Click the middle mouse button. Keeping Ctrl pressed, point to the symbol, and click, when it turns to a hyperlink. You can also see declaration at the tooltip while keeping Ctrl pressed.   By defining a Scope when searching, you can include/exclude arbitrary files/folders from that scope. Detailed Answer One way to achieve your requirement (excluding files and folders from a search) is to define a custom scope. This is specifically useful because sometimes you just want to exclude a folder from your search and not from the whole project. Follow these steps: Edit -&gt; Find -&gt; Find in path or press Ctrl+Shift+F.Choose Custom in the Scope section and then choose    how to add xx-properties project to xx project as dependencies  select project and press F4 to open properites, chose ‘module’ in left pane and then click “+”, chose ‘import module’, then chose the properties project   Intelij classes   Annotation Nullable  /*  * Copyright 2000-2009 JetBrains s.r.o.  *  * Licensed under the Apache License, Version 2.0 (the \"License\");  * you may not use this file except in compliance with the License.  * You may obtain a copy of the License at  *  * http://www.apache.org/licenses/LICENSE-2.0  *  * Unless required by applicable law or agreed to in writing, software  * distributed under the License is distributed on an \"AS IS\" BASIS,  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  * See the License for the specific language governing permissions and  * limitations under the License.  */  package org.jetbrains.annotations;  import java.lang.annotation.*;  /**  * An element annotated with Nullable claims &lt;code&gt;null&lt;/code&gt; value is perfectly &lt;em&gt;valid&lt;/em&gt;  * to return (for methods), pass to (parameters) and hold (local variables and fields).  * Apart from documentation purposes this annotation is intended to be used by static analysis tools  * to validate against probable runtime errors and element contract violations.  * @author max  */ @Documented @Retention(RetentionPolicy.CLASS) @Target({ElementType.METHOD, ElementType.FIELD, ElementType.PARAMETER, ElementType.LOCAL_VARIABLE}) public @interface Nullable {   String value() default \"\"; }  ","categories": [],
        "tags": ["java","intelliJ"],
        "url": "/2017/01/03/IntelliJ-Tips.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Arbitrage vs Heading",
        "excerpt":"What is the difference between arbitrage and hedging?      Hedging involves the concurrent use of more than one bet in opposite directions to limit risk of serious investment loss.   Arbitrage is the practice of trading a price difference between more than one market for the same good in an attempt **to profit from the imbalance.  These two concepts play important roles in finance, economics and investments.   Each transaction relies involves two competing types of trades: betting short versus betting long (hedging) and **buying versus selling **(arbitrage). Both are used by traders who operate in volatile, dynamic market environments. Other than these two similarities, however, they are very different techniques that are used for very different purposes.   Arbitrage   Arbitrage involves both purchase and sale within a very short period of time. If a good is being sold for $100 in one market and $108 in another market, a savvy trader could purchase the $100 item and then sell it in the other market for $108. The trader enjoys a risk-free return of 8% ($8 / $100), minus any transaction or transportation expenses.   With the proliferation of high-speed computing technology and constant price information, arbitrage is much more difficult in financial markets than it used to be. Still, arbitrage opportunities can be found in the forex market, in bonds, in futures markets and sometimes in equities.   Hedging   Hedging is not the pursuit of risk-free **trades; instead, it is an attempt to **reduce known risks while trading. Options contacts, forward contracts, swaps and derivatives are all used by traders to purchase opposite positions in the market. By betting against both upward and downward movement, the hedger can ensure a certain amount of **reduced gain or loss ** on a trade.   Hedging can take place almost anywhere, but it has become a particularly important aspect of financial markets, business management and gambling. Much like any other risk/reward trade, hedging results in lower returns for the party involved, but it can offer significant protection against downside risk.  ","categories": [],
        "tags": [],
        "url": "/2017/01/04/Arbitrage-vs-Hedging.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Java 8 Tips",
        "excerpt":"This blog is listing key new features introduced in Java 8   It is best to think of a lambda expression as a function, not an object, and to accept that it can be converted to a functional interface.   Clean Code in Java 8   Here is the sample code in Java 8  calss DiscountService{    Integer discount =getDiscountPercentage(customer.getMemberCard()); }   New comparator method in Java 8   List&lt;Track&gt; tracks = asList(new Track(\"Bakai\", 524),                             new Track(\"Violets for Your Furs\", 378),                             new Track(\"Time Was\", 451));  Track shortestTrack = tracks.stream()                             .min(Comparator.comparing(track -&gt; track.getLength()))                             .get();  assertEquals(tracks.get(1), shortestTrack);   When we think about maximum and minimum elements, the first thing we need to think about is the ordering that we’re going to be using. When it comes to finding the shortest track, the ordering is provided by the length of the ”   In order to inform the Stream that we’re using the length of the track, we give it a Comparator. Conveniently, Java 8 has added a static method called comparing that lets us build a comparator using keys. Previously, we always encountered an ugly pattern in which we had to write code that got a field out of both the objects being compared, then compare these field values. Now, to get the same element out of both elements being compared, we just provide a getter function for the value. In this case we’ll use length, which is a getter function in disguise. It’s worth reflecting on the comparing method for a moment. This is actually a function that takes a function and returns a function. Pretty meta, I know, but also incredibly useful. At any point in the past, this method could have been added to the Java standard library, but the poor readability and verbosity issues surrounding anonymous inner classes would have made it impractical. Now, with lambda expressions, it’s convenient and concise. ” But thinking of passing code to methods as a mere consequence of Streamsdownplays its range of uses within Java 8. It gives you a new concise way to express behavior parameterization.   It might sound surprising, but interfaces in Java 8 can now declare methods with implementation code; this can happen in two ways. First, Java 8 allows static methods inside interfaces. Second, Java 8 introduces a new feature called default methods that allows you to provide a default implementation for methods in an interface. In other words, interfaces can provide concrete implementation for methods. As a result, existing classes implementing an interface will automatically inherit the default implementations if they don’t provide one explicitly. This allows you to evolve interfaces nonintrusively. You’ve been using several default methods all along. Two examples you’ve seen are sort in the List interface and stream in the Collection interface. Wow! Are interfaces like abstract classes now? Yes and no; there are fundamental differences, which we explain in this chapter. But more important, why should you care about default methods? The main users of default methods are library designers. As we explain later, default methods were introduced to evolve libraries such as the Java API in a compatible way,   Now that static methods can exist inside interfaces, such utility classes in your code can go away and their static methods can be moved inside an interface. These companion classes will remain in the Java API in order to preserve backward compatibility. Adding a new method to an interface is binary compatible; this means existing class file implementations will still run without the implementation of the new method, if there’s no attempt to recompile them. In this case the game will still run (unless it’s recompiled) despite adding the method setRelativeSize to the Resizable interface   Abstract classes vs. interfaces in Java 8  So what’s the difference between an abstract class and an interface? They both can contain abstract methods and methods with a body.           First, a class can extend only from one abstract class, but a class can implement multiple interfaces.            Second, an abstract class can enforce a common state through instance variables (fields). An interface can’t have instance variables.       Keeping interfaces minimal and orthogonal lets you achieve great reuse and composition of behavior inside your codebase.   Minimal interfaces with orthogonal functionalities Inheritance considered harmful Inheritance shouldn’t be your answer to everything when it comes down to reusing code. For example, inheriting from a class that has 100 methods and fields just to reuse one method is a bad idea, because it adds unnecessary complexity. You’d be better off using delegation: create a method that calls directly the method of the class you need via a member variable. This is why you’ll sometime find classes that are declared “final” intentionally: they can’t be inherited from to prevent this kind of antipattern or have their core behavior messed with. Note that sometimes final classes have a place; for example, String is final because we don’t want anybody to be able to interfere with such core functionality.   Three resolution rules to know  There are three rules to follow when a class inherits a method with the same signature from multiple places (such as another class or interface):           Classes always win. A method declaration in the class or a superclass takes priority over any default method declaration.            Otherwise, sub-interfaces win: the method with the same signature in the most specific default-providing interface is selected. (If B extends A, B is more specific than A).            Finally, if the choice is still ambiguous, the class inheriting from multiple interfaces has to explicitly select which default method implementation to use by overriding it and calling the desired method explicitly.       These are the only rules you need to know!   Lambda Interfaces   This conversion to interfaces is what makes lambda expressions so compelling. The syntax is short and simple.   BiFunction&lt;String, String, Integer&gt; comp    = (first, second) -&gt; Integer.compare(first.length(), second.length());   The expression System.out::printlnis a method reference that is equivalent to the lambda expression x -&gt; System.out.println(x).   There are three principal cases:      object::instanceMethod   Class::staticMethod   Class::instanceMethod   In the third case, the first parameter becomes the target of the method. For example, String::compareToIgnoreCaseis the same as (x, y) -&gt; x.compareToIgnoreCase(y).   Just like lambda expressions, method references don’t live in isolation. They are always turned into instances of functional interfaces.   Constructor References  Constructor references are just like method references, except that the name of the method is newå. For example, Button::new is a reference to a Button constructor. Which constructor? It depends on the context.   List&lt;String&gt; labels = ...; Stream&lt;Button&gt; stream = labels.stream().map(Button::new); List&lt;Button&gt; buttons = stream.collect(Collectors.toList());   For example, suppose we want to have an array of buttons. The Stream interface has a toArraymethod that returns an Object array:   Object[] buttons = stream.toArray();   we need to refine our understanding of a lambda expression. A lambda expression has three ingredients:      A block of code   Parameters   Values for the free variables, that is, the variables that are not parameters and not defined inside the code   The technical term for a block of code together with the values of the free variables is a closure. If someone gloats that their language has closures, rest assured that Java has them as well. In Java, lambda expressions are closures. In fact, inner classes have been closures all along. Java 8 gives us closures with an attractive syntax.   Inner classes can also capture values from an enclosing scope. Before Java 8, inner classes were only allowed to access finallocal variables. This rule has now been relaxed to match that for lambda expressions. An inner class can access any effectively final local variable—that is, any variable whose value does not change.   When you use the this keyword in a lambda expression, you refer to the this parameter of the method that creates the lambda. For example, consider   public class Application {    public void doWork() {       Runnable runner = () -&gt; { ...; System.out.println(this.toString()); ... };       ...    } }   The expression this.toString()calls the toString method of the Application object, not the Runnable instance. There is nothing special about the use of this in a lambda expression. The scope of the lambda expression is nested inside the doWork method, and this has the same meaning anywhere in that method.   default methods  The Java designers decided to solve this problem once and for all by allowing interface methods with concrete implementations (called default methods). Those methods can be safely added to existing interfaces.   interface Person {    long getId();    default String getName() { return \"John Q. Public\"; } }   The interface has two methods: getId, which is an abstract method, and the default method getName. A concrete class that implements the Person interface must, of course, provide an implementation of getId, but it can choose to keep the implementation of getName or to override it.   Default methods put an end to the classic pattern of providing an interface and an abstract class that implements most or all of its methods, such as Collection/AbstractCollectionor/WindowListener/WindowAdapter. Now you can just implement the methods in the interface.   To compare Person objects by name, use Comparator.comparing(Person::getName).   we have compared strings by length with the lambda expression  (first, second) -&gt; Integer.compare(first.length(), second.length()).   But with the static compare method, we can do much better and simply use  Comparator.comparing(String::length).   In Java 8, static methods have been added to quite a few interfaces. For example, the Comparator interface has a very useful static comparing method that accepts a “key extraction” function and yields a comparator that compares the extracted keys.   Stream vs collections   A stream seems superficially similar to a collection, allowing you to transform and retrieve data. But there are significant differences:     A stream does not store its elements. They may be stored in an underlying collection or generated on demand.   Stream operations don’t mutate their source. Instead, they return new streams that hold the result.   Stream operations are lazy when possible. This means they are not executed until their result is needed. For example, if you only ask for the first five long words instead of counting them all, then the filter method will stop filtering after the fifth match. As a consequence, you can even have infinite streams!   Streams follow the “what, not how” principle. In our stream example, we describe what needs to be done: get the long words and count them. We don’t specify in which order, or in which thread, this should happen.   Work with streams  When you work with streams, you set up a pipeline of operations in three stages.     You create a stream.   You specify intermediate operations for transforming the initial stream into others, in one or more steps.   You apply a terminal operation to produce a result. This operation forces the execution of the lazy operations that precede it. Afterwards, the stream can no longer be used.   long count = words.parallelStream().filter(w -&gt; w.length() &gt; 12).count();   Stream operations are not executed on the elements in the order in which they are invoked on the streams. In our example, nothing happens until count is called. When the count method asks for the first element, then the filter method starts requesting elements, until it finds one that has length &gt; 12.   To produce infinite sequences such as 0 1 2 3 …, use the iterate method instead. It takes a “seed” value and a function (technically, a UnaryOperator), and repeatedly applies the function to the previous result. For example,   Stream&lt;BigInteger&gt; integers    = Stream.iterate(BigInteger.ZERO, n -&gt; n.add(BigInteger.ONE));   The first element in the sequence is the seed   You can use the following statement to split a string into words:   Stream&lt;String&gt; words    = Pattern.compile(\"[\\\\P{L}]+\").splitAsStream(contents);   The static Files.linesmethod returns a Stream of all lines in a file.   The Stream interface has AutoCloseableas a superinterface. When the close method is called on the stream, the underlying file is also closed.   To make sure that this happens, it is best to use the Java 7 try-with-resources statement:   try (Stream&lt;String&gt; lines = Files.lines(path)) {    Do something with lines }   The stream, and the underlying file with it, will be closed when the try block exits normally or through an exception.   The filter, map, and flatMapMethods A stream transformation reads data from a stream and puts the transformed data into another stream. You have already seen the filter transformation that yields a new stream with all elements that match a certain condition.   2.3. The filter, map, and flatMap Methods A stream transformation reads data from a stream and puts the transformed data into another stream. You have already seen the filtertransformation that yields a new stream with all elements that match a certain condition. Here, we transform a stream of strings into another stream containing only long words:   List&lt;String&gt; wordList = ...; Stream&lt;String&gt; words = wordList.stream(); Stream&lt;String&gt; longWords = words.filter(w -&gt; w.length() &gt; 12);   The argument of filter is a Predicate—that is, a function from T to boolean.   Often, you want to transform the values in a stream in some way. Use the map method and pass the function that carries out the transformation. For example, you can transform all words to lowercase like this:   Stream&lt;String&gt; lowercaseWords = words.map(String::toLowerCase);   Here, we used mapwith a method reference. Often, you will use a lambda expression instead:   Stream&lt;Character&gt; firstChars = words.map(s -&gt; s.charAt(0));   The resulting stream contains the first character of each word.   When you use map, a function is applied to each element, and the return values are collected in a new stream. Now suppose that you have a function that returns not just one value but a stream of values, such as this one:   public static Stream&lt;Character&gt; characterStream(String s) {    List&lt;Character&gt; result = new ArrayList&lt;&gt;();    for (char c : s.toCharArray()) result.add(c);    return result.stream(); }   For example, characterStream(“boat”)is the stream [‘b’, ‘o’, ‘a’, ‘t’]. Suppose you map this method on a stream of strings:   Stream&lt;Stream&lt;Character&gt;&gt; result = words.map(w -&gt; characterStream(w));   You will get a stream of streams, like [… [‘y’, ‘o’, ‘u’, ‘r’], [‘b’, ‘o’, ‘a’, ‘t’], …] To flatten it out to a stream of characters [… ‘y’, ‘o’, ‘u’, ‘r’, ‘b’, ‘o’, ‘a’, ‘t’, …], use the flatMapmethod instead of map:   Stream&lt;Character&gt; letters = words.flatMap(w -&gt; characterStream(w))     // CallscharacterStream on each word and flattens the results   NOTE   You may find a flatMap method in classes other than streams. It is a general concept in computer science. Suppose you have a generic type G (such as Stream) and functions ffrom some type T to Gand g from U to G. Then you can compose them, that is, first apply f and then g, by using flatMap. This is a key idea in the theory of monads. But don’t worry—you can use flatMapwithout knowing anything about monads.   This method is particularly useful for cutting infinite streams down to size. For example,   Stream&lt;Double&gt; randoms = Stream.generate(Math::random).limit(100);   yields a stream with 100 random numbers.   The peek method yields another stream with the same elements as the original, but a function is invoked every time an element is retrieved. That is handy for debugging:   Object[] powers = Stream.iterate(1.0, p -&gt; p * 2)    .peek(e -&gt; System.out.println(\"Fetching \" + e))    .limit(20).toArray();   When an element is actually accessed, a message is printed. This way you can verify that the infinite stream returned by iterate is processed lazily.   The stream transformations of the preceding sections were stateless. When an element is retrieved from a filtered or mapped stream, the answer does not depend on the previous elements. There are also a few stateful transformations. For example, the distinct method returns a stream that yields elements from the original stream, in the same order, except that duplicates are suppressed.   The stream must obviously remember the elements that it has already seen.   Stream&lt;String&gt; uniqueWords    = Stream.of(\"merrily\", \"merrily\", \"merrily\", \"gently\").distinct();    // Only one\"merrily\" is retained   The sorted method must see the entire stream and sort it before it can give out any elements—after all, the smallest one might be the last one. Clearly, you can’t sort an infinite stream.   There are several sorted methods. One works for streams of Comparableelements, and another accepts a Comparator. Here, we sort strings so that the longest ones come first:   Click here to view code image   Stream longestFirst =    words.sorted(Comparator.comparing(String::length).reversed()); Of course, you can sort a collection without using streams. The sorted method is useful when the sorting process is a part of a stream pipeline.   NOTE   The Collections.sortmethod sorts a collection in place, whereas Stream.sortedreturns a new sorted stream.   The methods that we cover in this section are called reductions. They reduce the stream to a value that can be used in your program. Reductions are terminal operations. After a terminal operation has been applied, the stream ceases to be usable.   In Java 8, the Optional type is the preferred way of indicating a missing return value. We discuss the Optional type in detail in the next section. Here is how you can get the maximum of a stream:   Click here to view code image   Optional largest = words.max(String::compareToIgnoreCase); if (largest.isPresent())    System.out.println(\"largest: \" + largest.get());   reduce. Each segment needs to start out with its own empty hash set, and reduce only lets you supply one identity value. Instead, use collect. It takes three arguments:           A supplier to make new instances of the target object, for example, a constructor for a hash set            An accumulatorthat adds an element to the target, for example, an addmethod            A combiner that merges two objects into one, such as addAll       NOTE   The target object need not be a collection. It could be a StringBuilderor an object that tracks a count and a sum.   Here is how the collect method works for a hash set:   Click here to view code image   HashSet result = stream.collect(HashSet::new, HashSet::add, HashSet::addAll);   In practice, you don’t have to do that because there is a convenient Collector interface for these three functions, and a Collectors class with factory methods for common collectors. To collect a stream into a list or set, you can simply call   Click here to view code image   List result = stream.collect(Collectors.toList());   or   Click here to view code image   Set result = stream.collect(Collectors.toSet());   If you want to control which kind of set you get, use the following call instead:   Click here to view code image   TreeSet result = stream.collect(Collectors.toCollection(TreeSet::new));   Suppose you want to collect all strings in a stream by concatenating them. You can call   String result = stream.collect(Collectors.joining());   If you want a delimiter between elements, pass it to the joiningmethod:   Click here to view code image   String result = stream.collect(Collectors.joining(“, “));   If your stream contains objects other than strings, you need to first convert them to strings, like this:   Click here to view code image   String result = stream.map(Object::toString).collect(Collectors.joining(“, “));                  If you want to reduce the stream results to a sum, average, maximum, or minimum, then use one of the methods summarizing(Int       Long       Double). These methods take a function that maps the stream objects to a number and yield a result of type (Int       Long       Double)SummaryStatistics, with methods for obtaining the sum, average, maximum, and minumum.           Click here to view code image   IntSummaryStatistics summary = words.collect(    Collectors.summarizingInt(String::length)); double averageWordLength = summary.getAverage(); double maxWordLength = summary.getMax();   NOTE   So far, you have seen how to reduce or collect stream values. But perhaps you just want to print them or put them in a database. Then you can use the forEachmethod:   stream.forEach(System.out::println);   The function that you pass is applied to each element. On a parallel stream, it’s your responsibility to ensure that the function can be executed concurrently. We discuss this in Section 2.13, “Parallel Streams,” on page 40.   On a parallel stream, the elements can be traversed in arbitrary order. If you want to execute them in stream order, call forEachOrderedinstead. Of course, you might then give up most or all of the benefits of parallelism.   The forEachand forEachOrderedmethods are terminal operations. You cannot use the stream again after calling them. If you want to continue using the stream, use peekinstead—see   In the common case that the values should be the actual elements, use Function.identity()for the second function.   Click here to view code image   Map&lt;Integer, Person&gt; idToPerson = people.collect(    Collectors.toMap(Person::getId, Function.identity())); If there is more than one element with the same key, the collector will throw an IllegalStateException. You can override that behavior by supplying a third function argument that determines the value for the key, given the existing and the new value. Your function could return the existing value, the new value, or a combination of them.   Here, we construct a map that contains, for each language in the available locales, as key its name in your default locale (such as “German”), and as value its localized name (such as “Deutsch”).   Click here to view code image   Stream locales = Stream.of(Locale.getAvailableLocales()); Map&lt;String, String&gt; languageNames = locales.collect(    Collectors.toMap(       l -&gt; l.getDisplayLanguage(),       l -&gt; l.getDisplayLanguage(l),       (existingValue, newValue) -&gt; existingValue)); We don’t care that the same language might occur twice—for example, German in Germany and in Switzerland, and we just keep the first entry.   However, suppose we want to know all languages in a given country. Then we need a Map&lt;String, Set&gt;. For example, the value for \"Switzerland\"is the set [French, German, Italian]. At first, we store a singleton set for each language. Whenever a new language is found for a given country, we form the union of the existing and the new set.   Click here to view code image   Map&lt;String, Set&gt; countryLanguageSets = locales.collect(    Collectors.toMap(       l -&gt; l.getDisplayCountry(),       l -&gt; Collections.singleton(l.getDisplayLanguage()),       (a, b) -&gt; { // Union of a and b          Set r = new HashSet&lt;&gt;(a);          r.addAll(b);          return r; })); You will see a simpler way of obtaining this map in the next section.   If you want a TreeMap, then you supply the constructor as the fourth argument. You must provide a merge function. Here is one of the examples from the beginning of the section, now yielding a TreeMap:   Click here to view code image   Map&lt;Integer, Person&gt; idToPerson = people.collect(    Collectors.toMap(       Person::getId,       Function.identity(),       (existingValue, newValue) -&gt; { throw new IllegalStateException(); },       TreeMap::new));   For example, if you want sets instead of lists, you can use the Collectors.toSetcollector that you saw in the preceding section:   Click here to view code image   Map&lt;String, Set&gt; countryToLocaleSet = locales.collect(    groupingBy(Locale::getCountry, toSet()));   Several other collectors are provided for downstream processing of grouped elements:   • countingproduces a count of the collected elements. For example,   Click here to view code image   Map&lt;String, Long&gt; countryToLocaleCounts = locales.collect(    groupingBy(Locale::getCountry, counting())); counts how many locales there are for each country.                  • summing(Int       Long       Double) takes a function argument, applies the function to the downstream elements, and produces their sum. For example,           Click here to view code image   Map&lt;String, Integer&gt; stateToCityPopulation = cities.collect(    groupingBy(City::getState, summingInt(City::getPopulation))); computes the sum of populations per state in a stream of cities.   • maxBy and minBytake a comparator and produce maximum and minimum of the downstream elements. For example,   Click here to view code image   Map&lt;String, City&gt; stateToLargestCity = cities.collect(    groupingBy(City::getState,       maxBy(Comparator.comparing(City::getPopulation)))); produces the largest city per state.   • mapping applies a function to downstream results, and it requires yet another collector for processing its results. For example,   Click here to view code image   Map&lt;String, Optional&gt; stateToLongestCityName = cities.collect(    groupingBy(City::getState,       mapping(City::getName,          maxBy(Comparator.comparing(String::length))))); Here, we group cities by state. Within each state, we produce the names of the cities and reduce by maximum length.   The mappingmethod also yields a nicer solution to a problem from the preceding section, to gather a set of all languages in a country.   Click here to view code image   Map&lt;String, Set&gt; countryToLanguages = locales.collect(    groupingBy(l -&gt; l.getDisplayCountry(),       mapping(l -&gt; l.getDisplayLanguage(),          toSet()))); In the preceding section, I used toMap instead of groupingBy. In this form, you don’t need to worry about combining the individual sets.   • If the grouping or mapping function has return type int, long, or double, you can collect elements into a summary statistics object, as discussed in Section 2.9, “Collecting Results,” on page 33. For example,   Click here to view code image   Map&lt;String, IntSummaryStatistics&gt; stateToCityPopulationSummary = cities.collect(    groupingBy(City::getState,       summarizingInt(City::getPopulation))); Then you can get the sum, count, average, minimum, and maximum of the function values from the summary statistics objects of each group.   • Finally, the reducingmethods apply a general reduction to downstream elements. There are three forms: reducing(binaryOperator), reducing(identity, binaryOperator), and reducing(identity, mapper, binaryOperator). In the first form, the identity is null. (Note that this is different from the forms of Stream::reduce, where the method without an identity parameter yields an Optional result.) In the third form, the mapperfunction is applied and its values are reduced.   Here is an example that gets a comma-separated string of all city names in each state. We map each city to its name and then concatenate them.   Click here to view code image   Map&lt;String, String&gt; stateToCityNames = cities.collect(    groupingBy(City::getState,       reducing(“”, City::getName,          (s, t) -&gt; s.length() == 0 ? t : s + “, “ + t))); As with Stream.reduce, Collectors.reducingis rarely necessary. In this case, you can achieve the same result more naturally as   Click here to view code image   Map&lt;String, String&gt; stateToCityNames = cities.collect(    groupingBy(City::getState,       mapping(City::getName,          joining(“, “)))); Frankly, the downstream collectors can yield very convoluted expressions. You should only use them in connection with groupingBy or partitioningBy to process the “downstream” map values. Otherwise, simply apply methods such as map, reduce, count, max, or mindirectly on streams.   2.12. Primitive Type Streams So far, we have collected integers in a Stream, even though it is clearly inefficient to wrap each integer into a wrapper object. The same is true for the other primitive types double, float, long, short, char, byte, and boolean. The stream library has specialized types IntStream, LongStream, and DoubleStream that store primitive values directly, without using wrappers. If you want to store short, char, byte, and boolean, use an IntStream, and for float, use a DoubleStream. The library designers didn’t think it was worth adding another five stream types.   To create an IntStream, you can call the IntStream.of and Arrays.streammethods:   Click here to view code image   IntStream stream = IntStream.of(1, 1, 2, 3, 5); stream = Arrays.stream(values, from, to); // values is an int[] array As with object streams, you can also use the static generate and iterate methods. In addition, IntStreamand LongStreamhave static methods range and rangeClosed that generate integer ranges with step size one:   Click here to view code image   IntStream zeroToNinetyNine = IntStream.range(0, 100); // Upper bound is excluded IntStream zeroToHundred = IntStream.rangeClosed(0, 100); // Upper bound is included The CharSequenceinterface has methods codePoints and chars that yield an IntStream of the Unicode codes of the characters or of the code units in the UTF-16 encoding. (If you don’t know what code units are, you probably shouldn’t use the chars method. Read up on the sordid details in Core Java, 9th Edition, Volume 1, Section 3.3.3.)   Click here to view code image   String sentence = “\\uD835\\uDD46 is the set of octonions.”;    // \\uD835\\uDD46 is the UTF-16 encoding of the letter   , unicode U+1D546   IntStream codes = sentence.codePoints();    // The stream with hex values 1D546 20 69 73 20 … When you have a stream of objects, you can transform it to a primitive type stream with the mapToInt, mapToLong, or mapToDoublemethods. For example, if you have a stream of strings and want to process their lengths as integers, you might as well do it in an IntStream:   Click here to view code image   Stream words = ...; IntStream lengths = words.mapToInt(String::length); To convert a primitive type stream to an object stream, use the boxed method:   Click here to view code image   Stream integers = IntStream.range(0, 100).boxed();   Generally, the methods on primitive type streams are analogous to those on object streams. Here are the most notable differences:   • The toArraymethods return primitive type arrays.   • Methods that yield an optional result return an OptionalInt, OptionalLong, or OptionalDouble. These classes are analogous to the Optional class, but they have methods getAsInt, getAsLong, and getAsDoubleinstead of the getmethod.   • There are methods sum, average, max, and min that return the sum, average, maximum, and minimum. These methods are not defined for object streams.   • The summaryStatisticsmethod yields an object of type IntSummaryStatistics, LongSummaryStatistics, or DoubleSummaryStatisticsthat can simultaneously report the sum, average, maximum, and minimum of the stream.   NOTE   The Randomclass has methods ints, longs, and doubles that return primitive type streams of random numbers.   2.13. Parallel Streams Streams make it easy to parallelize bulk operations. The process is mostly automatic, but you need to follow a few rules. First of all, you must have a parallel stream. By default, stream operations create sequential streams, except for Collection.parallelStream(). The parallelmethod converts any sequential stream into a parallel one. For example:   Click here to view code image   Stream parallelWords = Stream.of(wordArray).parallel();   As long as the stream is in parallel mode when the terminal method executes, all lazy intermediate stream operations will be parallelized.   When stream operations run in parallel, the intent is that the same result is returned as if they had run serially. It is important that the operations are stateless and can be executed in an arbitrary order.   Here is an example of something you cannot do. Suppose you want to count all short words in a stream of strings:   Click here to view code image   int[] shortWords = new int[12]; words.parallel().forEach(    s -&gt; { if (s.length() &lt; 12) shortWords[s.length()]++; });       // Error—race condition! System.out.println(Arrays.toString(shortWords)); This is very, very bad code. The function passed to forEachruns concurrently in multiple threads, updating a shared array. That’s a classic race condition. If you run this program multiple times, you are quite likely to get a different sequence of counts in each run, each of them wrong.   It is your responsibility to ensure that any functions that you pass to parallel stream operations are threadsafe. In our example, you could use an array of AtomicIntegerobjects for the counters (see Exercise 12). Or you could simply use the facilities of the streams library and group strings by length (see Exercise 13).   By default, streams that arise from ordered collections (arrays and lists), from ranges, generators, and iterators, or from calling Stream.sorted, are ordered. Results are accumulated in the order of the original elements, and are entirely predictable. If you run the same operations twice, you will get exactly the same results.   Ordering does not preclude parallelization. For example, when computing stream.map(fun), the stream can be partitioned into nsegments, each of which is concurrently processed. Then the results are reassembled in order.   Some operations can be more effectively parallelized when the ordering requirement is dropped. By calling the Stream.unorderedmethod, you indicate that you are not interested in ordering. One operation that can benefit from this is Stream.distinct. On an ordered stream, distinct retains the first of all equal elements. That impedes parallelization—the thread processing a segment can’t know which elements to discard until the preceding segment has been processed. If it is acceptable to retain any of the unique elements, all segments can be processed concurrently (using a shared set to track duplicates).   You can also speed up the limit method by dropping ordering. If you just want any nelements from a stream and you don’t care which ones you get, call   Click here to view code image   Stream sample = stream.parallel().unordered().limit(n);   As discussed in Section 2.10, “Collecting into Maps,” on page 34, merging maps is expensive. For that reason, the Collectors.groupingByConcurrentmethod uses a shared concurrent map. Clearly, to benefit from parallelism, the order of the map values will not be the same as the stream order. Even on an ordered stream, that collector has a “characteristic” of being unordered, so that it can be used efficiently without having to make the stream unordered. You still need to make the stream parallel, though:   Click here to view code image   Map&lt;String, List&gt; result = cities.parallel().collect(    Collectors.groupingByConcurrent(City::getState));    // Values aren’t collected in stream order   CAUTION   It is very important that you don’t modify the collection that is backing a stream while carrying out a stream operation (even if the modification is threadsafe). Remember that streams don’t collect their own data—the data is always in a separate collection. If you were to modify that collection, the outcome of the stream operations would be undefined. The JDK documentation refers to this requirement as noninterference. It applies both to sequential and parallel streams.   To be exact, since intermediate stream operations are lazy, it is possible to mutate the collection up to the point when the terminal operation executes. For example, the following is correct:   Click here to view code image   List wordList = ...; Stream words = wordList.stream(); wordList.add(\"END\"); // Ok long n = words.distinct().count(); But this code is not:   Click here to view code image   Stream words = wordList.stream(); words.forEach(s -&gt; if (s.length() &lt; 12) wordList.remove(s));    // Error—interference   Exercises          Write a parallel version of the forloop in Section 2.1, “From Iteration to Stream Operations,” on page 22. Obtain the number of processors. Make that many separate threads, each working on a segment of the list, and total up the results as they come in. (You don’t want the threads to update a single counter. Why?)            Verify that asking for the first five long words does not call the filter method once the fifth long word has been found. Simply log each method call.            Measure the difference when counting long words with a parallelStreaminstead of a stream. Call System.nanoTimebefore and after the call, and print the difference. Switch to a larger document (such as War and Peace) if you have a fast computer.            Suppose you have an array int[] values = { 1, 4, 9, 16 }. What is Stream.of(values)? How do you get a stream of intinstead?            Using Stream.iterate, make an infinite stream of random numbers—not by calling Math.random but by directly implementing a linear congruential generator. In such a generator, you start with x0 = seedand then produce xn + 1 = (a xn + c) %m, for appropriate values of a, c, and m. You should implement a method with parameters a, c, m, and seed that yields a Stream. Try out a = 25214903917, c = 11, and m = 248.            The characterStreammethod in Section 2.3, “The filter, map, and flatMapMethods,” on page 25, was a bit clumsy, first filling an array list and then turning it into a stream. Write a stream-based one-liner instead. One approach is to make a stream of integers from 0 to s.length() - 1and map that with the s::charAtmethod reference.            Your manager asks you to write a method public static  boolean isFinite(Stream stream). Why isn’t that such a good idea? Go ahead and write it anyway.            Write a method public static  Stream zip(Stream first, Stream second) that alternates elements from the streams first and second, stopping when one of them runs out of elements.            Join all elements in a Stream&lt;ArrayList&gt;to one ArrayList. Show how to do this with the three forms of reduce.            Write a call to reduce that can be used to compute the average of a Stream. Why can’t you simply compute the sum and divide by count()?            It should be possible to concurrently collect stream results in a single ArrayList, instead of merging multiple array lists, provided it has been constructed with the stream’s size, since concurrent setoperations at disjoint positions are threadsafe. How can you achieve that?            Count all short words in a parallel Stream, as described in Section 2.13, “Parallel Streams,” on page 40, by updating an array of AtomicInteger. Use the atomic getAndIncrementmethod to safely increment each counter.            Repeat the preceding exercise, but filter out the short strings and use the collectmethod with Collectors.groupingByand Collectors.counting.       A function type is alwayscontravariant in its arguments and covariant in its return value. For example, if you have a Function&lt;Person, Employee&gt;, you can safely pass it on to someone who needs a Function&lt;Employee, Person&gt;. They will only call it with employees, whereas your function can handle any person. They will expect the function to return a person, and you give them something even better.   For example, look at the javadoc for Stream:   Click here to view code image   void forEach(Consumer&lt;? super T&gt; action) Stream filter(Predicate&lt;? super T&gt; predicate)   Stream map(Function&lt;? super T, ? extends R&gt; mapper) The general rule is that you use superfor argument types, extends for return types. That way, you can pass a Consumerto forEach on a Stream. If it is willing to consume any object, surely it can consume strings.  But the wildcards are not always there. Look at   For example, consider the doInOrderAsyncmethod of the preceding section. Instead of  Click here to view code image  public static  void doInOrderAsync(Supplier first,    Consumer second, Consumer handler) it should be  Click here to view code image  public static  void doInOrderAsync(Supplier&lt;? extends T&gt; first,    Consumer&lt;? super T&gt; second, Consumer&lt;? superThrowable&gt; handler)   In our example, we can call  Click here to view code image  largest.updateAndGet(x -&gt; Math.max(x, observed));  or  Click here to view code image  largest.accumulateAndGet(observed, Math::max);  The accumulateAndGetmethod takes a binary operator that is used to combine the atomic value and the supplied argument.       ## **default** in Java  ### There are three rules about **default** Regarding how to handle the situation of same default method in multiple inheritance. - **class win**, any class wins over any interfaces.So if there’s a method with a body, or an abstract declaration, in the superclass chain, we can ignore the interfaces completely. - **subtype win supertype**, “which two interfaces are competing to provide a default method and one interface extends the other, the subclass wins.” - **No rule 3**. if the previous two rules don't give us the answer, the subclass must either implement the method or declare it **abstract**.  “Interfaces give you multiple inheritance but no fields, while abstract classes let you “inherit fields but you don’t get multiple inheritance.”  ## Static method in Interface  We’ve seen a lot of calling of **Stream.of** but haven’t gotten into its details yet. You may recall that Stream is an interface, but this is a static method on an interface.  ```java stream.collect(toCollection(TreeSet::new)); ```   # **Optional** is a better of **null** ```java //Optional can be created via factory method 'of',  Optional a = Optional.of(\"a\"); // Optional is just a container, you can get the underlying value by 'get' method assertEquals(\"a\", a.get());  // at the meanwhile, Optional can represent 'absent' // factory method empty or ofNullable from a nullable object can be used @Test     public void testOptional(){         Optional optA=Optional.of(\"a\");         Assert.assertEquals(\"a\", optA.get());     }          @Test     public void testEmpty(){         Optional emp=Optional.ofNullable(null);         Assert.assertEquals(Optional.empty(), emp);         Assert.assertFalse(emp.isPresent());         Assert.assertEquals(\"b\", emp.orElse(\"b\"));         Assert.assertEquals(\"c\", emp.orElseGet(()-&gt;\"c\"));     } ```  # method reference - ** Classname::methodname** , such as Artist::getName is equivalant to artist-&gt;artist.getName() - For constructors can be used Artist::new - You can alos to create new array, String[]::new  # Stream “The purpose of streams isn’t just to convert from one collection to another; it’s to be able to provide a common set of operations over data.”  ## partitioningBy To split a stream into two groups, one for 'trueGroup' and another group  ## Lanmbda - It is best to think of a lambda expression as a function, not an object, and to accept that it can be converted to a functional interface. - This conversion to interfaces is what makes lambda expressions so compelling. The syntax is short and simple.  ```java BiFunction&lt;String, String, Integer&gt; comp    = (first, second) -&gt; Integer.compare(first.length(), second.length()); ```  The expression System.out::printlnis a method reference that is equivalent to the lambda expression x -&gt; System.out.println(x).  ### There are three principal cases:  - object::instanceMethod - Class::staticMethod - Class::instanceMethod  In the third case, the first parameter becomes the target of the method. For example, String::compareToIgnoreCaseis the same as (x, y) -&gt; x.compareToIgnoreCase(y).  Just like lambda expressions, method references don’t live in isolation. They are always turned into instances of functional interfaces.  ### Constructor References Constructor references are just like method references, except that the name of the method is new. For example, Button::new is a reference to a Button constructor. Which constructor? It depends on the context.  ```java List labels = ...; Stream stream = labels.stream().map(Button::new); List buttons = stream.collect(Collectors.toList()); ```  For example, suppose we want to have an array of buttons. The Stream interface has a toArraymethod that returns an Object array:  ```java  Object[] buttons = stream.toArray(); ```  we need to refine our understanding of a lambda expression. A lambda expression has three ingredients:  - 1. A block of code - 2. Parameters - 3. Values for the free variables, that is, the variables that are not parameters and not defined inside the code  The technical term for a block of code together with the values of the `free variables` is a `closure`. If someone gloats that their language has closures, rest assured that Java has them as well. In Java, lambda expressions are closures. In fact, inner classes have been closures all along. Java 8 gives us closures with an attractive syntax.  Inner classes can also capture values from an enclosing scope. Before Java 8, inner classes were only allowed to access `final local variables`. This rule has now been relaxed to match that for lambda expressions. An inner class can access any effectively final local variable—that is, any variable whose value does not change.   When you use the this keyword in a lambda expression, you refer to the this parameter of the method that creates the lambda. For example, consider  ```java  public class Application {    public void doWork() {       Runnable runner = () -&gt; { ...; System.out.println(this.toString()); ... };       ...    } } ```  The expression this.toString()calls the toString method of the Application object, not the Runnable instance. There is nothing special about the use of this in a lambda expression. The scope of the lambda expression is nested inside the doWorkmethod, and thishas the same meaning anywhere in that method.  The Java designers decided to solve this problem once and for all by allowing interface methods with concrete implementations (called default methods). Those methods can be safely added to existing interfaces.   ```java interface Person {    long getId();    default String getName() { return \"John Q. Public\"; } } ```  The interface has two methods: getId, which is an abstract method, and the default method getName. A concrete class that implements the Person interface must, of course, provide an implementation of getId, but it can choose to keep the implementation of getName or to override it.  Default methods put an end to the classic pattern of providing an interface and an abstract class that implements most or all of its methods, such as Collection/AbstractCollectionor WindowListener/WindowAdapter. Now you can just implement the methods in the interface.  To compare Person objects by name, use Comparator.comparing(Person::getName).  In this chapter, we have compared strings by length with the lambda expression (first, second) -&gt; Integer.compare(first.length(), second.length()). But But with the static compare method, we can do much better and simply use Comparator.comparing(String::length).   In Java 8, static methods have been added to quite a few interfaces. For example, the Comparator interface has a very useful static comparing method that accepts a “key extraction” function and yields a comparator that compares the extracted keys.   ### Exercises 1. Is the comparator code in the Arrays.sort method called in the same thread as the call to sortor a different thread?  2. Using the listFiles(FileFilter)and isDirectorymethods of the java.io.Fileclass, write a method that returns all subdirectories of a given directory. Use a lambda expression instead of a FileFilterobject. Repeat with a method reference.  3. Using the list(FilenameFilter)method of the java.io.Fileclass, write a method that returns all files in a given directory with a given extension. Use a lambda expression, not a FilenameFilter. Which variables from the enclosing scope does it capture?  4. Given an array of File objects, sort it so that the directories come before the files, and within each group, elements are sorted by path name. Use a lambda expression, not a Comparator.  5. Take a file from one of your projects that contains a number of ActionListener, Runnable, or the like. Replace them with lambda expressions. How many lines did it save? Was the code easier to read? Were you able to use method references?  6. Didn’t you always hate it that you had to deal with checked exceptions in a Runnable? Write a method uncheck that catches all checked exceptions and turns them into unchecked exceptions. For example,  ```java  new Thread(uncheck(    () -&gt; { System.out.println(\"Zzz\"); Thread.sleep(1000); })).start();       // Look, nocatch (InterruptedException)! ```  Hint: Define an interface RunnableExwhose runmethod may throw any exceptions. Then implement public static Runnable uncheck(RunnableEx runner). Use a lambda expression inside the uncheckmethod.  Why can’t you just use Callableinstead of RunnableEx?  7. Write a static method andThenthat takes as parameters two Runnableinstances and returns a Runnable that runs the first, then the second. In the main method, pass two lambda expressions into a call to andThen, and run the returned instance.  8. What happens when a lambda expression captures values in an enhanced forloop such as this one?  ```java  String[] names = { \"Peter\", \"Paul\", \"Mary\" }; List runners = new ArrayList&lt;&gt;(); for (String name : names)    runners.add(() -&gt; System.out.println(name)); ```  Is it legal? Does each lambda expression capture a different value, or do they all get the last value? What happens if you use a traditional loop for (int i = 0; i &lt; names.length; i++)?  9. Form a subinterface Collection2from Collectionand add a default method void forEachIf(Consumer action, Predicate filter) that applies action to each element for which filterreturns true. How could you use it?  10. Go through the methods of the Collectionsclass. If you were king for a day, into which interface would you place each method? Would it be a default method or a static method?  11. Suppose you have a class that implements two interfaces I and J, each of which has a method void f(). Exactly what happens if f is an abstract, default, or static method of I and an abstract, default, or static method of J? Repeat where a class extends a superclass S and implements an interface I, each of which has a method void f().  12. In the past, you were told that it’s bad form to add methods to an interface because it would break existing code. Now you are told that it’s okay to add new methods, provided you also supply a default implementation. How safe is that? Describe a scenario where the new streammethod of the Collectioninterface causes legacy code to fail compilation. What about binary compatibility? Will legacy code from a JAR file still run?  ### Stream vs Collections  A stream seems superficially similar to a collection, allowing you to transform and retrieve data. But there are significant differences: 1. A stream `does not store` its elements. They may be stored in an underlying collection or generated on demand. 2. `Stream operations don’t mutate their source`. Instead, they return new streams that hold the result. 3. `Stream operations are lazy` when possible. This means they are not executed until their result is needed. For example, if you only ask for the first five long words instead of counting them all, then the filter method will stop filtering after the fifth match. As a consequence, you can even have infinite streams!   - Streams follow `the “what, not how” principle`. In our stream example, we describe what needs to be done: get the long words and count them. We don’t specify in which order, or in which thread, this should happen.   - When you work with streams, you set up a pipeline of operations in three stages. 1. You create a stream. 2. You specify intermediate operations for transforming the initial stream into others, in one or more steps. 3. You apply a terminal operation to produce a result. This operation forces the execution of the lazy operations that precede it. Afterwards, the stream can no longer be used. ```java long count = words.parallelStream().filter(w -&gt; w.length() &gt; 12).count(); ```  `Stream operations are not executed on the elements in the order in which they are invoked on the streams`. In our example, nothing happens until count is called. When the count method asks for the first element, then the filter method starts requesting elements, until it finds one that has length &gt; 12.   To produce infinite sequences such as 0 1 2 3 ..., use the iterate method instead. It takes a “seed” value and a function (technically, a UnaryOperator), and repeatedly applies the function to the previous result. For example,  ```java Stream integers    = Stream.iterate(BigInteger.ZERO, n -&gt; n.add(BigInteger.ONE)); ```  The first element in the sequence is the seed  You can use the following statement to split a string into words:  Stream words    = Pattern.compile(\"[\\\\P{L}]+\").splitAsStream(contents); The static Files.linesmethod returns a Stream of all lines in a file.   The Streaminterface has AutoCloseableas a superinterface. When the close method is called on the stream, the underlying file is also closed.   To make sure that this happens, it is best to use the Java 7 try-with-resources statement:  ```java try (Stream lines = Files.lines(path)) {    Do something with lines } ```  The stream, and the underlying file with it, will be closed when the try block exits normally or through an exception.  The filter, map, and flatMapMethods A stream transformation reads data from a stream and puts the transformed data into another stream. You have already seen the filtertransformation that yields a new stream with all elements that match a certain condition.   2.3. The filter, map, and flatMapMethods A stream transformation reads data from a stream and puts the transformed data into another stream. You have already seen the filtertransformation that yields a new stream with all elements that match a certain condition. Here, we transform a stream of strings into another stream containing only long words:  ```java List wordList = ...; Stream words = wordList.stream(); Stream longWords = words.filter(w -&gt; w.length() &gt; 12); ```  The argument of filter is a Predicate—that is, a function from T to boolean.  Often, you want to transform the values in a stream in some way. Use the map method and pass the function that carries out the transformation. For example, you can transform all words to lowercase like this:  ```java Stream lowercaseWords = words.map(String::toLowerCase); ```  Here, we used map with a method reference. Often, you will use a lambda expression instead:  ```java Stream firstChars = words.map(s -&gt; s.charAt(0)); ```  The resulting stream contains the first character of each word.  When you use map, a function is applied to each element, and the return values are collected in a new stream. Now suppose that you have a function that returns not just one value but a stream of values, such as this one:  ```java public static Stream characterStream(String s) {    List result = new ArrayList&lt;&gt;();    for (char c : s.toCharArray()) result.add(c);    return result.stream(); } ```  For example, characterStream(\"boat\")is the stream ['b', 'o', 'a', 't']. Suppose you map this method on a stream of strings:  ```java Stream&lt;Stream&gt; result = words.map(w -&gt; characterStream(w)); ```  You will get a stream of streams, like [... ['y', 'o', 'u', 'r'], ['b', 'o', 'a', 't'], ...] To flatten it out to a stream of characters [... 'y', 'o', 'u', 'r', 'b', 'o', 'a', 't', ...], use the flatMapmethod instead of map:  ```java Stream letters = words.flatMap(w -&gt; characterStream(w))     // CallscharacterStream on each word and flattens the results  NOTE ```  You may find a flatMap method in classes other than streams. It is a general concept in computer science. Suppose you have a generic type G (such as Stream) and functions ffrom some type T to Gand g from U to G. Then you can compose them, that is, first apply f and then g, by using flatMap. This is a key idea in the theory of monads. But don’t worry—you can use flatMapwithout knowing anything about monads.   This method is particularly useful for cutting infinite streams down to size. For example,  ```java Stream randoms = Stream.generate(Math::random).limit(100); ```  yields a stream with 100 random numbers.  The peek method yields another stream with the same elements as the original, but a function is invoked every time an element is retrieved. That is handy for debugging:  ```java Object[] powers = Stream.iterate(1.0, p -&gt; p * 2)    .peek(e -&gt; System.out.println(\"Fetching \" + e))    .limit(20).toArray(); ```  When an element is actually accessed, a message is printed. This way you can verify that the infinite stream returned by iterate is processed lazily.   The stream transformations of the preceding sections were `stateless`. When an element is retrieved from a filtered or mapped stream, the answer does not depend on the previous elements. There are also a few stateful transformations. For example, the distinct method returns a stream that yields elements from the original stream, in the same order, except that duplicates are suppressed.  The stream must obviously remember the elements that it has already seen.  ```java Stream uniqueWords    = Stream.of(\"merrily\", \"merrily\", \"merrily\", \"gently\").distinct();    // Only one\"merrily\" is retained ```  The sorted method must see the entire stream and sort it before it can give out any elements—after all, the smallest one might be the last one. Clearly, you can’t sort an infinite stream.  - There are several sorted methods. One works for streams of Comparableelements, and another accepts a Comparator. Here, we sort strings so that the longest ones come first:  ```java Stream longestFirst =    words.sorted(Comparator.comparing(String::length).reversed()); ```  Of course, you can sort a collection without using streams. The sorted method is useful when the sorting process is a part of a stream pipeline.   `The Collections.sort method sorts a collection in place, whereas Stream.sorted returns a new sorted stream.`  The methods that we cover in this section are called reductions. `They reduce the stream to a value that can be used in your program`. Reductions are terminal operations. After a terminal operation has been applied, the stream ceases to be usable.  In Java 8, `the Optional type is the preferred way of indicating a missing return value`. We discuss the Optional type in detail in the next section. Here is how you can get the maximum of a stream:  ```java Optional largest = words.max(String::compareToIgnoreCase); if (largest.isPresent())    System.out.println(\"largest: \" + largest.get()); ```  reduce. Each segment needs to start out with its own empty hash set, and reduce only lets you supply one identity value.   ## Instead, use collect. It takes three arguments:  1. A `supplier to make new instances of the target object`, for example, a constructor for a hash set 2. An `accumulatorthat adds an element to the target`, for example, an addmethod 3. A `combiner that merges two objects into one`, such as addAll   The target object need not be a collection. It could be a StringBuilder or an object that tracks a count and a sum.  Here is how the collect method works for a hash set:  ```java HashSet result = stream.collect(HashSet::new, HashSet::add, HashSet::addAll); ```  In practice, you don’t have to do that because there is a convenient Collector interface for these three functions, and a Collectors class with factory methods for common collectors. To collect a stream into a list or set, you can simply call  ```java List result = stream.collect(Collectors.toList()); // or  Set result = stream.collect(Collectors.toSet()); ```  If you want to control which kind of set you get, use the following call instead:  ```java TreeSet result = stream.collect(Collectors.toCollection(TreeSet::new)); ```  Suppose you want to collect all strings in a stream by concatenating them. You can call  ```java String result = stream.collect(Collectors.joining()); ```  If you want a delimiter between elements, pass it to the joiningmethod:  ```java String result = stream.collect(Collectors.joining(\", \")); ```  If your stream contains objects other than strings, you need to first convert them to strings, like this:  ```java String result = stream.map(Object::toString).collect(Collectors.joining(\", \")); ```  If you want to reduce the stream results to a sum, average, maximum, or minimum, then use one of the methods summarizing(Int|Long|Double). These methods take a function that maps the stream objects to a number and yield a result of type (Int|Long|Double)SummaryStatistics, with methods for obtaining the sum, average, maximum, and minumum.  ```java IntSummaryStatistics summary = words.collect(    Collectors.summarizingInt(String::length)); double averageWordLength = summary.getAverage(); double maxWordLength = summary.getMax(); ```   So far, you have seen how to reduce or collect stream values. But perhaps you just want to print them or put them in a database. Then you can use the forEach method:  ```java stream.forEach(System.out::println); ```  The function that you pass is applied to each element. On a parallel stream, it’s your responsibility to ensure that the function can be executed concurrently. We discuss this in Section 2.13, “Parallel Streams,”.  On a parallel stream, the elements can be traversed in arbitrary order. If you want to execute them in stream order, call forEachOrderedinstead. Of course, you might then give up most or all of the benefits of parallelism.  `The forEach and forEachOrdered methods are terminal operations`. You cannot use the stream again after calling them. If you want to continue using the stream, use peek instead—see   In the common case that the values should be the actual elements, use Function.identity() for the second function.  ```java Map&lt;Integer, Person&gt; idToPerson = people.collect(    Collectors.toMap(Person::getId, Function.identity())); ```  If there is more than one element with the same key, the collector will throw an IllegalStateException. You can override that behavior by supplying a third function argument that determines the value for the key, given the existing and the new value. Your function could return the existing value, the new value, or a combination of them.  Here, we construct a map that contains, for each language in the available locales, as key its name in your default locale (such as \"German\"), and as value its localized name (such as \"Deutsch\").  ```java Stream locales = Stream.of(Locale.getAvailableLocales()); Map&lt;String, String&gt; languageNames = locales.collect(    Collectors.toMap(       l -&gt; l.getDisplayLanguage(),       l -&gt; l.getDisplayLanguage(l),       (existingValue, newValue) -&gt; existingValue)); ```  We don’t care that the same language might occur twice—for example, German in Germany and in Switzerland, and we just keep the first entry.  However, suppose we want to know all languages in a given country. Then we need a Map&lt;String, Set&gt;. For example, the value for \"Switzerland\"is the set [French, German, Italian]. At first, we store a singleton set for each language. Whenever a new language is found for a given country, we form the union of the existing and the new set.  ```java Map&lt;String, Set&gt; countryLanguageSets = locales.collect(    Collectors.toMap(       l -&gt; l.getDisplayCountry(),       l -&gt; Collections.singleton(l.getDisplayLanguage()),       (a, b) -&gt; { // Union of a and b          Set r = new HashSet&lt;&gt;(a);          r.addAll(b);          return r; })); ```  You will see a simpler way of obtaining this map in the next section.  If you want a TreeMap, then you supply the constructor as the fourth argument. You must provide a merge function. Here is one of the examples from the beginning of the section, now yielding a TreeMap:  ```java Map&lt;Integer, Person&gt; idToPerson = people.collect(    Collectors.toMap(       Person::getId,       Function.identity(),       (existingValue, newValue) -&gt; { throw new IllegalStateException(); },       TreeMap::new)); ```  For example, if you want sets instead of lists, you can use the Collectors.toSetcollector that you saw in the preceding section:  ```java Map&lt;String, Set&gt; countryToLocaleSet = locales.collect(    groupingBy(Locale::getCountry, toSet())); ```  Several other collectors are provided for downstream processing of grouped elements:  • counting produces a count of the collected elements. For example,  ```java Map&lt;String, Long&gt; countryToLocaleCounts = locales.collect(    groupingBy(Locale::getCountry, counting())); ``` counts how many locales there are for each country.  • summing(Int|Long|Double) takes a function argument, applies the function to the downstream elements, and produces their sum. For example,  ```java Map&lt;String, Integer&gt; stateToCityPopulation = cities.collect(    groupingBy(City::getState, summingInt(City::getPopulation))); ```  computes the sum of populations per state in a stream of cities.  • maxBy and minBy take a comparator and produce maximum and minimum of the downstream elements. For example,  ```java Map&lt;String, City&gt; stateToLargestCity = cities.collect(    groupingBy(City::getState,       maxBy(Comparator.comparing(City::getPopulation)))); ``` produces the largest city per state.  • mapping applies a function to downstream results, and it requires yet another collector for processing its results. For example,  ```java Map&lt;String, Optional&gt; stateToLongestCityName = cities.collect(    groupingBy(City::getState,       mapping(City::getName,          maxBy(Comparator.comparing(String::length))))); ```  Here, we group cities by state. Within each state, we produce the names of the cities and reduce by maximum length.  The mapping method also yields a nicer solution to a problem from the preceding section, to gather a set of all languages in a country.  ```java Map&lt;String, Set&gt; countryToLanguages = locales.collect(    groupingBy(l -&gt; l.getDisplayCountry(),       mapping(l -&gt; l.getDisplayLanguage(),          toSet()))); ```  In the preceding section, I used toMap instead of groupingBy. In this form, you don’t need to worry about combining the individual sets.  • If the grouping or mapping function has return type int, long, or double, you can collect elements into a summary statistics object, as discussed in Section 2.9, “Collecting Results,” on page 33. For example,  ```java Map&lt;String, IntSummaryStatistics&gt; stateToCityPopulationSummary = cities.collect(    groupingBy(City::getState,       summarizingInt(City::getPopulation))); ```  Then you can get the sum, count, average, minimum, and maximum of the function values from the summary statistics objects of each group.  • Finally, the reducingmethods apply a general reduction to downstream elements. There are three forms: reducing(binaryOperator), reducing(identity, binaryOperator), and reducing(identity, mapper, binaryOperator). In the first form, the identity is null. (Note that this is different from the forms of Stream::reduce, where the method without an identity parameter yields an Optional result.) In the third form, the mapperfunction is applied and its values are reduced.  Here is an example that gets a comma-separated string of all city names in each state. We map each city to its name and then concatenate them.  Click here to view code image  Map&lt;String, String&gt; stateToCityNames = cities.collect(    groupingBy(City::getState,       reducing(\"\", City::getName,          (s, t) -&gt; s.length() == 0 ? t : s + \", \" + t))); As with Stream.reduce, Collectors.reducingis rarely necessary. In this case, you can achieve the same result more naturally as  Click here to view code image  Map&lt;String, String&gt; stateToCityNames = cities.collect(    groupingBy(City::getState,       mapping(City::getName,          joining(\", \")))); Frankly, the downstream collectors can yield very convoluted expressions. You should only use them in connection with groupingBy or partitioningBy to process the “downstream” map values. Otherwise, simply apply methods such as map, reduce, count, max, or mindirectly on streams.  2.12. Primitive Type Streams So far, we have collected integers in a Stream, even though it is clearly inefficient to wrap each integer into a wrapper object. The same is true for the other primitive types double, float, long, short, char, byte, and boolean. The stream library has specialized types IntStream, LongStream, and DoubleStream that store primitive values directly, without using wrappers. If you want to store short, char, byte, and boolean, use an IntStream, and for float, use a DoubleStream. The library designers didn’t think it was worth adding another five stream types.  To create an IntStream, you can call the IntStream.of and Arrays.streammethods:  Click here to view code image  IntStream stream = IntStream.of(1, 1, 2, 3, 5); stream = Arrays.stream(values, from, to); // values is an int[] array As with object streams, you can also use the static generate and iterate methods. In addition, IntStreamand LongStreamhave static methods range and rangeClosed that generate integer ranges with step size one:  Click here to view code image  IntStream zeroToNinetyNine = IntStream.range(0, 100); // Upper bound is excluded IntStream zeroToHundred = IntStream.rangeClosed(0, 100); // Upper bound is included The CharSequenceinterface has methods codePoints and chars that yield an IntStream of the Unicode codes of the characters or of the code units in the UTF-16 encoding. (If you don’t know what code units are, you probably shouldn’t use the chars method. Read up on the sordid details in Core Java, 9th Edition, Volume 1, Section 3.3.3.)  Click here to view code image  String sentence = \"\\uD835\\uDD46 is the set of octonions.\";    // \\uD835\\uDD46 is the UTF-16 encoding of the letter  , unicode U+1D546  IntStream codes = sentence.codePoints();    // The stream with hex values 1D546 20 69 73 20 ... When you have a stream of objects, you can transform it to a primitive type stream with the mapToInt, mapToLong, or mapToDoublemethods. For example, if you have a stream of strings and want to process their lengths as integers, you might as well do it in an IntStream:  Click here to view code image  Stream words = ...; IntStream lengths = words.mapToInt(String::length); To convert a primitive type stream to an object stream, use the boxed method:  Click here to view code image  Stream integers = IntStream.range(0, 100).boxed();  Generally, the methods on primitive type streams are analogous to those on object streams. Here are the most notable differences:  • The toArraymethods return primitive type arrays.  • Methods that yield an optional result return an OptionalInt, OptionalLong, or OptionalDouble. These classes are analogous to the Optional class, but they have methods getAsInt, getAsLong, and getAsDoubleinstead of the getmethod.  • There are methods sum, average, max, and min that return the sum, average, maximum, and minimum. These methods are not defined for object streams.  • The summaryStatisticsmethod yields an object of type IntSummaryStatistics, LongSummaryStatistics, or DoubleSummaryStatisticsthat can simultaneously report the sum, average, maximum, and minimum of the stream.   NOTE  The Randomclass has methods ints, longs, and doubles that return primitive type streams of random numbers.  2.13. Parallel Streams Streams make it easy to parallelize bulk operations. The process is mostly automatic, but you need to follow a few rules. First of all, you must have a parallel stream. By default, stream operations create sequential streams, except for Collection.parallelStream(). The parallelmethod converts any sequential stream into a parallel one. For example:  Click here to view code image  Stream parallelWords = Stream.of(wordArray).parallel();  As long as the stream is in parallel mode when the terminal method executes, all lazy intermediate stream operations will be parallelized.  When stream operations run in parallel, the intent is that the same result is returned as if they had run serially. It is important that the operations are stateless and can be executed in an arbitrary order.  Here is an example of something you cannot do. Suppose you want to count all short words in a stream of strings:  Click here to view code image  int[] shortWords = new int[12]; words.parallel().forEach(    s -&gt; { if (s.length() &lt; 12) shortWords[s.length()]++; });       // Error—race condition! System.out.println(Arrays.toString(shortWords)); This is very, very bad code. The function passed to forEachruns concurrently in multiple threads, updating a shared array. That’s a classic race condition. If you run this program multiple times, you are quite likely to get a different sequence of counts in each run, each of them wrong.  It is your responsibility to ensure that any functions that you pass to parallel stream operations are threadsafe. In our example, you could use an array of AtomicIntegerobjects for the counters (see Exercise 12). Or you could simply use the facilities of the streams library and group strings by length (see Exercise 13).  By default, streams that arise from ordered collections (arrays and lists), from ranges, generators, and iterators, or from calling Stream.sorted, are ordered. Results are accumulated in the order of the original elements, and are entirely predictable. If you run the same operations twice, you will get exactly the same results.  Ordering does not preclude parallelization. For example, when computing stream.map(fun), the stream can be partitioned into nsegments, each of which is concurrently processed. Then the results are reassembled in order.  Some operations can be more effectively parallelized when the ordering requirement is dropped. By calling the Stream.unorderedmethod, you indicate that you are not interested in ordering. One operation that can benefit from this is Stream.distinct. On an ordered stream, distinct retains the first of all equal elements. That impedes parallelization—the thread processing a segment can’t know which elements to discard until the preceding segment has been processed. If it is acceptable to retain any of the unique elements, all segments can be processed concurrently (using a shared set to track duplicates).  You can also speed up the limit method by dropping ordering. If you just want any nelements from a stream and you don’t care which ones you get, call  Click here to view code image  Stream sample = stream.parallel().unordered().limit(n);  As discussed in Section 2.10, “Collecting into Maps,” on page 34, merging maps is expensive. For that reason, the Collectors.groupingByConcurrentmethod uses a shared concurrent map. Clearly, to benefit from parallelism, the order of the map values will not be the same as the stream order. Even on an ordered stream, that collector has a “characteristic” of being unordered, so that it can be used efficiently without having to make the stream unordered. You still need to make the stream parallel, though:  Click here to view code image  Map&lt;String, List&gt; result = cities.parallel().collect(    Collectors.groupingByConcurrent(City::getState));    // Values aren’t collected in stream order  CAUTION  It is very important that you don’t modify the collection that is backing a stream while carrying out a stream operation (even if the modification is threadsafe). Remember that streams don’t collect their own data—the data is always in a separate collection. If you were to modify that collection, the outcome of the stream operations would be undefined. The JDK documentation refers to this requirement as noninterference. It applies both to sequential and parallel streams.  To be exact, since intermediate stream operations are lazy, it is possible to mutate the collection up to the point when the terminal operation executes. For example, the following is correct:  Click here to view code image  List wordList = ...; Stream words = wordList.stream(); wordList.add(\"END\"); // Ok long n = words.distinct().count(); But this code is not:  Click here to view code image  Stream words = wordList.stream(); words.forEach(s -&gt; if (s.length() &lt; 12) wordList.remove(s));    // Error—interference   Exercises 1. Write a parallel version of the forloop in Section 2.1, “From Iteration to Stream Operations,” on page 22. Obtain the number of processors. Make that many separate threads, each working on a segment of the list, and total up the results as they come in. (You don’t want the threads to update a single counter. Why?)  2. Verify that asking for the first five long words does not call the filter method once the fifth long word has been found. Simply log each method call.  3. Measure the difference when counting long words with a parallelStreaminstead of a stream. Call System.nanoTimebefore and after the call, and print the difference. Switch to a larger document (such as War and Peace) if you have a fast computer.  4. Suppose you have an array int[] values = { 1, 4, 9, 16 }. What is Stream.of(values)? How do you get a stream of intinstead?  5. Using Stream.iterate, make an infinite stream of random numbers—not by calling Math.random but by directly implementing a linear congruential generator. In such a generator, you start with x0 = seedand then produce xn + 1 = (a xn + c) %m, for appropriate values of a, c, and m. You should implement a method with parameters a, c, m, and seed that yields a Stream. Try out a = 25214903917, c = 11, and m = 248.  6. The characterStreammethod in Section 2.3, “The filter, map, and flatMapMethods,” on page 25, was a bit clumsy, first filling an array list and then turning it into a stream. Write a stream-based one-liner instead. One approach is to make a stream of integers from 0 to s.length() - 1and map that with the s::charAtmethod reference.  7. Your manager asks you to write a method public static  boolean isFinite(Stream stream). Why isn’t that such a good idea? Go ahead and write it anyway.  8. Write a method public static  Stream zip(Stream first, Stream second) that alternates elements from the streams first and second, stopping when one of them runs out of elements.  9. Join all elements in a Stream&lt;ArrayList&gt;to one ArrayList. Show how to do this with the three forms of reduce.  10. Write a call to reduce that can be used to compute the average of a Stream. Why can’t you simply compute the sum and divide by count()?  11. It should be possible to concurrently collect stream results in a single ArrayList, instead of merging multiple array lists, provided it has been constructed with the stream’s size, since concurrent setoperations at disjoint positions are threadsafe. How can you achieve that?  12. Count all short words in a parallel Stream, as described in Section 2.13, “Parallel Streams,” on page 40, by updating an array of AtomicInteger. Use the atomic getAndIncrementmethod to safely increment each counter.  13. Repeat the preceding exercise, but filter out the short strings and use the collectmethod with Collectors.groupingByand Collectors.counting.  A function type is alwayscontravariant in its arguments and covariant in its return value. For example, if you have a Function&lt;Person, Employee&gt;, you can safely pass it on to someone who needs a Function&lt;Employee, Person&gt;. They will only call it with employees, whereas your function can handle any person. They will expect the function to return a person, and you give them something even better.    For example, look at the javadoc for Stream:  Click here to view code image  void forEach(Consumer&lt;? super T&gt; action) Stream filter(Predicate&lt;? super T&gt; predicate)  Stream map(Function&lt;? super T, ? extends R&gt; mapper) The general rule is that you use superfor argument types, extends for return types. That way, you can pass a Consumerto forEach on a Stream. If it is willing to consume any object, surely it can consume strings.  But the wildcards are not always there. Look at   For example, consider the doInOrderAsyncmethod of the preceding section. Instead of  Click here to view code image  public static  void doInOrderAsync(Supplier first,    Consumer second, Consumer handler) it should be  Click here to view code image  public static  void doInOrderAsync(Supplier&lt;? extends T&gt; first,    Consumer&lt;? super T&gt; second, Consumer&lt;? superThrowable&gt; handler)   In our example, we can call  Click here to view code image  largest.updateAndGet(x -&gt; Math.max(x, observed));  or  Click here to view code image  largest.accumulateAndGet(observed, Math::max);  The accumulateAndGetmethod takes a binary operator that is used to combine the atomic value and the supplied argument.    If you anticipate high contention, you should simply use a LongAdder instead of an AtomicLong. The method names are slightly different. Call increment to increment a counter or add to add a quantity, and sum to retrieve the total.  Click here to view code image  final LongAdder adder = new LongAdder(); for (...)    pool.submit(() -&gt; {       while (...) {          ...          if (...) adder.increment();       }    }); ... long total = adder.sum());  NOTE  The Arrays class now has a number of parallelized operations. The static Arrays.parallelSortmethod can sort an array of primitive values or objects. For example,  Click here to view code image  String contents = new String(Files.readAllBytes(     Paths.get(\"alice.txt\")), StandardCharsets.UTF_8); // Read file into string String[] words = contents.split(\"[\\\\P{L}]+\"); // Split along nonletters Arrays.parallelSort(words); When you sort objects, you can supply a Comparator. With all methods, you can supply the bounds of a range, such as  Click here to view code image  Arrays.parallelSort(values, values.length / 2, values.length); // Sort the upper half   There was no easy way of saying: “When the result becomes available, here is how to process it.” This is the crucial feature that the new CompletableFutureclass provides.   This composability is the key aspect of the CompletableFutureclass. Composing future actions solves a serious problem in programming asynchronous applications. The traditional approach for dealing with nonblocking calls is to use event handlers. The programmer registers a handler for the next action after completion. Of course, if the next action is also asynchronous, then the next action after that is in a different event handler. Even though the programmer thinks in terms of “first do step 1, then step 2, then step 3,” the program logic becomes dispersed in different places.   For example, here is a two-stage pipeline for reading and processing the web page:  Click here to view code image  CompletableFuture&lt;List&gt; links    = CompletableFuture.supplyAsync(() -&gt; blockingReadPage(url))       .thenApply(Parser::getLinks); You can have additional processing steps. Eventually, you’ll be done, and you will need to save the results somewhere. Here, we just print the result.  Click here to view code image  CompletableFuture links    = CompletableFuture.supplyAsync(() -&gt; blockingReadPage(url))       .thenApply(Parser::getLinks)       .thenAccept(System.out::println); The thenAcceptmethod takes a Consumer—that is, a function with return type void.  Ideally, you would never call get on a future. The last step in the pipeline simply deposits the result where it belongs.  Files.lineslazily reads a stream of lines.  • Files.listlazily lists the entries of a directory, and Files.walktraverses them recursively.  8.4.2. Comparators The Comparatorinterface has a number of useful new methods, taking advantage of the fact that interfaces can now have concrete methods.   The static comparingmethod takes a “key extractor” function that maps a type T to a comparable type (such as String). The function is applied to the objects to be compared, and the comparison is then made on the returned keys. For example, suppose you have an array of Personobjects. Here is how you can sort them by name:  Click here to view code image  Arrays.sort(people, Comparator.comparing(Person::getName));  You can chain comparators with the thenComparingmethod for breaking ties. For example,  Click here to view code image  Arrays.sort(people,    Comparator.comparing(Person::getLastName)    .thenComparing(Person::getFirstName)); If two people have the same last name, then the second comparator is used.  There are a few variations of these methods. You can specify a comparator to be used for the keys that the comparingand thenComparingmethods extract. For example, here we sort people by the length of their names:  Click here to view code image  Arrays.sort(people, Comparator.comparing(Person::getName,    (s, t) -&gt; Integer.compare(s.length(), t.length()))); Moreover, both the comparing and thenComparingmethods have variants that avoid boxing of int, long, or doublevalues. An easier way of producing the preceding operation would be  Click here to view code image  Arrays.sort(people, Comparator.comparingInt(p -&gt; p.getName().length()));  If your key function can return null, you will like the nullsFirst and nullsLast adapters. These static methods take an existing comparator and modify it so that it doesn’t throw an exception when encountering nullvalues but ranks them as smaller or larger than regular values. For example, suppose getMiddleNamereturns a null when a person has no middle name. Then you can use Comparator.comparing(Person::getMiddleName(), Comparator.nullsFirst(...)).  The nullsFirstmethod needs a comparator—in this case, one that compares two strings. The naturalOrdermethod makes a comparator for any class implementing Comparable. A Comparator.naturalOrder()is what we need. Here is the complete call for sorting by potentially null middle names. I use a static import of java.util.Comparator.*, to make the expression more legible. Note that the type for naturalOrder is inferred.  Click here to view code image  Arrays.sort(people, comparing(Person::getMiddleName,    nullsFirst(naturalOrder()))); The static reverseOrdermethod gives the reverse of the natural order. To reverse any comparator, use the reversed instance method. For example, naturalOrder().reversed()is the same as reverseOrder().  To read the lines of a file lazily, use the Files.linesmethod. It yields a stream of strings, one per line of input:  Click here to view code image  Stream lines = Files.lines(path); Optional passwordEntry = lines.filter(s -&gt; s.contains(\"password\")).findFirst(); As soon as the first line containing password is found, no further lines are read from the underlying file.   You will want to close the underlying file. Fortunately, the Stream interface extends AutoCloseable. The streams that you have seen in Chapter 2didn’t need to close any resources. But the Files.linesmethod produces a stream whose closemethod closes the file. The easiest way to make sure the file is indeed closed is to use a Java 7 try-with-resources block:  Click here to view code image  try (Stream lines = Files.lines(path)) {    Optional passwordEntry       = lines.filter(s -&gt; s.contains(\"password\")).findFirst();    ... } // The stream, and hence the file, will be closed here When a stream spawns another, the close methods are chained. Therefore, you can also write  Click here to view code image  try (Stream filteredLines       = Files.lines(path).filter(s -&gt; s.contains(\"password\"))) {    Optional passwordEntry = filteredLines.findFirst();    ... } When filteredLines is closed, it closes the underlying stream, which closes the underlying file.     If you want to be notified when the stream is closed, you can attach an onClosehandler. Here is how you can verify that closing filteredLinesactually closes the underlying stream:  Click here to view code image  try (Stream filteredLines    = Files.lines(path).onClose(() -&gt; System.out.println(\"Closing\"))       .filter(s -&gt; s.contains(\"password\"))) { ... } If an IOExceptionoccurs as the stream fetches the lines, that exception is wrapped into an UncheckedIOExceptionwhich is thrown out of the stream operation.    In Java 8, just use Files.list.  The list method does not enter subdirectories. To process all descendants of a directory, use the Files.walk method instead.  Click here to view code image  try (Stream entries = Files.walk(pathToRoot)) {    // Contains all descendants, visited in depth-first order }      ","categories": [],
        "tags": ["Coding","Java"],
        "url": "/2017/01/14/Java-8.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "HashMap in JDK",
        "excerpt":"Hashmap in JDK  Some note worth points about hashmap     Lookup process            Step# 1: Quickly determine the bucket number in which this element may reside (using key.hashCode()).       Step# 2: Go over the mini-list and return the element that matches the key (using key.equals()).           Immutability of keys   In Node&lt;K,V&gt; node of hashMap, besides key, value, hash, there is Node next link inside. So undelrying table is a linked list.   For get(), firstly using hashcode calculation, divide by bucket number and get reminder, to locate the bucket, then compare key via “.euqals()”, if matched, the value will be returned.   Load factor and resize When new hashHap is being populated, the linkedList associated with each bucket of source hashMap is iterated and nodes are copied to the destination bucket. However, note that these new nodes are prepended to the head of the destination linkedList. So resizing has an side effect of reversing the order of the items in the list. Default load factor for hashMap is 0.75.   Worst-case performance: In the worst case, a hashMap reduces to a linkedList. However with Java 8, there is a change, Java 8 intelligently determines if we are running in the worst-case performance scenario and converts the list into a binary search tree.   Collisions Collisions happen when 2 distinct keys generate the same hashCode() value. Multiple collisions are the result of bad hashCode() algorithm.There are many collision-resolution strategies - chaining, double-hashing, clustering. However, java has chosen chaining strategy for hashMap, so in case of collisions, items are chained together just like in a linkedList.   Some specialized hashMaps for specific purposes: – ConcurrentHashMap: HashMap to be used in multithreaded applications. – EnumMap: HashMap with Enum values as keys. – LinkedHashMap: HashMap with predictable iteration order (great for FIFO/LIFO caches)   WeakReferenceMap, SoftReference Map etc.  Weak references (a WeakHashMap) aren’t particularly good for this either, because as the elements in the cache become dereferenced by the application code they will quickly be removed from the cache by the garbage collector. This basically means there will be cache faults often (in other words, the cache lookups will fail).   Soft references can be very handy in situations such as this. Because soft references only exist if the memory is available, they can make very effective use of the space that is available. Unfortunately, although there is a WeakHashMap , there is no java.util.SoftHashMap . Why? I’m not really sure. Thankfully, a little trip back over to Jakarta-Commons-Collections digs up the org.apache.commons.collections.map.ReferenceMap .   SoftReferences are typically used for implementing memory caching. The JVM should try to keep softly referenced objects in memory as long as possible, and when memory is low clear the oldest soft references first. According to the JavaDoc, there are no guarantees though.   WeakReferences is the reference type I use most frequently. It’s typically used when you want weak listeners or if you want to connect additional information to an object (using WeakHashMap for example). Very useful stuff when you want to reduce class coupling.   Phantom references can be used to perform pre-garbage collection actions such as freeing resources. Instead, people usually use the finalize() method for this which is not a good idea. Finalizers have a horrible impact on the performance of the garbage collector and can break data integrity of your application if you’re not very careful since the “finalizer” is invoked in a random thread, at a random time.   In the constructor of a phantom reference, you specify a ReferenceQueue where the phantom references are enqueued once the referenced objects becomes “phantom reachable”. Phantom reachable means unreachable other than through the phantom reference. The initially confusing thing is that although the phantom reference continues to hold the referenced object in a private field (unlike soft or weak references), its getReference() method always returns null. This is so that you cannot make the object strongly reachable again.   From time to time, you can poll the ReferenceQueue and check if there are any new PhantomReferences whose referenced objects have become phantom reachable. In order to be able to to anything useful, one can for example derive a class from java.lang.ref.PhantomReference that references resources that should be freed before garbage collection. The referenced object is only garbage collected once the phantom reference becomes unreachable itself.   Source code analysis  Internal data structure     transient Entry&lt;K, V&gt;[] elementData;   transient int modCount = 0;   private transient V[] cache;   Put  It will call return putImpl(key, value); directly   V putImpl(K key, V value)     if(key == null) – entry = findNullKeyEntry();     entry = findNullKeyEntry();           if (entry == null) {               modCount++;               entry = createHashedEntry(null, 0, 0);               if (++elementCount &gt; threshold) {                   rehash();               }           }          — findNullKeyEntry Iterate the internal array to locate null entry      final Entry&lt;K,V&gt; findNullKeyEntry() {       Entry&lt;K,V&gt; m = elementData[0];       while (m != null &amp;&amp; m.key != null) {           m = m.next;       }       return m;   }           else (if key is not null)  ","categories": [],
        "tags": ["java"],
        "url": "/2017/01/16/Java-HashMap.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Java Collections Misc",
        "excerpt":"Difference between equals and deepEquals of Arrays in Java     Arrays.equals() method does not compare recursively if an array contains another array   on other hand Arrays.deepEquals() method compare recursively if an array contains another array.   Arrays.equals() check is if element is null or not and then calls equals() method, it does not check for Array type.   It’s better to use Arrays.equals() to compare non-nested Array and Arrays.deepEquals() to compare nested Array, as former is faster than later in the case of non-nested Array.   Checking Array for duplicate elements Java     brute force method which compares each element of Array to all other elements and returns true if it founds duplicates. Though this is not an efficient choice it is the one which first comes to mind. complexity on order of O(n^2) not advised in production   Another quick way of checking if a Java array contains duplicates or not is to convert that array into Set. Since Set doesn’t allow duplicates size of  the corresponding Set will be smaller than original Array if Array contains duplicates otherwise the size of both Array and Set will be same.      List inputList = Arrays.asList(input);  Set inputSet = new HashSet(inputList);           One more way to detect duplication in java array is adding every element of the array into HashSet which is a Set implementation. Since the add(Object obj) method of Set returns false if Set already contains an element to be added, it can be used to find out if the array contains duplicates in Java or not.      If you don’t prefer converting List to Set than you can still go with copying data from one ArrayList to other ArrayList ** and removing duplicates **by checking with ArrayList.contains() method.     List vs Set      In short main difference between List and Set in Java is that List is an ordered collection which allows duplicates while Set is an unordered collection which doesn’t allow duplicates.       how to sort list in reverse order     Collections.sort(unsortedList, Collections.reverseOrder());   customized comparator   Collections.sort(unsortedList, String.CASE_INSENSITIVE_ORDER); // search insensitive order   Find length of a linked list   Iterative Solutions  public int length(){  int count=0;  Node current = this.head;   while(current != null){   count++;   current=current.next()  }  return count; }   Recursive Solution:  public int length(Node current){  if(current == null){ //base case    return 0;  }  return 1+length(current.next()); }   You can see that we have used the fact that last node will point to null to terminate the recursion. This is called the base case. It’s very important to identify a base case while coding a recursive solution, without a base case, your program will never terminate and result in StackOverFlowError.   Iterator     is nothing but a traversing object, made specifically for Collection objects like List and Set.   we have already aware about different kind of traversing methods like for-loop ,while loop,do-while,for each lop etc,they all are  index based traversing but as we know Java is purely object oriented language there is always possible ways of doing things using objects so Iterator is a way to traverse as well as access the data from the collection.   ListIterator     in Java is an Iterator which allows user to traverse Collection like ArrayList and HashSet in both direction by using method previous() and next (). You can obtain ListIterator from all List implementation including ArrayList and LinkedList. ListIterator doesn’t keep current index and its current position is determined by call to next() or previous() based on direction of traversing.   List collection type also supports ListIterator which has add() method to add elements** in collection while Iterating.   Blocking queue     BlockingQueue in Java doesn’t allow null elements, various implementation of BlockingQueue like ArrayBlockingQueue, LinkedBlockingQueue throws NullPointerException when you try to add null on queue.     BlockingQueue can be bounded or unbounded.       A bounded BlockingQueue is one which is initialized with initial capacity and call to put() will be blocked if BlockingQueue is full and size is equal to capacity. This bounding nature makes it ideal to use a shared queue between multiple threads like in most common Producer consumer solutions in Java.   An unbounded Queue is one which is initialized without capacity, actually by default it initialized with Integer.MAX_VALUE. most common example of BlockingQueue uses bounded BlockingQueue as shown in below example.     BlockingQueue&lt;String&gt; bQueue = new ArrayBlockingQueue&lt;String&gt;(2);          BlockingQueue implementations like ArrayBlockingQueue, LinkedBlockingQueue and PriorityBlockingQueue are thread-safe.       All queuing method uses concurrency control and internal locks to perform operation atomically.   BlockingQueue interface extends Collection, Queue and Iterable interface which provides it all Collection and Queue related methods like poll(), and peak(), unlike take(), peek() method returns head of the queue without removing it, poll() also retrieves and removes elements from head but can wait till specified time if Queue is empty.   Reference     http://javarevisited.blogspot.com/2012/12/how-to-compare-arrays-in-java-equals-deepequals-primitive-object.html#ixzz4WBifkyjg   http://javarevisited.blogspot.in/2012/02/how-to-check-or-detect-duplicate.html   http://javarevisited.blogspot.com/2012/12/how-to-remove-duplicates-elements-from-ArrayList-Java.html#ixzz4WBkudSwR   http://javarevisited.blogspot.in/2012/04/difference-between-list-and-set-in-java.html   http://javarevisited.blogspot.in/2012/01/how-to-sort-arraylist-in-java-example.html   http://javarevisited.blogspot.in/2016/05/how-do-you-find-length-of-singly-linked.html   http://javarevisited.blogspot.in/2011/10/java-iterator-tutorial-example-list.html   http://javarevisited.blogspot.in/2012/12/blocking-queue-in-java-example-ArrayBlockingQueue-LinkedBlockingQueue.html  ","categories": [],
        "tags": ["java","Collections"],
        "url": "/2017/01/17/Java-Collections-Misc.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Compare-In-Java",
        "excerpt":"Concepts     If you implement Comparable interface and override compareTo() method it must be consistent with equals() method i.e. for equal object by equals() method compareTo() must return zero. failing to so will affect contract of SortedSet e.g. TreeSet and SortedMap like TreeMap, which uses compareTo() method for checking equality   Remember to use Collections.reverseOrder() comparator for sorting Object in reverse order or descending order, as shown in this example.   Use Generics while implementing Comparator and Comparable interface, that prevents error of accidentally overloading compareTo() and compare() method instead of overriding it because both of these methods accept Object as a parameter. By using Generics and @Override annotation we effectively remove that subtle error.   Comparable vs Comparator     Comparator in Java is defined in java.util package while Comparable interface in Java is defined in java.lang package, which very much says that Comparator should be used as an utility to sort objects which Comparable should be provided by default   Comparator interface in Java has method public int compare (Object o1, Object o2) which returns a negative integer, zero, or a positive integer as the first argument is less than, equal to, or greater than the second. While Comparable interface has method public int compareTo(Object o) which returns a negative integer, zero, or a positive integer as this object is less than, equal to, or greater than the specified object.   If any class implement Comparable interface in Java then collection of that object either List or Array can be sorted automatically by using  Collections.sort() or Arrays.sort() method and object will be sorted based on there natural order defined by CompareTo method.   Objects which implement Comparable in Java  can be used as keys in a SortedMap like TreeMap or elements in a SortedSet  for example TreeSet, without specifying any Comparator.   Generally you should not use difference of integers to decide output of compareTo method as result of **integer subtraction can overflow ** but if you are sure that both operands are positive then its one of the quickest way to compare two objects.   Some time you write code to sort object of a class for which you are not the original author, or you don’t have access to code. In these cases you can not implement Comparable and Comparator is only way to sort those objects.   Beware with the fact that How those object will behave if stored in SorteSet or SortedMap like TreeSet and TreeMap. If an object doesn’t implement Comparable than while putting them into SortedMap, always provided corresponding Comparator which can provide sorting logic.   Comparator has a distinct advantage of being self descriptive  for example if you are writing Comparator to compare two Employees based upon there salary than name that comparator as SalaryComparator, on the other hand compareTo()   So in Summary if you want to sort objects based on natural order then use Comparable in Java and if you want to **sort on some other attribute of object then use Comparator ** in Java.   Samples of Java class     How to Compare String in Java String is immutable in Java and one of the most used value class. For comparing String in Java we should not be worrying because String implements Comparable interface and provides a lexicographic implementation for CompareTo method which compare two strings based on contents of characters or you can say in lexical order. You just need to call String.compareTo(AnotherString) and Java will determine whether specified String is greater than , equal to or less than current object.   Dates are represented by java.util.Date class in Java and like String,  Date also implements Comparable in Java so they will be automatically sorted based on there natural ordering if they got stored in any sorted collection like TreeSet or TreeMap. If you explicitly wants to compare two dates in Java you can call Date.compareTo(AnotherDate) method in Java and it will tell whether specified date is greater than , equal to or less than current String.   Exceptions   One example where compareTo is not consistent with equals in JDK is BigDecimal class. two BigDecimal number for which compareTo returns zero, equals returns false as clear from following BigDecimal comparison example:  BigDecimal bd1 = new BigDecimal(\"2.0\"); BigDecimal bd2 = new BigDecimal(\"2.00\");  System.out.println(\"comparing BigDecimal using equals: \" + bd1.equals(bd2)); System.out.println(\"comparing BigDecimal using compareTo: \" + bd1.compareTo(bd2));  Output:  comparing BigDecimal using equals: false comparing BigDecimal using compareTo: 0  How does it affect BigDecimal ? well if you store these two BigDecimal in HashSet you will end up with duplicates (violation of Set Contract) i.e. two elements while if you store them in TreeSet you will end up with just 1 element because HashSet uses equals to check duplicates while TreeSet uses compareTo to check duplicates. That’s why its suggested to keep compareTo consistent with equals method in java.      Another important point to note is don’t use subtraction for comparing integral values because result of subtraction can overflow as every int operation in Java is modulo 2^32. use either Integer.compareTo()  or logical operators for comparison.   In summary compareTo should provide natural ordering and compareTo must be consistent with equals() method in Java.   package test;  import java.util.ArrayList; import java.util.Collections; import java.util.Comparator; import java.util.List;  /**  *  * Java program to test Object sorting in Java. This Java program  * test Comparable and Comparator implementation provided by Order  * class by sorting list of Order object in ascending and descending order.  * Both in natural order using Comparable and custom Order using Comparator in Java  *  * @author http://java67.blogspot.com  */ public class ObjectSortingExample {      public static void main(String args[]) {               //Creating Order object to demonstrate Sorting of Object in Java         Order ord1 = new Order(101,2000, \"Sony\");         Order ord2 = new Order(102,4000, \"Hitachi\");         Order ord3 = new Order(103,6000, \"Philips\");               //putting Objects into Collection to sort         List&lt;Order&gt; orders = new ArrayList&lt;Order&gt;();         orders.add(ord3);         orders.add(ord1);         orders.add(ord2);               //printing unsorted collection         System.out.println(\"Unsorted Collection : \" + orders);               //Sorting Order Object on natural order - ascending         Collections.sort(orders);               //printing sorted collection         System.out.println(\"List of Order object sorted in natural order : \" + orders);               // Sorting object in descending order in Java         Collections.sort(orders, Collections.reverseOrder());         System.out.println(\"List of object sorted in descending order : \" + orders);                       //Sorting object using Comparator in Java         Collections.sort(orders, new Order.OrderByAmount());         System.out.println(\"List of Order object sorted using Comparator - amount : \" + orders);               // Comparator sorting Example - Sorting based on customer         Collections.sort(orders, new Order.OrderByCustomer());         System.out.println(\"Collection of Orders sorted using Comparator - by customer : \" + orders);     } }  /*  * Order class is a domain object which implements  * Comparable interface to provide sorting on the natural order.  * Order also provides couple of custom Comparators to  * sort object based upon amount and customer  */ class Order implements Comparable&lt;Order&gt; {      private int orderId;     private int amount;     private String customer;      /*      * Comparator implementation to Sort Order object based on Amount      */     public static class OrderByAmount implements Comparator&lt;Order&gt; {          @Override         public int compare(Order o1, Order o2) {             return o1.amount &gt; o2.amount ? 1 : (o1.amount &lt; o2.amount ? -1 : 0);         }     }      /*      * Anohter implementation or Comparator interface to sort list of Order object      * based upon customer name.      */     public static class OrderByCustomer implements Comparator&lt;Order&gt; {          @Override         public int compare(Order o1, Order o2) {             return o1.customer.compareTo(o2.customer);         }     }      public Order(int orderId, int amount, String customer) {         this.orderId = orderId;         this.amount = amount;         this.customer = customer;     }        public int getAmount() {return amount; }     public void setAmount(int amount) {this.amount = amount;}      public String getCustomer() {return customer;}     public void setCustomer(String customer) {this.customer = customer;}      public int getOrderId() {return orderId;}     public void setOrderId(int orderId) {this.orderId = orderId;}      /*      * Sorting on orderId is natural sorting for Order.      */     @Override     public int compareTo(Order o) {         return this.orderId &gt; o.orderId ? 1 : (this.orderId &lt; o.orderId ? -1 : 0);     }       /*      * implementing toString method to print orderId of Order      */     @Override     public String toString(){         return String.valueOf(orderId);     } }  Output Unsorted Collection : [103, 101, 102] List of Order object sorted in natural order : [101, 102, 103] List of object sorted in descending order : [103, 102, 101] List of Order object sorted using Comparator - amount : [101, 102, 103] Collection of Orders sorted using Comparator - by customer : [102, 103, 101]     Reference     http://www.java67.com/2012/10/how-to-sort-object-in-java-comparator-comparable-example.html  ","categories": [],
        "tags": ["java","compare"],
        "url": "/2017/01/19/Compare-In-Java.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "NavigableMap Misc",
        "excerpt":"What is NavigableMap      NavigableMap in Java 6 is an extension of SortedMap  like TreeMap which provides convenient navigation method like lowerKey, floorKey, ceilingKey and higherKey.   NavigableMap is added on Java 1.6 and along with these popular navigation method it also provide ways to create a Sub Map from existing Map in Java e.g. headMap whose keys are less than specified key, tailMap whose keys are greater than specified key and a subMap which is strictly contains keys which falls between toKey and fromKey. All of these methods also provides a boolean to include specified key or not. TreeMap and ConcurrentSkipListMap are two concrete implementation of NavigableMap in Java 1.6 API.   use NavigableMap   How to create subMap from Map using Navigable Map in Java with Example In this Java tutorial we will explore some API methods of NavigableMap to show its functionality. This Java program shows example of lowerKey which returns keys less than specified, floorKey returns key less than or equal to, ceilingKey return greater than or equal to and higherKey which returns keys which are greater than specified key.   This Java example also demonstrate use of headMap(), tailMap() and subMap() method which is used to create Map from an existing Map in Java. headMap returns a Map whose keys are lower than specified keys while tailMap returns Map which contains keys, those are higher than specified. Here is complete code example of How to use NavigableMap in Java.   import java.util.NavigableMap; import java.util.TreeMap;  /**  *  * Java program to demonstrate What is NavigableMap in Java and How to use NavigableMap  * in Java. NavigableMap provides two important features navigation methods  * like lowerKey(), floorKey, ceilingKey() and higherKey().  * There Entry counterpart and methods to create subMap e.g. headMap(), tailMap()  * and subMap().  *  * @author Javin Paul  */ public class NavigableMapExample {      public static void main(String args[]) {               //NavigableMap extends SortedMap to provide useful navigation methods         NavigableMap&lt;String, String&gt; navigableMap = new TreeMap&lt;String, String&gt;();               navigableMap.put(\"C++\", \"Good programming language\");         navigableMap.put(\"Java\", \"Another good programming language\");         navigableMap.put(\"Scala\", \"Another JVM language\");         navigableMap.put(\"Python\", \"Language which Google use\");               System.out.println(\"SorteMap : \" + navigableMap);               //lowerKey returns key which is less than specified key         System.out.println(\"lowerKey from Java : \" + navigableMap.lowerKey(\"Java\"));               //floorKey returns key which is less than or equal to specified key         System.out.println(\"floorKey from Java: \" + navigableMap.floorKey(\"Java\"));               //ceilingKey returns key which is greater than or equal to specified key         System.out.println(\"ceilingKey from Java: \" + navigableMap.ceilingKey(\"Java\"));               //higherKey returns key which is greater specified key         System.out.println(\"higherKey from Java: \" + navigableMap.higherKey(\"Java\"));                     //Apart from navigagtion methodk, it also provides useful method         //to create subMap from existing Map e.g. tailMap, headMap and subMap               //an example of headMap - returns NavigableMap whose key is less than specified         NavigableMap&lt;String, String&gt; headMap = navigableMap.headMap(\"Python\", false);         System.out.println(\"headMap created form navigableMap : \" + headMap);                       //an example of tailMap - returns NavigableMap whose key is greater than specified         NavigableMap&lt;String, String&gt; tailMap = navigableMap.tailMap(\"Scala\", false);         System.out.println(\"tailMap created form navigableMap : \" + tailMap);               //an example of subMap - return NavigableMap from toKey to fromKey         NavigableMap&lt;String, String&gt; subMap = navigableMap.subMap(\"C++\", false ,                                                                   \"Python\", false);         System.out.println(\"subMap created form navigableMap : \" + subMap);     } }  Output: SorteMap : {C++=Good programming language, Java=Another good programming language, Python=Language which Google use, Scala=Another JVM language} lowerKey from Java : C++ floorKey from Java: Java ceilingKey from Java: Java higherKey from Java: Python headMap created form navigableMap : {C++=Good programming language, Java=Another good programming language} tailMap created form navigableMap : {} subMap created form navigableMap : {Java=Another good programming language}   That’s all on What is NavigableMap in Java and How to use NavigableMap with example. We have seen examples of popular navigation method on TreeMap e.g. floorKey. You can also use similar method like lowerEntry, floorEntry, ceilingEntry and higherEntry to retrieve Entry instead of key. NavigableMap is also a good utility to create subset of a Map in Java.   Reference  http://javarevisited.blogspot.com/2013/01/what-is-navigablemap-in-java-6-example-submap-head-tail.html#ixzz4WBdqjJCC  ","categories": [],
        "tags": ["java","NavigableMap"],
        "url": "/2017/01/19/Java-NavigableMap.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Java-Tricky-Tech-Questions.md",
        "excerpt":"What is the difference between Serializable and Externalizable in Java?     In earlier version of Java, reflection was very slow, and so serializaing large object graphs (e.g. in client-server RMI applications) was a bit of a performance problem. To handle this situation, the java.io.Externalizable interface was provided, which is like java.io.Serializable but with custom-written mechanisms to perform the marshalling and unmarshalling functions (you need to implement readExternal and writeExternal methods on your class). This gives you the means to get around the reflection performance bottleneck.   In recent versions of Java (1.3 onwards, certainly) the performance of reflection is vastly better than it used to be, and so this is much less of a problem. I suspect you’d be hard-pressed to get a meaningful benefit from Externalizable with a modern JVM.   Also, the built-in Java serialization mechanism isn’t the only one, you can get third-party replacements, such as JBoss Serialization, which is considerably quicker, and is a drop-in replacement for the default.   A big downside of Externalizable is that you have to maintain this logic yourself - if you add, remove or change a field in your class, you have to change your writeExternal/readExternal methods to account for it.   In summary, Externalizable is a relic of the Java 1.1 days. There’s really no need for it any more.   write transcient fields     Storing and reconstituting the transient data can also be achieved by implementing the Externalizable interface and implementing the writeExternal( ) and the readExternal( ) methods of that interface.   Java Class     Class objects for known types can also be written as “class literals”:     // Express a class literal as a type name followed by \".class\" c = int.class; // Same as Integer.TYPE c = String.class; // Same as \"a string\".getClass() c = byte[].class; // Type of byte arrays           For primitive types and void, we also have class objects that are represented as literals:     // Obtain a Class object for primitive types with various  // predefined constants c = Void.TYPE; // The special \"no-return-value\" type c = Byte.TYPE; // Class object that represents a byte c = Integer.TYPE; // Class object that represents an int c = Double.TYPE; // etc; see also Short, Character, Long, Float           Convert arrays to ArrayList, two different approaches  // View array as an ungrowable list List l = Arrays.asList(a);           // Make a growable copy of the view List m = new ArrayList(l);   What’s seed in Java  Since the next number in a pseudorandom generator is determined by the pre- vious number(s), such a generator always needs a place to start, which is called its seed. The sequence of numbers generated for a given seed will always be the same. The seed for an instance of the java.util.Random class can be set in its constructor or with its setSeed() method.   Cipher     Given a variable c that is known to be an uppercase letter, the Java computation, j = c − ‘A’ produces the desired index j. As a sanity check, if character c is ‘A’, then j = 0. When c is ‘B’, the difference is 1.   Uncaught exception handler     The Thread API also provides the UncaughtExceptionHandler facility, which lets you detect when a thread dies due to an uncaught exception. The two approaches are complementary: taken together, they provide defense-in- depth against thread leakage.   When a thread exits due to an uncaught exception, the JVM reports this event to an application-provided UncaughtExceptionHandler (see Listing 7.24); if no handler exists, the default behavior is to print the stack trace to System.err   In long-running applications, always use uncaught exception handlers for all threads that at least log the exception.   To set an UncaughtExceptionHandler for pool threads, provide a ThreadFac- tory to the ThreadPoolExecutor constructor.   Somewhat confusingly, exceptions thrown from tasks make it to the uncaught exception handler only for tasks submitted with execute ; for tasks submitted with submit, any thrown exception, checked or not, is considered to be part of the task’s return status. If a task submitted with submit terminates with an exception, it is rethrown by Future.get, wrapped in an ExecutionException.     public class UEHLogger implements Thread.UncaughtExceptionHandler{ public void uncaughtException(Thread t, Throwable e){   Logger logger=Logger.getAnonymousLogger();   logger.log(Level.SEVERE, \"Thread terminated with exception:\"+e.getName(),e); } }           What is the difference between Iterator and Enumeration     Iterator duplicate functionality of Enumeration with one addition of remove() method   Another difference is that Iterator is more safe than Enumeration and doesn’t allow another thread to modify collection object during iteration except remove() method and throws ConcurrentModificaitonException.   How does HashSet is implemented in Java, How does it use Hashing     HashSet is built on top of HashMap. If you look at source code of java.util.HashSet class, you will find that that it uses a HashMap with same values for all keys, as shown below:     private transient HashMap map; // Dummy value to associate with an Object in the backing Map private static final Object PRESENT = new Object(); // When you call add() method of HashSet, it put entry in HashMap : public boolean add(E e) { return map.put(e, PRESENT)==null; }          What do you need to do to use a custom object as a key in Collection classes like Map or Set? (answer)       If you are using any custom object in Map as key, you need to override equals() and hashCode() method, and make sure they follow their contract.   On the other hand if you are storing a custom object in Sorted Collection e.g. SortedSet or SortedMap, you also need to make sure that your equals() method is consistent to compareTo() method, otherwise that collection will not follow there contacts e.g. Set may allow duplicates.   Java Generics ? , E and T what is the difference?  Well there’s no difference between the first two - they’re just using different names for the type parameter (E or T).   The third isn’t a valid declaration - ? is used as a wildcard which is used when providing a type argument, e.g. List&lt;?&gt; foo = … means that foo refers to a list of some type, but we don’t know what.   Question: What does the following Java program print?   public class Test {     public static void main(String[] args) {         System.out.println(Math.min(Double.MIN_VALUE, 0.0d));     } }   Anwser 0.0d  The Double.MIN_VALUE is 2^(-1074), a double constant whose magnitude is the least among all double values.   What will happen if you put return statement or System.exit () on try or catch block? Will finally block execute?  Answer of this tricky question in Java is that finally block will execute even if you put a return statement in the try block or catch block but finally block won’t run if you call System.exit() from try or catch block.   Question: Can you override a private or static method in Java?   Another popular Java tricky question, As I said method overriding is a good topic to ask trick questions in Java. Anyway, you can not override a private or static method in Java, if you create a similar method with same return type and same method arguments in child class then it will hide the superclass method, this is known as method hiding.   Similarly, you cannot override a private method in sub class because it’s not accessible there, what you do is create another private method with the same name in the child class. See Can you override a private method in Java or more details   Question: What do the expression 1.0 / 0.0 will return? will it throw Exception? any compile time error?   Answer: This is another tricky question from Double class. Though Java developer knows about the double primitive type and Double class, while doing floating point arithmetic they don’t pay enough attention to Double.INFINITY , NaN, and -0.0 and other rules that govern the arithmetic calculations involving them. The simple answer to this question is that it will not throw ArithmeticExcpetion and return Double.INFINITY.   Also, note that the comparison x == Double.NaN always evaluates to false, even if x itself is a NaN. To test if x is a NaN, one should use the method call Double.isNaN(x) to check if given number is NaN or not. If you know SQL, this is very close to NULL there.   Overloading vs Overriding     Main difference comes from the fact that method overloading is resolved during compile time, while method overriding is resolved at runtime   Also rules of overriding or overloading a method are different in Java.            a private, static and final method cannot be overriding in Java but you can still overload them       overriding both name and signature of the method must remain same, but in for overloading method, the signature must be different       call to overloaded methods are resolved using static binding while the call to overridden method is resolved using dynamic binding in Java.           Java programmer to declare method with same name but different behavior. Method overloading and method overriding is based on Polymorphism in Java.   In case of method overloading, method with same name co-exists in same class but they must have different method signature, while in case of method overriding, method with same name is declared in derived class or sub class   If you have two methods with same name in one Java class with different method signature than its called overloaded method in Java. You can also overload constructor in Java   Since return type is not part of method signature simply changing return type will result in duplicate method and you will get compile time error in Java.   you can also overload private and final method in Java but you can not override them   When you override a method in Java its signature remains exactly same including return type. Rules of override:            Method signature must be same including return type, number of method parameters, type of parameters and order of parameters . Be advised: as of Java 5, you’re allowed to change the return type in the overriding method as long as the new return type is a subtype of the declared return type of the overridden (super class) method       Overriding method can not throw higher Exception than original or overridden method. means if original method throws IOException than overriding method can not throw super class of IOException e.g. Exception but it can throw any sub class of IOException or simply does not throw any Exception. This rule only applies to checked Exception in Java, overridden method is free to throw any unchecked Exception.       Overriding method can not reduce accessibility of overridden method , means if original or overridden method is public than overriding method can not make it protected.           Another important point is that you can not override static method in Java because they are associated with Class rather than object and resolved and bonded during compile time and that’s the reason you cannot override main method in Java   From Java 5 onwards you can use annotation in Java to declare overridden method just like we did with @override. ** @override annotation allows compiler** , IDE like NetBeans and Eclipse to cross verify or check  if this method is really overrides super class method or not.   Static vs. Dynamic binding     private, final and static methods and variables uses static binding and bonded by compiler while virtual methods are bonded during runtime based upon runtime object.   Here is sample of static binding     public class StaticBindingTest {     public static void main(String args[])  {      Collection c = new HashSet();      StaticBindingTest et = new StaticBindingTest();      et.sort(c);         }       //overloaded method takes Collection argument   public Collection sort(Collection c){       System.out.println(\"Inside Collection sort method\");       return c;   }   //another overloaded method which takes HashSet argument which is sub class   public Collection sort(HashSet hs){       System.out.println(\"Inside HashSet sort method\");       return hs;   }     } Output: Inside Collection sort method          because it was bonded on compile time based on type of variable (Static binding)  which was collection.       Here is sample of dynamic binding ```java public class DynamicBindingTest {   public static void main(String args[]) {       Vehicle vehicle = new Car(); //here Type is vehicle but object will be Car       vehicle.start();       //Car’s start called because start() is overridden method   } }   class Vehicle {     public void start() {         System.out.println(“Inside start method of Vehicle”);     } }   class Car extends Vehicle {     @Override     public void start() {         System.out.println(“Inside start method of Car”);     } }   Output: Inside start method of Car   In summary, bottom line is static binding is a compile time operation while dynamic binding is a runtime. one uses Type and other uses Object to bind. static, private and final methods and variables are resolved using static binding which makes there execution fast because no time is wasted to find correct method during runtime.  # Static in Java - static keyword can not be applied on top level class. Making a top level class static in Java will result in compile time error. -  Beware that if your static initialize block throws Exception than you may **get java.lang.NoClassDefFoundError** when you try to access the class which failed to load. -  **Static method can not be overridden** in Java as they belong to class and not to object ```java  public class TradingSystem {      public static void main(String[] args) {         TradingSystem system = new DirectMarketAccess();         DirectMarketAccess dma = new DirectMarketAccess();                  // static method of Instrument class will be called,          // even though object is of sub-class DirectMarketAccess         system.printCategory();                  //static method of EquityInstrument class will be called         dma.printCategory();     }      public static void printCategory(){         System.out.println(\"inside super class static method\");     } }  class DirectMarketAccess extends TradingSystem{     public static void printCategory(){         System.out.println(\"inside sub class static method\");     } }  Output: inside super class static method inside sub class static method     This shows that static method can not be overridden in Java and concept of method overloading doesn’t apply to static methods. Instead declaring same static method on Child class is known as method hiding in Java   Consider making a static variable final in Java to make it constant and avoid changing it from anywhere in the code   when to use Singleton vs Static Class in Java for those purpose,answer is that if its completely stateless and it work on provided data then you can go for static class otherwise Singleton pattern is a better choice.     When to make a method static in Java       Method doesn’t depends on object’s state, in other words doesn’t depend on any member variable and everything they need is passes as parameter to them   Utility methods are good candidate of making static in Java because then they can directly be accessed using class name without even creating any instance. Classic example is java.lang.Math   Static method in Java is very popular to implement Factory design pattern. Since Generics also provides type inference during method invocation, use of static factory method to create object is popular Java idiom.   Phone     Key to success in telephonic is to the point and concise answer.   Difference between String, StringBuffer and StringBuilder in Java     String is immutable while both StringBuffer and StringBuilder is mutable, which means any change e.g. converting String to upper case or trimming white space will produce another instance rather than changing the same instance. On later two, StringBuffer is synchronized while StringBuilder is not, in fact its a ditto replacement of StringBuffer added in Java 1.5.     Difference between extends Thread vs implements Runnable in Java?       Difference comes from the fact that you can only extend one class in Java, which means if you extend Thread class you lose your opportunity to extend another class, on the other hand if you implement Runnable, you can still extend another class.   Difference between Runnable and Callable interface in Java?     Runnable was the only way to implement a task in Java which can be executed in parallel before JDK 1.5 adds Callable. Just like Runnable, Callable also defines a single call() method but unlike run() it can return values and throw exceptions.   Difference between ArrayList and LinkedList in Java?     In short, ArrayList is backed by array in Java, while LinkedList is just collection of nodes, similar to linked list data structure. ArrayList also provides random search if you know the index, while LinkedList only allows sequential search. On other hand, adding and removing element from middle is efficient in LinkedList as compared to ArrayList because it only require to modify links and no other element is rearranged.   What is difference between wait and notify in Java?     Both wait and notify methods are used for inter thread communication, where wait is used to pause the thread on a condition and notify is used to send notification to waiting threads. Both must be called from synchronized context e.g. synchronized method or block.   Difference between HashMap and Hashtable in Java?     Though both HashMap and Hashtable are based upon hash table data structure, there are subtle difference between them. HashMap is non synchronized while Hashtable is synchronized and because of that HashMap is faster than Hashtable, as there is no cost of synchronization associated with it. One more minor difference is that HashMap allows a null key but Hashtable doesn’t.   Check a number is prime or not     We learned numbers are prime if the only divisors they have are 1 and itself. Trivially, we can check every integer from 1 to itself (exclusive) and test whether it divides evenly. Check the source code of ** PrimeTester.java**            naive approach:  We learned numbers are prime if the only divisors they have are 1 and itself. Trivially,  we can check every integer from 1 to itself (exclusive)  and test whether it divides evenly.          for(int i=2;2*i&lt;=n;i++){        if(n%i==0)                       power of 2 approach:    further enhance, as if 2 divides some interger n, then (n/2) divides n as well so we’ll times of 2. Please be advised in for loop, should use 2*i&lt;=n, rather than “&lt;n”,   otherwise, 4 will be return as ture mistakely          for(int i=2;2*i&lt;=n;i++){        if(n%i==0)                       isPrimeSquare approach:  \t we notice that you really only have to go up to the square root of n,   because if you list out all of the factors of a number,  the square root will always be in the middle Finally, we know 2 is the “oddest” prime - it happens to be the only even prime number. Because of this, we need only check 2 separately, then traverse  odd numbers up to the square root of n.  In the end, our code will resemble this:          // check if n is a multiple of 2  if(n&gt;2&amp;&amp;n%2==0)        return false;\t\t    // if not, then just check the odds    for(int i=3;i*i&lt;=n;i+=2){        if(n%i==0)                   Difference between abstract class and interface?                   From Java 8 onwards difference between abstract class and interface in Java has minimized, now even interface can have implementation in terms of default and static method. BTW, in Java you can still extend just one class but can extend multiple inheritance. Abstract class is used to provide default implementation with just something left to customize, while interface is used heavily in API to define contract of a class.   How to Swap Two Numbers without Temp or Third variable in Java  Swapping two numbers without using temp variable in Java  int a = 10; int b = 20; System.out.println(\"value of a and b before swapping, a: \" + a +\" b: \" + b);  //swapping value of two numbers without using temp variable a = a+ b; //now a is 30 and b is 20 b = a -b; //now a is 30 but b is 10 (original value of a) a = a -b; //now a is 20 and b is 10, numbers are swapped  System.out.println(\"value of a and b after swapping, a: \" + a +\" b: \" + b); Output: value of a and b before swapping, a: 10 b: 20 value of a and b after swapping, a: 20 b: 10  Swapping two numbers without using temp variable in Java with bitwise operator  int a = 2; //0010 in binary int b = 4; //0100 in binary System.out.println(\"value of a and b before swapping, a: \" + a +\" b: \" + b); // 6  is     0110 //swapping value of two numbers without using temp variable and XOR bitwise operator      a = a^b; //now a is 6 (0110) and b is 4(0100) b = a^b; //now a is 6 but b is 2 (0010) (original value of a) a = a^b; //now a is 4 and b is 2, numbers are swapped  System.out.println(\"value of a and b after swapping using XOR bitwise operation, a: \" + a +\" b: \" + b); value of a and b before swapping, a: 2 b: 4 value of a and b after swapping using XOR bitwise operation, a: 4 b: 2   Swapping two numbers without using temp variable in Java with division and multiplication     There is another, third way of swapping two numbers without using third variable, which involves multiplication and division operator. ```java int a = 6; int b = 3; System.out.println(“value of a and b before swapping, a: “ + a +” b: “ + b);   //swapping value of two numbers without using temp variable using multiplication and division a = a*b; //now a is 18 and b is 3 b = a/b; //now a is 18 but b is 6 (original value of a) a = a/b; //now a is 3 and b is 6, numbers are swapped   System.out.println(“value of a and b after swapping using multiplication and division, a: “ + a +” b: “ + b); Output: value of a and b before swapping, a: 6 b: 3 value of a and b after swapping using multiplication and division, a: 3 b: 6 ```     That’s all on 3 ways to swap two variables without using third variable in Java. Its good to know multiple ways of swapping two variables without using temp or third variable to handle any follow-up question. Swapping numbers using bitwise operator is the fastest among three, because it involves bitwise operation. It’s also great way to show your knowledge of bitwise operator in Java and impress interviewer, which then may ask some question on bitwise operation. A nice trick to drive interview on your expert area.   Bitwise operator     ”~” inverts a bit pattern; it can be applied to any of the integral types, making every “0” a “1” and every “1” a “0”.   The bitwise &amp; operator performs a bitwise AND operation.   The bitwise ^ operator performs a bitwise exclusive OR operation.                                   The bitwise           operator performs a bitwise inclusive OR operation.                           How to check if linked list contains loop in Java?     Algorithm to find if linked list contains loops or cycles. Two pointers, fast and slow is used while iterating over linked list. Fast pointer moves two nodes in each iteration, while slow pointer moves to one node. If linked list contains loop or cycle than both fast and slow pointer will meet at some point during iteration. If they don’t meet and fast or slow will point to null, then linked list is not cyclic and it doesn’t contain any loop.            Use two pointers fast and slow       Move fast two nodes and slow one node in each iteration       If fast and slow meet then linked list contains cycle       if fast points to null or fast.next points to null then linked list is not cyclic           This algorithm is also known as Floyd’s cycle finding algorithm and popularly known as tortoise and hare algorithm to find cycles in linked list. Sample can be found via “LoopInList.java”   Reference     http://www.java67.com/2012/09/top-10-tricky-java-interview-questions-answers.html   http://javarevisited.blogspot.in/2012/03/what-is-static-and-dynamic-binding-in.html   http://www.java67.com/2015/03/top-40-core-java-interview-questions-answers-telephonic-round.html   http://www.mkyong.com/java/how-to-determine-a-prime-number-in-java/   http://javarevisited.blogspot.in/2013/02/swap-two-numbers-without-third-temp-variable-java-program-example-tutorial.html   http://docs.oracle.com/javase/tutorial/java/nutsandbolts/op3.html   http://javarevisited.blogspot.in/2013/05/find-if-linked-list-contains-loops-cycle-cyclic-circular-check.html  ","categories": [],
        "tags": ["java","Questions"],
        "url": "/2017/01/19/Java-Tricky-Tech-Questions.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Dead Lock",
        "excerpt":"Concept      Two or more threads are waiting for each other to release lock and get stuck for infinite time.   It will only happen in case of multitasking.   One screnario of dead lock  If method1() and method2() both will be called by two or many threads , there is a good chance of deadlock because if thread 1 acquires lock on Sting object while executing method1() and thread 2 acquires lock on Integer object while executing method2() both will be waiting for each other to release lock on Integer and String to proceed further which will never happen.   The root cause is NOT multithreading, but the way they are requiring lock  Now there would not be any deadlock because both methods are accessing lock on Integer and String class literal in same order. So, if thread A acquires lock on Integer object , thread B will not proceed until thread A releases Integer lock, same way thread A will not be blocked even if thread B holds String lock because now thread B will not expect thread A to release Integer lock to proceed further.   Reference     http://javarevisited.blogspot.in/2010/10/what-is-deadlock-in-java-how-to-fix-it.html  ","categories": [],
        "tags": ["java","dead lock"],
        "url": "/2017/01/20/DeadLock.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Algorithm",
        "excerpt":"This page is about key points about Algorithm   Methodology     The easiest way to improve search efficiency on a set of data is to put it in a data structure that allows more efficient searching. What data structures can be searched more efficiency than O(n)? Binary tree can be searched in O(log(n)). Arrays and hash tables both have constant time element look up (has tables have worse-case lookup of O(n) but the average case is O(1)).   Then need to determine which data structure to be used. If the underlying characters are just ASCII, then a array[128] would be enough. But characters are UNICODe, then it need 100,000 (100K) array, which is a concern of memory, so hash table would be a better option, which only keep exist characters. In general, arrays are a better choice for long strings with a limited set of possible characters values, hash tables are more efficient for shorter strings or when there are many possible character values.   For some problems, obvious iterative alternatives like the one just shown don’t exist, but it’s always possible to implement a recursive algorithm without using recursive calls.   For a simple recursive function like factorial, many computer architectures spend more time on call overhead than on the actual calculation. Iterative functions, which use looping constructs instead of recursive function calls, do not suffer from this overhead and are frequently more efficient.   NOTE Iterative solutions are usually more efficient than recursive solutions.   NOTE Every recursive case must eventually lead to a base case.   NOTE Recursive algorithms have two cases: recursive cases and base cases   Sort  I collections.sort()   Object[] a = list.toArray();         Arrays.sort(a);         ListIterator&lt;T&gt; i = list.listIterator();         for (int j=0; j&lt;a.length; j++) {             i.next();             i.set((T)a[j]);         }    Arrays.sort  public static void sort(Object[] a) {         if (LegacyMergeSort.userRequested)             legacyMergeSort(a);         else             ComparableTimSort.sort(a);     } private static void mergeSort(Object[] src,                                   Object[] dest,                                   int low,                                   int high,                                   int off) {         int length = high - low;          // Insertion sort on smallest arrays         if (length &lt; INSERTIONSORT_THRESHOLD) { // threshold is 7             for (int i=low; i&lt;high; i++)                 for (int j=i; j&gt;low &amp;&amp;                          ((Comparable) dest[j-1]).compareTo(dest[j])&gt;0; j--)                     swap(dest, j, j-1);             return;         }  // else use mergeSort    Self review  CeasarCipher:   Generally: it’s a rotation of English alphabic. E.g. if rotation is 2, the encode is start from A+2, i.e. A is at -2 of the encode array.  And decode is start with 26-2, and “A” start at positon 2, then the increase by 1 character to constitute the array That’s why need to “%26”, to make it loop across 26 characters   If rotation is 2: --- encrytpion code is:[C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z, A, B] --- decrytpion code is:[Y, Z, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X] If rotation is 4: --- encrytpion code is:[E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z, A, B, C, D] --- decrytpion code is:[W, X, Y, Z, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V]   ABC: Msg ={‘A’,’B’,’C’}; Char[] encode=’A’+(k+rotation)%26); Char[] decode=’A’+(k-rotation+26)%26); // +26 to avoid negative  Encode={‘C’,’D’,’E’}; // rotation=3, so A+3, A+4,A+5,xxx, A+26=&gt;3,4,5,6,xxx,0 Decode={‘M’,’N’,’O’};//as k-rotation+26 % 26, so it’s 26-3+0,26-3+1 ,xx: =&gt; 23,24,25,0,1,2,3,4,is: A+23,A+24,A+25=&gt;‘M’,’N’,’O’. that’s rotation, rotain-1, rotaion -2 xxxx For(int i=0;i&lt;msg.length;i++){   Int j=msg[i]-‘A’; // to remove the base ‘A”, so sync with the “k” in encode, 3,4,5,xxx 3+26   Msg[i]=codes[j]; } // encode Int j=’A’-‘A’; //0 Msg[0]=’C’; Msg[1]=’D’; Msg[2]=’E’;  //decode Int j=msg[i]-‘A’; //’C’-‘A’=3 Msg[i]=decode[j]; // correspoindg to the postion 0,-xxx, 26 in decode,  Msg[0]=      If you says “tree,” it’s a good idea to clarify whether she is referring to a generic tree or a binary tree.   To print content of Array  Import java.util.Arrays; Arrays.toString(ary); Arrays.deepToString(ary);   search without recursive  Node findNode( Node root, int value ){     while( root != null ){         int currval = root.getValue();         if( currval == value ) break;         if( currval &lt; value ){             root = root.getRight();         } else { // currval &gt; value             root = root.getLeft();         }     }   return root; }           preceding lookup operation can be reimplemented recursively as follows: Node findNode( Node root, int value ){   if( root == null ) return null;   int currval = root.getValue();          if( currval == value ) return root;             if( currval &lt; value ){       return findNode( root.getRight(), value );   } else { // currval &gt; value       return findNode( root.getLeft(), value );   }  }            This subtree property is conducive to recursion because recursion generally involves solving a problem in terms of similar subproblems and a base case.       Big O sequencey  1, logn, n, n log n, n2, n3, 2n(2 power n).   Big O     It is also considered poor taste to include constant factors and lower-order terms in the big-Oh notation. For example, it is not fashionable to say that the function 2n2 is O(4n2 + 6n log n), although this is completely correct. We should strive instead to describe the function in the big-Oh in simplest terms.   So, for example, we would say that an algorithm that runs in worst-case time 4n2 + n log n is a quadratic-time algorithm, since it runs in O(n2) time. Likewise, an algorithm running in time at most 5n + 20logn + 4 would be called a linear-time algorithm.   Big Omega     Just as the big-Oh notation provides an asymptotic way of saying that a function is “less than or equal to” another function, the following notations provide an asymptotic way of saying that a function grows at a rate that is “greater than or equal to” that of another.   Example 4.14: 3n log n ? 2n is Ω(n log n).   Big-Theta     In addition, there is a notation that allows us to say that two functions grow at the same rate, up to constant factors. We say that f(n) is Θ(g(n)), pronounced “f(n) is big-Theta of g(n),”   Comparative Analysis     asymptotically[,?simp’t?tik,-k?l] better   Suppose two algorithms solving the same problem are available: an algorithm A, which has a running time of O(n), and an algorithm B, which has a running time of O(n2). Which algorithm is better? We know that n is O(n2), which implies that algorithm A is asymptotically better than algorithm B, although for a small value of n, B may have a lower running time than A.   Some Words of Caution     First, note that the use of the big-Oh and related notations can be somewhat misleading should the constant factors they “hide” be very large. For example, while it is true that the function 10100n is O(n), if this is the running time of an algorithm being compared to one whose running time is 10n log n, we should prefer the O(nlog n)-time algorithm, even though the linear-time algorithm is asymptotically faster. This preference is because the constant factor, 10100, which is called “one googol,” is believed by many astronomers to be an upper bound on the number of atoms in the observable universe. So we are unlikely to ever have a real-world problem that has this number as its input size.   Exponential [,eksp?’nen?(?)l] Running Times     To see how fast the function 2n grows, consider the famous story about the inventor of the game of chess. He asked only that his king pay him 1 grain of rice for the first square on the board, 2 grains for the second, 4 grains for the third, 8 for the fourth, and so on. The number of grains in the 64th square would be 263 = 9, 223, 372, 036, 854, 775, 808, which is about nine billion billions!   If we must draw a line between efficient and inefficient algorithms, therefore, it is natural to make this distinction be that between those algorithms running in polynomial [,p?l?’n??m??l] time and those running in exponential time. That is, make the distinction between algorithms with a running time that is O(nc) (power c based on n), for some constant c &gt; 1, and those with a running time that is O(bn) (power n based on b), for some constant b &gt; 1. Like so many notions we have discussed in this section, this too should be taken with a “grain of salt,” for an algorithm running in O(n100) time should probably not be considered “efficient.” Even so, the distinction between polynomial-time and exponential-time algorithms is considered a robust measure of tractability.   Examples of Algorithm Analysis  constant time operation     All of the primitive operations, originally described on page 154, are assumed to run in constant time; Assume that variable A is an array of n elements. The expression A.length in Java is evaluated in constant time, because arrays are represented internally with an explicit variable that records the length of the array. Another central behavior of arrays is that for any valid index j, the individual element, A[j], can be accessed in constant time. This is because an array uses a consecutive block of memory. The jth element can be found, not by iterating through the array one element at a time, but by validating the index, and using it as an offset from the beginning of the array in determining the appropriate memory address. Therefore, we say that the expression A[j] is evaluated in O(1) time for an array.   Finding the Maximum of an Array  Proposition 4.16: The algorithm, arrayMax, for computing the maximum element of an array of n numbers, runs in O(n) time.   Justification: The initialization at lines 3 and 4 and the return statement at line 8 require only a constant number of primitive operations. Each iteration of the loop also requires only a constant number of primitive operations, and the loop executes n ? 1 times.   Composing Long Strings          Therefore, the overall time taken by this algorithm is proportional to 1 + 2 + ··· + n, which we recognize as the familiar O(n2) summation from Proposition 4.3. Therefore, the total time complexity of the repeat1 algorithm is O(n2).            x = logbn if and only if bx = n. The value b is known as the base of the logarithm. Note that by the above definition, for any base b &gt; 0, we have that logb 1 = 0.       Three-Way Set Disjointness  Origional solution  Suppose we are given three sets, A, B, and C, stored in three different integer arrays. We will assume that no individual set contains duplicate values, but that there may be some numbers that are in two or three of the sets. The three-way set disjointness problem is to determine if the intersection of the three sets is empty, namely, that there is no element x such that x ∈ A, x ∈ B, and x ∈ C.       private static boolean disjoint1(int[]  groupA, int[] groupB, int[] groupC){         for ( int i : groupA) {             for (int j : groupB) {                 for (int k : groupC) {                     if(i==j &amp;&amp; j==k){                         return false;                     }                 }             }         }         return true;     }     This simple algorithm loops through each possible triple of values from the three sets to see if those values are equivalent. If each of the original sets has size n, then the worst-case running time of this method is O(n3) .     private static boolean disjoint2(int[]  groupA, int[] groupB, int[] groupC){       for ( int i : groupA) {           for (int j : groupB) {               if(i==j){                   // add this checking to reduce complexitiy                   for (int k : groupC) {                       if(j==k){                           return false;                       }                   }               }            }       }       return true;   }           In the improved version, it is not simply that we save time if we get lucky. We claim that the worst-case running time for disjoint2 is O(n2).   by sorting  Sorting algorithms will be the focus of Chapter 12. The best sorting algorithms (including those used by Array.sort in Java) guarantee a worst-case running time of O(nlog n). Once the data is sorted, the subsequent loop runs in O(n) time, and so the entire unique2 algorithm runs in O(n log n) time. Exercise C-4.35 explores the use of sorting to solve the three-way set disjointness problem in O(n log n) time.   prefixAverage  Check the source code at PrefixAverage.java, the inital implementation is two for loop, which is O(n2), while the better approach is reuse existing total sum.  // naiive approach, for (int i = 0; i &lt; n; i++) { \t\t\tdouble total=0; \t\t\tfor (int j = 0; j &lt;=i; j++) { // be awre it's &lt;=, instead of \"&lt;\" \t\t\t\ttotal+=x[j];\t\t\t\t \t\t\t} \t\t\ta[i]=total/(i+1);\t\t\t \t\t} // better approach double total=0; \t\tfor (int i = 0; i &lt; n; i++) { \t\t\ttotal += x[i]; \t\t\ta[i]=total/(i+1);\t\t\t \t\t}   Recursive  Definitions     We have one or more base cases, which refer to fixed values of the function. e.g. for n!=1 as  n=1 is base.   Then we have one or more recursive cases, which define the function in terms of itself. for n!, it’s =n*(n-1)! for n&gt;=1            Repetition is achieved through repeated recursive invocations of the method. The process i finite because each time the method is invoked, its argument is smaller by one, and when a base case is reached, no further recursive calls are made.       In the case of computing the factorial function, there is no compelling reason for prefereing recursion over a direct iteration with a loop.           Tree  ADT (Abstract Data Type)     we define a tree ADT using the concept of a position as an abstraction for a node of a tree. An element is stored at each position, and positions satisfy parent-child relationships that define the tree structure.   Depth and Height  Depth     The depth of p is the number of ancestors of p, other than p itself.   The running time of depth(p) for position p is O(dp + 1), where dp denotes the depth of p in the tree, because the algorithm performs a constant-time recursive step for each ancestor of p. Thus, algorithm depth(p) runs in O(n) worst-case time, where n is the total number of positions of T, because a position of T may have depth n - 1 if all nodes form a single branch.   Method depth, as implemented within the AbstractTree class.     public int depth(Position&lt;E&gt; p){   if(isRoot(p))       return 0;   else       return 1+depth(parent(p)); }          Height       We next define the height of a tree to be equal to the maximum of the depths of its positions (or zero, if the tree is empty).   Folloing worst time cost is O(n), it progresses in a top-down fashion.   If the method is initially called on the root of T, it will eventually be called once for each position of T. This is because the root eventually invokes the recursion on each of its children, which in turn invokes the recursion on each of their children, and so on.     public int height(Position&lt;E&gt; p){   int h=0;   for(Position&lt;E&gt; c: children(p))       h=Math.max(h,1+height(c));   return h; }           Binary Tree     A binary tree is an ordered tree with the following properties:            Every node has at most two children.       Each child node is labeled as being either a left child or a right child.       A left child precedes a right child in the order of children of a node.           A binary tree is proper if each node has either zero or two children. Some people also refer to such trees as being full binary trees. Thus, in a proper binary tree, every internal node has exactly two children. A binary tree that is not proper is improper.     Some binary trees      decision tree       An important class of binary trees arises in contexts where we wish to represent a number of different outcomes that can result from answering a series of yes-or-no questions. Each internal node is associated with a question. Starting at the root, we go to the left or right child of the current node, depending on whether the answer to the question is “Yes” or “No.” With each decision, we follow an edge from a parent to a child, eventually tracing a path in the tree from the root to a leaf. Such binary trees are known as decision trees, because a leaf position p in such a tree represents a decision of what to do if the questions associated with p’s ancestors are answered in a way that leads to p. A decision tree is a proper binary tree.     Arithmetic expression       An arithmetic expression can be represented by a binary tree whose leaves are associated with variables or constants, and whose internal nodes are associated with one of the operators +, ?, *, and /, as demonstrated in Figure 8.6. Each node in such a tree has a value associated with it.            If a node is leaf, then its value is that of its variable or constant.       If a node is internal, then its value is defined by applying its operation to the values of its children.         Properties of Binary trees                   level d has at most 2d nodes   Let T be a nonempty binary tree, and let n, nE, nI, and h denote the number of nodes, number of external nodes, number of internal nodes, and height of T, respectively. Then T has the following properties:            h + 1 ≤ n ≤ 2h+1 - 1       1 ≤ nE ≤ 2h       h ≤ nI ≤ 2h - 1       log(n + 1) - 1 ≤ h ≤ n - 1           Also, if T is proper, then T has the following properties:            2h + 1 ≤ n ≤ 2h+1 - 1       h + 1 ≤ nE ≤ 2h       h ≤ nI ≤ 2h - 1       log(n + 1) - 1 ≤ h ≤ (n - 1)/2           In a nonempty proper binary tree T, with nE external nodes and nI internal nodes, we have nE = nI + 1.   Why use tree     You can search, insert/delete items quickly in a tree   Ordered Arrays are bad at Insertions/Deletions   Finding items in a Linkedlist is slow   Time needed to perform an operation on a tree is O(log N)   On average a tree is more efficient if you need to perform many different types of operations.   Code practice  http://www.practice.geeksforgeeks.org/problem-page.php?pid=700159   Geek IDE  http://code.geeksforgeeks.org/index.php   Reference     http://www.geeksforgeeks.org/maximum-width-of-a-binary-tree/  ","categories": [],
        "tags": ["Dev","algorithm"],
        "url": "/2017/01/21/Algorithm.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "IT-Architect",
        "excerpt":"SOA     SOA is a set of design principles for building a suite of interoperable, flexible and reusable services based architecture.   top-down and bottom-up approach SOA patterns   Design Patterns  Singleton     Being single sometimes has its advantages you know. I’m often used to manage pools of resources, like connection or thread pools.   The Singleton Pattern ensures a class has only one instance, and provides a global point of access to it.   WATCH IT! Double-checked locking doesn’t work in Java 1.4 or earlier! Unfortunately, in Java version 1.4 and earlier, many JVMs contain implementations of the volatile keyword that allow improper synchronization for double-checked locking. If you must use a JVM earlier than Java 5, consider other methods of implementing your Singleton.   RAS  Reliability, availability and serviceability (RAS), also known as reliability, availability, and maintainability (RAM), is a computer hardware engineering term involving reliability engineering, high availability, and serviceability design.   Difference between reliability and availability  The distinction between reliability and availability: reliability measures the ability of a system to function correctly, including avoiding data corruption, whereas availability measures how often the system is available for use, even though it may not be functioning correctly. For example, a server may run forever and so have ideal availability, but may be unreliable, with frequent data corruption   Reference  ","categories": [],
        "tags": ["Architect","Questions"],
        "url": "/2017/01/26/IT-Architect.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Network Protocols",
        "excerpt":"Net Protocols      Like most models, this OSI physical layer contains the electrical, mechanical, and functional means to establish physical connections between Layer-2 devices.   In addition to interfacing with the Network Layer, the data link connection can be built upon one or more physical layer interfaces.      ","categories": [],
        "tags": ["TCP","Protocols"],
        "url": "/2017/01/28/Network-Protocols.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Java New IO",
        "excerpt":"Notes     JDK 1.0 introduced rudimentary I/O facilities for accessing the file system (to create a directory, remove a file, or perform another task), accessing file content randomly (as opposed to sequentially), and streaming byte-oriented data between sources and destinations in a sequential manner.  ","categories": [],
        "tags": ["java","IO"],
        "url": "/2017/02/02/Java-New-IO.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Maven-Notes",
        "excerpt":"Maven philosophy     “It is important to note that in the pom.xml file you specify the what and not the how. The pom.xml file can also serve as a documentation tool, conveying your project dependencies and their versions.”   Concepts  Life cycles  Maven defines three Lifecycles default, clean and site and each Lifecycle consists of predefined Phases.   The clean argument which we pass to mvn command is phase which is in lifecycle named clean.   Maven Lifecycle Phases clean lifecycle The clean lifecycle contains three phases pre-clean, clean and post-clean . When we invoke command mvn clean, Maven loads Clean Lifecycle and executes the phases pre-clean and clean. We can invoke any of these three phases and all preceding phases up to and including the invoked phase are executed sequentially.   Lifecycle phases can’s do anything by themselves. For example, phase clean by itself doesnӴ have ability or functionality to delete the build directory. It delegate the task to a plugin named maven-clean-plugin. So, lifecycles phases are just some predefined steps which Maven invokes sequentially one after another. As we can see, phases are similar to the steps in a job which are executed one after another.   Concepts summary  Lifecycles, Lifecycle Phases, Plugins and Plugin Goals are the core of Maven. and we summarize the concepts learned so far: Maven comes with three lifecycles ֠default, clean and site. each lifecycle is made up of lifecycle phases and in all, there are 28 phases ֠default 21, clean 3 and site 4. when a lifecycle phase is invoked using mvn command, all preceding phases are executed sequentially one after another. lifecycle phases by themselves doesnӴ have any capabilities to accomplish some task and they rely on plugins to carryout the task. depending on project and packaging type, Maven binds various plugin goals to lifecycle phases and goals carryout the task entrusted to them.   default lifecycle  Default Lifecycle The most important of the three lifecycles is the Default Lifecycle . Maven uses default lifecycle to build, test and distribute the project. Default lifecycle contains 21 phases.   Project may contain resources such as properties, XML configuration files etc., and phases process-resources and process-test-resources copy and process such resources files.  The phases compile and test-compile complies the source Java files and test files respectively. The phases package, install, and deploy are used to distribute the project. As we have already seen, the package phase creates JAR file of resources and compiled classes for distribution. The phase install, installs the artifacts of the project i.e jar and pom.xml to the local repository at $HOME/.m2 so that other projects can use them as dependencies. The phase deploy installs the artifacts of the project to a remote repository (probably on Internet) so that a wider group of projects can use it as dependency. We will cover these phases in a later chapter.   goal  If we see the usage description of mvn command, apart from the options it accepts only two things ֠goal or phase. Maven Lifecycle Phases - mvn usage description   mvn [options] [&lt;goal(s)&gt;] [&lt;phase(s)&gt;]   For example, we can directly compile the source with the following command. $ cd simple-app $ mvn compiler:compile To run a goal with mvn, use the format :. In the above example, compiler:compile, the compiler is plugin prefix of maven-compiler-plugin and compile is the goal name. We can get the prefix of all plugins from Maven Plugin Directory.   – When we invoke a goal directly, Maven executes just that goal, whereas when we invoke a lifecycle phase all the phases up to that phase are executed. We can see this in action with following example. $ cd simple-app $ mvn clean $ mvn surefire:test    In very few situations we invoke plugin goals directly and more often than not, lifecycle phases are preferred.   Lifecycle Phases and Plugin Goals When a lifecycle phase is run, depending on project type and packaging type, Maven binds plugin goals to lifecycle phases. When we run mvn package in a Java Project.    To process-resources phase, Maven binds resources goal of maven-resources-plugin and to test phase, it binds test goal of maven-surefire-plugin and so on. Whatӳ happens at package phase is bit interesting. In a Java Project, Maven binds jar goal of maven-jar-plugin. However, when we run the same command in a webapp project, up to test phase Maven binds same goals, but to the package phase Maven binds war goal of maven-war-plugin the war:war instead of jar:jar.   Samples   For example, consider the command below. The clean and package arguments are build phases, while the dependency:copy-dependencies is a goal (of a plugin).  mvn clean dependency:copy-dependencies package  If this were to be executed, the clean phase will be executed first (meaning it will run all preceding phases of the clean lifecycle, plus the clean phase itself), and then the dependency:copy-dependencies goal, before finally executing the package phase (and all its preceding build phases of the default lifecycle).   Moreover, if a goal is bound to one or more build phases, that goal will be called in all those phases.   To see what goals bined to lifecycle phase  mvn help:describe -Dcmd=install  [INFO] ‘install’ is a phase corresponding to this plugin: org.apache.maven.plugins:maven-install-plugin:2.4:install   It is a part of the lifecycle for the POM packaging ‘pom’. This lifecycle includes the following phases:     validate: Not defined   initialize: Not defined   generate-sources: Not defined   process-sources: Not defined   generate-resources: Not defined   process-resources: Not defined   compile: Not defined   process-classes: Not defined   generate-test-sources: Not defined   process-test-sources: Not defined   generate-test-resources: Not defined   process-test-resources: Not defined   test-compile: Not defined   process-test-classes: Not defined   test: Not defined   prepare-package: Not defined   package: Not defined   pre-integration-test: Not defined   integration-test: Not defined   post-integration-test: Not defined   verify: Not defined   install: org.apache.maven.plugins:maven-install-plugin:2.4:install   deploy: org.apache.maven.plugins:maven-deploy-plugin:2.7:deploy   Plugin  Maven is ֠at its heart ֠a plugin execution framework; all work is done by plugins.   Plugins are broadly grouped as build plugins and reporting plugins.   Plugins are artifacts that provide goals to Maven. Furthermore, a plugin may have one or more goals wherein each goal represents a capability of that plugin. For example, the Compiler plugin has two goals: compile and testCompile. The former compiles the source code of your main code, while the latter compiles the source code of your test code.   Plugin Goals Maven plugin is a collection of one or more goals which do some task or job. It is also known as Mojo ֠Maven Plain Old Java Object.   Similarly, Maven uses maven-compiler-plugin to compile the source and test files and it provides three goals ֠compiler:compile, compiler:testCompile and compiler:help.   Suffice it to say for now that a plugin is a collection of goals with a general common purpose. For example the jboss-maven-plugin, whose purpose is “deal with various jboss items”.   To configure plugins, we use project build element in pom.xml. The next listing shows the top level elements used to configure a plugin. pom.xml  &lt;project&gt; ...   &lt;build&gt;     ...     &lt;plugins&gt;       &lt;plugin&gt;         &lt;groupId&gt;...&lt;/groupId&gt;         &lt;artifactId&gt;...&lt;/artifactId&gt;         &lt;version&gt;...&lt;/version&gt;         &lt;configuration&gt;...&lt;/configuration&gt;                 &lt;executions&gt;            &lt;execution&gt;...&lt;/executions&gt;         &lt;/executions&gt;       &lt;/plugin&gt;     &lt;/plugins&gt;   &lt;/build&gt; &lt;/project&gt;   The elements are     build : defines the project build.   plugins : parent element for one or more  elements.   plugin : the plugin to configure.   groupId, artifactId, version : coordinates of the plugin to configure.   configuration : holds the parameters and properties to be passed to the plugin.   executions : parent element for one or more  element.   execution : configures the execution of a goal of the plugin.   In Maven, there are the build and the reporting plugins:      Build plugins will be executed during the build and then, they should be configured in the  element.   Reporting plugins will be executed during the site generation and they should be configured in the  element.   position  Normally, the  block is placed after the project coordinates block and before the dependencies block.   To show plugin details in configuration  Easiest way to know the available parameters for a goal is to run plugin help goal. $ mvn compiler:help -Dgoal=compile -Ddetail It will list the available parameters for compile goal of compiler. But it will not show the default value of the parameters and to know the available parameters and also, the default value for each parameter, run help:describe goal of Maven Help plugin (maven-help-plugin). $ mvn help:describe -Dplugin=compiler -Dmojo=compile -Ddetail Note that maven-help-plugins uses -Dmojo for goal, instead of -Dgoal,   Help Goal  Recent Maven plugins have generally an help goal to have in the command line the description of the plugin, with their parameters and types. For instance, to understand the javadoc goal, you need to call:  mvn javadoc:help -Ddetail -Dgoal=javadoc   Samples     To include or exclude certain files or directories in the project Jar, configure Maven Jar Plugin with  and  parameters. simple-app/pom.xml ...                    org.apache.maven.plugins       maven-jar-plugin                           **/service/*                              …       Maven Clean Plugin deletes the target directory by default and we may configure it to delete additional directories and files. simple-app/pom.xml …               maven-clean-plugin                                       src/main/generated           false           true                        *.java                                   Template*                                &lt;/configuration&gt;   &lt;/plugin&gt;  &lt;/plugins&gt; &lt;/build&gt; ... The above configuration forces the clean plugin to delete *.java files in src/main/generated directory, but excludes the Template*.       plugin execution  Firstly demonstrate plugin execution with an example. In Apache Ant, itӳ quite easy to output echo any property or message during the build and many of us frequently use it to understand the build flow or to debug. But, Maven comes with no such feature, and only way to echo any message is to use Ant within Maven. The Apache AntRun Plugin provides the ability to run Ant tasks within Maven. Letӳ configure maven-antrun-plugin to output message to console. simple-app/pom.xml   …                       maven-antrun-plugin                                                run                          compile                                             Build Dir: ${project.build.directory}                                                                … Build the project with  mvn package and it echoes the build directory name in compile phase.   sample  The clean and package arguments are build phases, while the dependency:copy-dependencies is a goal (of a plugin).   mvn clean dependency:copy-dependencies package If this were to be executed, the clean phase will be executed first (meaning it will run all preceding phases of the clean lifecycle, plus the clean phase itself), and then the dependency:copy-dependencies goal, before finally executing the package phase (and all its preceding build phases of the default lifecycle).   concept  The element / allows you to configure the execution of a plugin goal. With it, you can accomplish the following things. bind a plugin goal to a lifecycle phase. configure plugin parameters of a specific goal. configure plugin parameters of a specific goal such as compiler:compile, surefire:test etc., that are by default binds to a lifecycle phase         core concepts  phase  you may notice the second is simply a single word - package. Rather than a goal, this is a phase. A phase is a step in the build lifecycle, which is an ordered sequence of phases. When a phase is given, Maven will execute every phase in the sequence up to and including the one defined. For example, if we execute the compile phase, the phases that actually get executed are:      validate   generate-sources   process-sources   generate-resources   process-resources   compile   Maven Phases Although hardly a comprehensive list, these are the most common default lifecycle phases executed.      validate: validate the project is correct and all necessary information is available   compile: compile the source code of the project   test: test the compiled source code using a suitable unit testing framework. These tests should not require the code be packaged or deployed   package: take the compiled code and package it in its distributable format, such as a JAR.   integration-test: process and deploy the package if necessary into an environment where integration tests can be run   verify: run any checks to verify the package is valid and meets quality criteria   install: install the package into the local repository, for use as a dependency in other projects locally        deploy: done in an integration or release environment, copies the final package to the remote repository for sharing with other developers and projects. There are two other Maven lifecycles of note beyond the default list above. They are       clean: cleans up artifacts created by prior builds   site: generates site documentation for this project Phases are actually mapped to underlying goals. The specific goals executed per phase is dependant upon the packaging type of the project. For example, package executes jar:jar if the project type is a JAR, and war:war if the project type is - you guessed it - a WAR.   An interesting thing to note is that phases and goals may be executed in sequence.   mvn clean dependency:copy-dependencies package This command will clean the project, copy dependencies, and package the project (executing all phases up to package, of course).   – Lifecycle and phases are just formal names for jobs and steps. Maven calls the jobs as lifecycles and tasks (or steps) as phases. — when a phase is invoked using mvn command, all preceding phases up to and including the invoked phase are executed sequentially. For example, mvn compile ֠will run phases process-resources and then compile. mvn test ֠will run phases process-resources, compile, process-test-resources, test-compile and finally test. mvn install ֠will run phases process-resources, compile, process-test-resources, test-compile, test and finally install. Like in clean lifecycle, in default lifecycle too, lifecycle phases by themselves donӴ have capabilities to accomplish some task. For example, compile phase by itself canӴ do anything but, it delegates compilation job to a plugin named maven-compiler-plugin.   Some Phases Are Not Usually Called From the Command Line  The phases named with hyphenated-words (pre-, post-, or process-*) are not usually directly called from the command line. These phases sequence the build, producing intermediate results that are not useful outside the build. In the case of invoking integration-test, the environment may be left in a hanging state.   Maven default repository   http://repo.maven.apache.org/maven2/   Maven Settings  The settings element in the settings.xml file contains elements used to define values which configure Maven execution in various ways, like the pom.xml, but should not be bundled to any specific project, or distributed to an audience. These include values such as the local repository location, alternate remote repository servers, and authentication information.   There are two locations where a settings.xml file may live:      The Maven install: ${maven.home}/conf/settings.xml   A user’s install: ${user.home}/.m2/settings.xml The former settings.xml are also called global settings, the latter settings.xml are referred to as user settings. If both files exists, their contents gets merged, with the user-specific settings.xml being dominant.   Tip: If you need to create user-specific settings from scratch, it’s easiest to copy the global settings from your Maven installation to your ${user.home}/.m2 directory. Maven’s default settings.xml is a template with comments and examples so you can quickly tweak it to match your needs.   The contents of the settings.xml can be interpolated using the following expressions:   ${user.home} and all other system properties (since Maven 3.0) ${env.HOME} etc. for environment variables Note that properties defined in profiles within the settings.xml cannot be used for interpolation.   All Maven pom.xml inherits from super POM   Coc: Convention over Configuration     Convention over configuration is a simple concept. Systems, libraries, and frameworks should assume reasonable defaults without requiring that unnecessary configuration systems should “just work.” Popular frameworks such as Ruby on Rails and EJB3 have started to adhere to these principles in reaction to the configuration complexity of frameworks such as the initial Enterprise JavaBeans? (EJB) specifications.   “Popularized by the Ruby on Rails community, CoC emphasizes sensible defaults, thereby reducing the number of decisions to be made.”   “Gradle’s flexibility, like that of Ant, can be abused, which results in difficult and complex builds. ”   Notes     This is the Project Object Model (POM), a declarative description of a project.   Coordinate     “group, artifact, and version (GAV) coordinates”   We’ve highlighted the Maven coordinates for this project: groupId, artifactId, version and packaging. These combined identifiers make up a project’s coordinates.[3] Just as in any other coordinate system, a Maven coordinate is an address for a specific point in “space”: from general to specific. Maven pinpoints a project via its coordinates when one project relates to another, either as a dependency, a plugin, or a parent project reference.   Maven coordinates are often written using a colon as a delimiter in the following format: groupId:artifactId:packaging:version.   Projects undergoing active development can use a special identifier that marks a version as a SNAPSHOT.   The packaging format of a project is also an important component in the Maven coordinates, but it isn’t a part of a project’s unique identifiers. A project’s groupId:artifactId:version make that project unique; you can’t have a project with the same three groupId, artifactId, and version identifiers.   packaging: The type of project, defaulting to jar, describing the packaged output produced by a project. A project with packaging jar produces a JAR archive; a project with packaging war produces a web application.   The core of Maven is pretty dumb; it doesn’t know how to do much beyond parsing a few XML documents and keeping track of a lifecycle and a few plugins. Maven has been designed to delegate most responsibility to a set of Maven plugins that can affect the Maven lifecycle and offer access to goals. Most of the action in Maven happens in plugin goals that take care of things like compiling source, packaging bytecode, publishing sites, and any other task that needs to happen in a build.   You benefit from the fact that plugins are downloaded from a remote repository and maintained centrally. This is what is meant by universal reuse through Maven plugins.   working developers are starting to realize that Maven not only simplifies the task of build management, it is helping to encourage a common interface between developers and software projects.   Maven vs Ant  The differences between Ant and Maven in this example are:  Apache Ant  Ant doesn’t have formal conventions such as a common project directory structure; you have to tell Ant exactly where to find the source and where to put the output. Informal conventions have emerged over time, but they haven’t been codified into the product.     Ant is procedural; you have to tell Ant exactly what to do and when to do it. You have to tell it to compile, then copy, then compress.   Ant doesn’t have a lifecycle; you have to define goals and goal dependencies. You have to attach a sequence of tasks to each goal manually.     Apache Maven       Maven has conventions: in the example, it already knew where your source code was because you followed the convention. It put the bytecode in target/classes, and it produced a JAR file in target.   Maven is declarative; all you had to do was create a pom.xml file and put your source in the default directory. Maven took care of the rest.   Maven has a lifecycle, which you invoked when you executed mvn install. This command told Maven to execute a series of sequence steps until it reached the lifecycle. As a side effect of this journey through the lifecycle, Maven executed a number of default plugin goals that did things such as compile and create a JAR.   A Maven plugin is a collection of one or more goals (see Figure 3-1). Examples of Maven plugins can be simple core plugins such as the Jar plugin that contains goals for creating JAR files, the Compiler plugin that contains goals for compiling source code and unit tests, or the Surefire plugin that contains goals for executing unit tests and generating reports.   Maven life cycle     The second command we ran in the previous section was mvn install. This command didn’t specify a plugin goal; instead, it specified a Maven lifecycle phase. A phase is a step in what Maven calls the “build lifecycle.” The build lifecycle is an ordered sequence of phases involved in building a project.   Plugin goals can be attached to a lifecycle phase. As Maven moves through the phases in a lifecycle, it will execute the goals attached to each particular phase. Each phase may have zero or more goals bound to it. In the previous section, when you ran mvn install, you might have noticed that more than one goal was executed. Examine the output after running mvn install and take note of the various goals that are executed.   Maven steps through the phases preceding package in the Maven lifecycle; executing a phase will first execute all proceeding phases in order, ending with the phase specified on the command line. Each phase corresponds to zero or more goals, and since we haven’t performed any plugin configuration or customization, this example binds a set of standard plugin goals to the default lifecycle.   Dependency     “Maven provides declarative dependency management. With this approach, you declare your project’s dependencies in an external file called pom.xml. Maven will automatically download those dependencies and hand them over to your project for the purpose of building, testing, or packaging.”   “The default remote repository with which Maven interacts is called Maven Central, and it is located at repo.maven.apache.org and uk.maven.org.”   “The internal repository manager acts as a proxy to remote repositories. Because you have full control over the internal repository, you can regulate the types of artifacts allowed in your company. Additionally, you can also push your organization’s artifacts onto the server, thereby enabling collaboration. ”, such as Nexus   Transitive Dependcies     “A key benefit of Maven is that it automatically “deals with transitive dependencies and includes them in your project.   “Maven uses a technique known as dependency mediation to resolve version conflicts. Simply stated, dependency mediation allows Maven to pull the dependency that is closest to the project in the tree. In Figure 3-3, there are two versions of dependency B: 0.0.8 and 1.0.0. In this scenario, version 0.0.8 of dependency B is included in the project, because it is a direct dependency and closest to the tree. Now look at the three versions of dependency F: 0.1.3, 1.0.0, and 2.2.0. All three dependencies are at the same depth. In this scenario, Maven will use the first-found dependency, which would be 0.1.3, and not the latest 2.2.0 version.”   Dependency Scope     “Maven uses the concept of scope, which allows you to specify when and where you need a particular dependency.” “Maven provides the following six scopes:”   “compile: Dependencies with the compile scope are available in the class path in all phases on a project build, test, and run. This is the default scope.”   “provided: Dependencies with the provided scope are available in the class path during the build and test phases. They don’t get bundled within the generated artifact. Examples of dependencies that use this scope include Servlet api, JSP api, and so on.”   “runtime: Dependencies with the runtime scope are not available in the class path during the build phase. Instead they get bundled in the generated artifact and are available during runtime.”   “test: Dependencies with the test scope are available during the test phase. JUnit and TestNG are good examples of dependencies with the test scope.”   “system: Dependencies with the system scope are similar to dependencies with the provided scope, except that these dependencies are not retrieved from the repository. Instead, a hard-coded path to the file system is specified from which the dependencies are used.”   “import: The import scope is applicable for .pom file dependencies only. It allows you to include dependency management information from a remote .pom file. The import scope is available only in Maven 2.0.9 or later.”   Tips  runtime arguments  batch mode -B  -B,–batch-mode                        Run in non-interactive (batch)                                         mode (disables output color)   Common errors  Not authorized , ReasonPhrase:Unauthorized  This is likely you are behind corporate proxy, so you have to config to make Maven can connect to nexus successfully:     Go to make sure your settings.xml is correct, normally, it’s under your home directory, e.g. tzhang/.m2/settings.xml to make sure the username and password same as the settings in nexus ```xml                my-nexus      1jhAX6LZ V7MxFm3k9fiodhqNQDT/kSh8V81JT8bUWRHEel339rwq \t&lt;/server&gt;\t &lt;mirrors&gt;     &lt;mirror&gt;         &lt;id&gt;my-nexus&lt;/id&gt;         &lt;url&gt;https://nexus.internal.abc.com/content/groups/public&lt;/url&gt;         &lt;mirrorOf&gt;*,!sonar,!eclipse-misc,!m2-proxy,!eclipse-releases,!eclipse-snapshots,!mfs-m2-repository,!fsg_internal_repository,!fsg_snapshot_repository&lt;/mirrorOf&gt;     &lt;/mirror&gt; &lt;/mirrors&gt; ``` - double check your login token in nexus, log into nexus, click your login name in top right dropdown, chose \"profile\" and then trigger button \"Access User Token\" ## Newly added dependency in pom not working, class not found If you are in intellij, please go to the exact module (the child module, rather than parent pom), right click the pom.xml, chose maven -&gt; Reinstall # Setup Proxy - “Maven requires an Internet connection to download plug-ins and dependencies. Some companies employ HTTP proxies to restrict access to the Internet. In those scenarios, running Maven will result in Unable to download artifact errors. To address this, edit the settings.xml file and add the proxy information specific to your company.” ```xml               companyProxy       true       http       proxy.company.com       8080       proxyusername       proxypassword                    # Multimodule projects ## Generate parent ```sh mvn archetype:generate -DgroupId=com.apress.gswmbook -DartifactId=gswm-parent -Dversion=1.0.0-SNAPSHOT -DarchetypeGroupId=org.codehaus.mojo.archetypes -DarchetypeArtifactId=pom-root  Listing 6-5. Parent pom.xml File with Modules    &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt;   &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;   &lt;groupId&gt;com.apress.gswmbook&lt;/groupId&gt;   &lt;artifactId&gt;gswm-parent&lt;/artifactId&gt;   &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;   &lt;packaging&gt;pom&lt;/packaging&gt;   &lt;name&gt;gswm-parent&lt;/name&gt;   &lt;modules&gt;     &lt;module&gt;gswm-web&lt;/module&gt;     &lt;module&gt;gswm-service&lt;/module&gt;     &lt;module&gt;gswm-repository&lt;/module&gt;   &lt;/modules&gt; &lt;/project&gt;  Generate web module  mvn archetype:generate -DgroupId=com.todzhang.mywebApp -DartifactId=main-web -Dversion=1.0.0-SNAPSHOT -Dpackage=war -DarchetypeArtifactId=maven-archetype-webapp  &lt;?xml version=\"1.0\"?&gt; &lt;project xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\" xmlns=\"http://maven.apache.org/POM/4.0.0\"     xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"&gt;   &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;   &lt;parent&gt;     &lt;groupId&gt;com.apress.gswmbook&lt;/groupId&gt;     &lt;artifactId&gt;gswm-parent&lt;/artifactId&gt;     &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;   &lt;/parent&gt;   &lt;groupId&gt;com.apress.gswmbook&lt;/groupId&gt;   &lt;artifactId&gt;gswm-web&lt;/artifactId&gt;   &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt;   &lt;packaging&gt;war&lt;/packaging&gt;   &lt;name&gt;gswm-web Maven Webapp&lt;/name&gt;   &lt;url&gt;http://maven.apache.org&lt;/url&gt;   &lt;dependencies&gt;     &lt;dependency&gt;       &lt;groupId&gt;junit&lt;/groupId&gt;       &lt;artifactId&gt;junit&lt;/artifactId&gt;       &lt;version&gt;3.8.1&lt;/version&gt;       &lt;scope&gt;test&lt;/scope&gt;     &lt;/dependency&gt;   &lt;/dependencies&gt;   &lt;build&gt;     &lt;finalName&gt;gswm-web&lt;/finalName&gt;   &lt;/build&gt; &lt;/project&gt;  Generate a service jar module  mvn archetype:generate -DgroupId=com.todzhang.mywebApp -DartifactId=back-service -Dversion=1.0.0-SNAPSHOT -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false     Notice that you didn’t provide the package parameter, as the maven-archetype-quickstart produces a JAR project by default.     To start the module      mvn packages          Create archetype from project      mvn archetype:create-from-project          Site life cycle      mvn site           The site life cycle uses Maven’s site plug-in to generate the site for a single project. Once this command completes, a site folder gets created under the project’s target.   Open the index.html file to browse the generated site. You will notice that Maven used the information provided in the pom.xml file to generate most of the documentation. It also automatically applied the default skin and generated the corresponding images and CSS files.   To regenerate site     mvn clean site           JavaDoc     Maven provides a Javadoc plug-in, which uses the Javadoc tool for generating Javadocs. Integrating the Javadoc plug-in simply involves declaring it in the reporting element of pom.xml file, as shown in Listing 7-4. Plug-ins declared in the pom reporting element are executed during site generation. ```xml           &lt;!—Content removed for brevity--&gt;                       org.apache.maven.plugins         maven-javadoc-plugin         2.10.1                 Then run **mvn clean site** to generate javadoc,  the apidocs folder created under gswm /target/site # Unit test report - Maven offers the Surefire plug-in that provides a uniform interface for running tests created by frameworks such as JUnit or TestNG. It also generates execution results in various formats such as XML and HTML. These published results enable developers to find and fix broken tests quickly. The Surefire plug-in is configured in the same way as the Javadoc plug-in in the reporting section of the pom file. Listing 7-5 shows the Surefire plug-in configuration. ```xml &lt;project&gt;         &lt;!—Content removed for brevity--&gt;   &lt;reporting&gt;     &lt;plugins&gt;       &lt;plugin&gt;         &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;         &lt;artifactId&gt;maven-surefire-report-plugin&lt;/artifactId&gt;         &lt;version&gt;2.17&lt;/version&gt;       &lt;/plugin&gt;     &lt;/plugins&gt;   &lt;/reporting&gt; &lt;/project&gt;     you will see a Surefire Reports folder generated under gswm\\target. It contains the test execution results in XML and TXT formats. The same information will be available in HTML format in the surefire-report.html file under site folder.   Code coverate report     Code coverage is a measurement of how much source code is being exercised by automated tests. Essentially, it provides an indication of the quality of your tests. Emma and Cobertura are two popular open source code coverage tools for Java. In this section, you will use Cobertura for measuring this project’s code coverage. Configuring Cobertura is similar to other plug-ins, as shown in ```xml           &lt;!—Content removed for brevity--&gt;                        org.codehaus.mojo         cobertura-maven-plugin         2.6                # Find bug reports - FindBugs is a tool for detecting defects in Java code. It uses static analysis to detect bug patterns, such as infinite recursive loops and null pointer dereferences. Listing 7-7 shows the FindBugs configuration.  ```xml &lt;project&gt;         &lt;!—Content removed for brevity--&gt;   &lt;reporting&gt;     &lt;plugins&gt;       &lt;plugin&gt;         &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;         &lt;artifactId&gt;findbugs-maven-plugin&lt;/artifactId&gt;         &lt;version&gt;3.0.0&lt;/version&gt;       &lt;/plugin&gt;    &lt;/plugins&gt;  &lt;/reporting&gt; &lt;/project&gt;   Integration with Nexus     Repository managers act as a proxy of public repositories, facilitate artifact sharing and team collaboration, ensure build stability, and enable the governance of artifacts used in the enterprise.   Nexus is a popular open source repository manager from Sonatype. It is a web application that allows you to maintain internal repositories and access external repositories. It allows repositories to be grouped and accessed via a single URL. This enables the repository administrator to add and remove new repositories behind the scenes without requiring developers to change the configuration on their computers. Additionally, it provides hosting capabilities for sites generated using Maven site and artifact search capabilities.   To enable nexus in Maven     You will start by adding a distributionManagement element in the pom.xml file, as shown in Listing 8-1. This element is used to declare the location where the project’s artifacts will be when deployed. The repository element indicates the location where released artifacts will be deployed. Similarly, the snapshotRepository element identifies the location where the SNAPSHOT versions of the project will be stored. ```xml  &lt;project xmlns=”http://maven.apache.org/POM/4.0.0” xmlns:xsi=http://www.w3.org/2001/XMLSchema-instance” xsi:schemaLocation=”http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd”&gt;  4.0.0       &lt;!-- Content removed for brevity --&gt;     &lt;distributionManagement&gt;        &lt;repository&gt;             &lt;id&gt;nexusReleases&lt;/id&gt;           &lt;name&gt;Releases&lt;/name&gt;     &lt;url&gt;http://localhost:8081/nexus/content/repositories/releases&lt;/url&gt;        &lt;/repository&gt;        &lt;snapshotRepository&gt;           &lt;id&gt;nexusSnapshots&lt;/id&gt;           &lt;name&gt;Snapshots&lt;/name&gt;           &lt;url&gt;http://localhost:8081/nexus/content/repositories/snapshots&lt;/url&gt;        &lt;/snapshotRepository&gt;     &lt;/distributionManagement&gt; &lt;!-- Content removed for brevity --&gt;  &lt;/project&gt;  - Out of the box, Nexus comes with **Releases and Snapshots repositories**. By default, SNAPSHOT artifacts will be stored in the Snapshots Repository, and release artifacts will be stored in the Releases repository. Like most repository managers, deployment to Nexus is a protected operation. You provide the credentials needed to interact with Nexus in the **settings.xml** file. - Listing 8-2. Settings.xml File with Server Information - Listing 8-2 shows the settings.xml file with the server information. The Nexus deployment user with password deployment123 is provided out of the box. Notice that the IDs declared in the server tag — nexusReleases and nexusSnapshots must match the IDs of the repository and snapshotRepository declared in the pom.xml file. Replace the contents of the settings.xml file in the C:\\Users\\&lt;&lt;USER_NAME&gt;&gt;\\.m2 folder with the code in Listing 8-2. ```xml &lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt; &lt;settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\"&gt; &lt;servers&gt;    &lt;server&gt;       &lt;id&gt;nexusReleases&lt;/id&gt;       &lt;username&gt;deployment&lt;/username&gt;       &lt;password&gt;deployment123&lt;/password&gt;    &lt;/server&gt;    &lt;server&gt;       &lt;id&gt;nexusSnapshots&lt;/id&gt;       &lt;username&gt;deployment&lt;/username&gt;       &lt;password&gt;deployment123&lt;/password&gt;    &lt;/server&gt; &lt;/servers&gt; &lt;/settings&gt;     This concludes the configuration steps for interacting with Nexus. At the command line, run the command mvn deploy under the directory C:\\apress\\gswm-book\\chapter8\\gswm. Upon successful execution of the command, you will see the SNAPSHOT artifact under Nexus   POM   Effective POM  At the start of every build, Maven internally merges project pomx.ml with Super POM and constructs a new POM which is known as Effective POM .   Earlier in Maven Lifecycle and Plugin Goals, we learned that Maven binds plugins goals to lifecycle phases. Actually, this magic happens in effective POM and it is highly instructive to go through the effective POM to know the what goes on under the hood.   To dump effective POM  use Maven Help Plugin to dump the effective POM to a file for investigation.  $ mvn help:effective-pom -Doutput=target/effective-pom.xml   Packaging  The first, and most common way, is to set the packaging for your project via the equally named POM element . Some of the valid packaging values are jar, war, ear and pom. If no packaging value has been specified, it will default to jar.   Each packaging contains a list of goals to bind to a particular phase. For example, the jar packaging will bind the following goals to build phases of the default lifecycle.  This is an almost standard set of bindings; however, some packagings handle them differently. For example, a project that is purely metadata (packaging value is pom) only binds goals to the install and deploy phases (for a complete list of goal-to-build-phase bindings of some of the packaging types, refer to the Lifecycle Reference).   build tools options  Maven and Ant are just different approaches: imperative and declarative (see Imperative vs Declarative build systems)   Maven is better for managing dependencies (but Ant is ok with them too, if you use Ant+Ivy) and build artefacts. The main benefit from maven - its lifecycle. You can just add specific actions on correct phase, which seems pretty logical: just launch you integration tests on integration-test phase for example. Also, there are many existing plugins, which can could almost everything. Maven archetype is powerful feature, which allows you to quickly create project.   Ant is better for controlling of build process. Before your very first build you have to write you build.xml. If your build process is very specific, you have to create complicated scripts. For long-term projects support of ant-scripts could become really painful: scripts become too complicated, people, who has written them, could leave project, etc.   Both of them use xml, which could become too big in big long-term projects.   Anyway, you shoud read specific documentation (not hate-articles) on both. Also, there is ant-maven-plugin, which allow to launch ant-scripts with maven.   P.S. You can take a look on Gradle, which for me could provide more freedom than Maven, but is easier to use than Ant.   ","categories": [],
        "tags": ["java","maven","mvn"],
        "url": "/2017/02/03/Maven-Notes.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Java JVM",
        "excerpt":"Class loading subsystem   Consist of three sections   load  There are three class loaders     bootstrap class loader, e.g. rt.jar   extension class loader, e.g. jre/lib/ext   application class loader, e.g. -cp     Link       Verify   Prepare   Resolve     Initialize       While deadlock is the most widely encountered liveness hazard, there are sev- eral other liveness hazards you may encounter in concurrent programs including starvation, missed signals, and livelock.   Metaspace  Since Java 8, the introduce of metaspace is kind of using memory or even virtual memory in OS, so theriotically there is no limit of metaspace. But PermGen is part of Method area, so that is upper limit for PermGen.   ","categories": [],
        "tags": ["java","JVM"],
        "url": "/2017/02/05/JVM.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Eclipse notes",
        "excerpt":"How do I remove a plug-in?  Run Help &gt; About Eclipse &gt; Installation Details, select the software you no longer want and click Uninstall. (On Macintosh it is Eclipse &gt; About Eclipse &gt; Installation Details.)   Where is the Eclipse Plugin update error log?  The log is located at current workspace: {workspace_path}/.metadata/.log - also you can view this log in view “Error Log”: Window &gt; Show View &gt; Other &gt; Find here “Error Log”  ","categories": [],
        "tags": ["Eclipse","Java"],
        "url": "/2017/02/06/Eclipse-tips.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "SSH and Cryptography",
        "excerpt":"SFTP versus FTPS     SS: Secure Shell   An increasing number of our customers are looking to move away from standard FTP for transferring data, so we are often asked which secure FTP protocol we recommend. In the next few paragraphs, we will explain what options are available and their main differences.   The two mainstream protocols available for Secure FTP transfers are named SFTP (FTP over SSH) and FTPS (FTP over SSL). Both SFTP and FTPS offer a high level of protection since they implement strong algorithms such as AES and Triple DES to encrypt any data transferred. Both options also support a wide variety of functionality with a broad command set for transferring and working with files. So the most notable differences between SFTP and FTPS is how connections are authenticated and managed.   Authentication: SFTP vs. FTPS     With SFTP (FTP over SSH), a connection can be authenticated using a couple different techniques.  For basic authentication, you (or your trading partner) may just require a user id and password to connect to the SFTP server. Its important to note that any user ids and passwords supplied over the SFTP connection will be encrypted, which is a big advantage over standard FTP.   SSH keys can also be used to authenticate SFTP connections in addition to, or instead of, passwords. With key-based authentication, you will first need to generate a SSH private key and public key beforehand. If you need to connect to a trading partner’s SFTP server, you would send your SSH public key to them, which they will load onto their server and associate with your account. When you connect to their SFTP server, your client software will transmit your public key to the server for authentication. If the keys match, along with any user/password supplied, then the authentication will succeed.   With FTPS (FTP over SSL), a connection is authenticated using a user id, password and certificate(s).  Like SFTP, the users and passwords for FTPS connections will also be encrypted. When connecting to a trading partner’s FTPS server, your FTPS client will first check if the server’s certificate is trusted. The certificate is considered trusted if either the certificate was signed off by a known certificate authority (CA), like Verisign, or if the certificate was self-signed (by your partner) and you have a copy of their public certificate in your trusted key store.   Your partner may also require that you supply a certificate when you connect to them.  Your certificate may be signed off by a 3rd party CA or your partner may allow you to just self-sign your certificate, as long as you send them the public portion of your certificate beforehand (which they will load in their trusted key store).   Implementation: SFTP vs. FTPS     In regards to how easy each of the secure FTP protocols are to implement, SFTP is the clear winner since it is very firewall friendly. SFTP only needs a single port number (default of 22) to be opened through the firewall.  This port will be used for all SFTP communications, including the initial authentication, any commands issued, as well as any data transferred.   On the other hand, FTPS can be very difficult to patch through a tightly secured firewall since FTPS uses multiple port numbers. The initial port number (default of 21) is used for authentication and passing any commands.  However, every time a file transfer request (get, put) or directory listing request is made, another port number needs to be opened.  You and your trading partners will therefore have to open a range of ports in your firewalls to allow for FTPS connections, which can be a security risk for your network.   In summary, SFTP and FTPS are both very secure with strong authentication options.  However since SFTP is much easier to port through firewalls, and we are seeing an increasing percentage of trading partners adopting SFTP, we believe SFTP is the clear winner for your secure FTP needs.   SSH     There are several ways to use SSH; one is to use automatically generated public-private key pairs to simply encrypt a network connection, and then use password authentication to log on.   Another is to use a manually generated public-private key pair to perform the authentication, allowing users or programs to log in without having to specify a password. In this scenario, anyone can produce a matching pair of different keys (public and private). The public key is placed on all computers that must allow access to the owner of the matching private key (the owner keeps the private key secret). While authentication is based on the private key, the key itself is never transferred through the network during authentication. SSH only verifies whether the same person offering the public key also owns the matching private key. In all versions of SSH it is important to verify unknown public keys, i.e. associate the public keys with identities, before accepting them as valid. Accepting an attacker’s public key without validation will authorize an unauthorized attacker as a valid user.   SSH is important in cloud computing to solve connectivity problems, avoiding the security issues of exposing a cloud-based virtual machine directly on the Internet. An SSH tunnel can provide a secure path over the Internet, through a firewall to a virtual machine.   The standard TCP port 22 has been assigned for contacting SSH servers.   Key management     On Unix-like systems, the list of authorized public keys is typically stored in the home directory of the user that is allowed to log in remotely, in the file ~/.ssh/authorized_keys.   WinFTP     cryptographic protocol is SSH-2   SSH implementation is OpenSSH_5.3   Server fingerprint: File transfer protocol = SFTP-3 Cryptographic protocol = SSH-2 SSH implementation = OpenSSH_5.3 Encryption algorithm = aes Compression = No ———————————————————— Server host key fingerprint ssh-rsa 2048 86:54:d9:09:25:c0:9b:f8:17:8c:c0:52:13:0c:9c:cc ————————————————————   ssh-keygen     ssh-keygen is a standard component of the Secure Shell (SSH) protocol suite found on Unix and Unix-like computer systems used to establish secure shell sessions between remote computers over insecure networks, through the use of various cryptographic techniques. The ssh-keygen utility is used to generate, manage, and convert authentication keys.   ssh-keygen is able to generate a key using one of three different digital signature algorithms.   With the help of the ssh-keygen tool, a user can create passphrase keys for any of these key types (to provide for unattended operation, the passphrase can be left empty, at increased risk).   algorithm     SSH protocol version 1 (now deprecated) only the RSA algorithm was supported.   The SSH protocol version 2 additionally introduced support for the DSA algorithm.   Subsequently, OpenSSH added support for a third digital signature algorithm, ECDSA   DSA     The Digital Signature Algorithm (DSA) is a Federal Information Processing Standard for digital signatures. It was proposed by the National Institute of Standards and Technology (NIST) in August 1991 for use in their Digital Signature Standard (DSS) and adopted as FIPS 186 in 1993   Hash  In cryptography applications, we often need a so-called secure hash function. Secure hash functions are generaelly a subset of hash functions that fulfil at least two extra criteria:      it must be computationally impossible to reverse the mapping, that is, go from a hash code to a message or piece of data that would have generated that hash code;   it must be infeasible for a collision to occur: that is, for two messages to be found that have the same hash code.   In order to fulfil these criteria (or at least, as a by-product of needing to fulfil these criteria), secure hash functions generally have these characteristics:   they are slower to compute than the hash codes typically used to key hash maps; they are wider (i.e. have more bits) than weak hash codes.   Secure hash codes are typically 128 bits wide at the very least; compare that, for example, to the 32-bit codes returned by Java hashCode() method, or the 64-bit hash codes recommended for key-less hash maps in Numerical Recipes.   Applications of secure hash functions Secure hash functions actually have various applications. A very common case is verifying the integrity of data. When we send some data, we append a hash of that data; on the receiving end, we re-hash the received data and check that the computed hash equals that sent; if any of the data has changed then (with overwhelming probability), the computed hash value will no longer match the original. Another case is where we need to authenticate some data, i.e. produce a kind of integrity check that only a party with a given private key could produce. (In this case, the general solution is to combine a hash code with encryption.)   In other cases, a secure hash function is useful to represent a particular item of data. For example, for the purpose of checking passwords, we need only store a hash of that password. When somebody enters their password, if the computed hash of what they entered matches the hash stored in the password file/database, we assume they knew the password.   This scheme, sometimes called compare by hash (CBH) can be used to search for duplicates of data on a hard drive or for synching data between multiple machines. Similarly, another example are the databases that various law enforcement agencies keep of known “disapproved” files that they want to search peoples hard drives for. In these applications, keeping a database of the actual file contents, and/or transmitting and comparing those entire contents, would be impractical. Instead, only hashes are stored and compared.   More broadly, secure hash functions are useful in a variety of cases where we need a trapdoor function (i.e. one that cannot feasibly be reversed), especially where we need one with a limited or fixed-size result.   As mentioned, secure hashes are sometimes called message digests. And in fact, the main class for computing them in Java is java.security.MessageDigest. We get an instance of MessageDigest, feed it an array (or several arrays) of bytes, then call its digest() method to get the resulting hash:   byte[] data = ....  MessageDigest md = MessageDigest.getInstance(\"SHA-1\"); md.update(data); byte[] hash = md.digest();   MD5  MD5 is a later hash function developed by Ron Rivest. It is one of the most common hash algorithms in use today. Like MD2, it is a 128-bit hash function but, unlike its predecessor, it is one of the fastest “secure” hash functions in common use, and the fastest provided in Java 6.   Unfortunately, it is now considered insecure. Aside from the relatively small hash size, there are well-published methods to find collisions analytically in a trivial amount of time. For example, Vlastimil Klima has published a C program to find MD5 collisions in around 30 seconds on an average PC. If you need security, dont use MD5!   Although insecure, MD5 still makes a good general strong hash function due to its speed. In non-security applications such as finding duplicate files on a hard disk (where you are not trying to protect against the threat model of somebody deliberately fooling your system), MD5 makes a good choice.   SHA algorithms  SHA (Secure Hash Algorithm) refers collectively to various hash functions developed by the US National Security Agency (NSA). The various algorithms are based on differing hash sizes and (in principle) offer corresponding levels of security:   PBE  password-based encryption   The technique of generating a secret key from a user-generated passphrase is usually called password-based encryption (PBE). As you might imagine, it is fraught with difficulty. In particular:      the user is requirement and the security requirement usually conflict: the user requires an easy-to-remember passphrase, or at least one that is made of recognisable characters and short enough to write down; yet for secure encryption by today is standards, we require at least 128 strongly random bits (and ideally more);   password-based encryption is typically used in applications where an attacker can repeatedly try to guess the password undetected and beyond the control of the genuine sender/recipient (if the password is being used to log into our server, we can detect that so many invalid attempts were made and in the worst case shut down our server to prevent further attempts; but if an eavesdropper takes a copy of the encrypted ZIP file we e-mailed, we will never know that they are sitting there with a 100,000-processor botnet trying to brute-force the password, and they can essentially sit doing it for as long as they like). The typical result is fairly dire: most password-protected data is encrypted with weak encryption keys, and an attacker can spend all the processor time they like trying to guess that weak key with complete impunity.   How to use PBE  There are two fundamental problems:  (a) user-memorable passwords typically dont contain as much randomness as we need for a secure key;  (b) in a typical application, an attacker gets as many tries as they like at the password. An additional problem is that if, say, the password abc123 always generated the same key in our application, then an attacker could calculate the key from this password once and then quickly decrypt any data protected with this password.   Two common techniques are used in password-based encryption to try to alleviate these problems:      a deliberately slow method is used to derive the encryption key from the password, reducing the number of guesses that an attacker can make in a given time frame;   some random bytes, called a salt, are appended to the password before it is used to calculate the key.   Cryptographic hash function     A cryptographic hash function is a special class of hash function that has certain properties which make it suitable for use in cryptography. It is a mathematical algorithm that maps data of arbitrary size to a bit string of a fixed size (a hash function) which is designed to also be a one-way function, that is, a function which is infeasible to invert. The only way to recreate the input data from an ideal cryptographic hash function is output is to attempt a brute-force search of possible inputs to see if they produce a match.   Bruce Schneier has called one-way hash functions the workhorses of modern cryptography.[1] The input data is often called the message, and the output (the hash value or hash) is often called the message digest or simply the digest.   in information-security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, even though all these terms stand for more general functions with rather different properties and purposes.   Another finalist from the NIST hash function competition, BLAKE, was optimized to produce BLAKE2 which is notable for being faster than SHA-3, SHA-2, SHA-1, or MD5, and is used in numerous applications and libraries.   The ideal cryptographic hash function has five main properties:     it is deterministic so the same message always results in the same hash   it is quick to compute the hash value for any given message   it is infeasible to generate a message from its hash value except by trying all possible messages   a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value   it is infeasible to find two different messages with the same hash value   Illustration     An illustration of the potential use of a cryptographic hash is as follows: Alice poses a tough math problem to Bob and claims she has solved it. Bob would like to try it himself, but would yet like to be sure that Alice is not bluffing. Therefore, Alice writes down her solution, computes its hash and tells Bob the hash value (whilst keeping the solution secret). Then, when Bob comes up with the solution himself a few days later, Alice can prove that she had the solution earlier by revealing it and having Bob hash it and check that it matches the hash value given to him before. (This is an example of a simple commitment scheme; in actual practice, Alice and Bob will often be computer programs, and the secret would be something less easily spoofed than a claimed puzzle solution).   Applications  Verifying the integrity of files or messages     An important application of secure hashes is verification of message integrity. Determining whether any changes have been made to a message (or a file), for example, can be accomplished by comparing message digests calculated before, and after, transmission (or any other event).   For this reason, most digital signature algorithms only confirm the authenticity of a hashed digest of the message to be “signed”. Verifying the authenticity of a hashed digest of the message is considered proof that the message itself is authentic.   MD5, SHA1, or SHA2 hashes are sometimes posted along with files on websites or forums to allow verification of integrity.[6] This practice establishes a chain of trust so long as the hashes are posted on a site authenticated by HTTPS.   Password verification     Storing all user passwords as cleartext can result in a massive security breach if the password file is compromised. One way to reduce this danger is to only store the hash digest of each password. To authenticate a user, the password presented by the user is hashed and compared with the stored hash.   The password is often concatenated with a random, non-secret salt value before the hash function is applied. The salt is stored with the password hash. Because users have different salts, it is not feasible to store tables of precomputed hash values for common passwords.   MD5     The MD5 algorithm is a widely used hash function producing a 128-bit hash value. Although MD5 was initially designed to be used as a cryptographic hash function, it has been found to suffer from extensive vulnerabilities. It can still be used as a checksum to verify data integrity, but only against unintentional corruption.   Like most hash functions, MD5 is neither encryption nor encoding. It can be reversed by brute-force attack and suffers from extensive vulnerabilities as detailed in the security section below.   MD5 was designed by Ronald Rivest in 1991 to replace an earlier hash function MD4.   The MD5 hash function receives its acronym MD from its structure using Merkle–Damg?rd construction.   Collision resistance     Collision resistance is a property of cryptographic hash functions: a hash function H is collision resistant if it is hard to find two inputs that hash to the same output; that is, two inputs a and b such that H(a) = H(b), and a ≠ b   Collision resistance does not mean that no collisions exist; simply that they are hard to find.   Cryptographic hash functions are usually designed to be collision resistant. But many hash functions that were once thought to be collision resistant were later broken. MD5 and SHA-1 in particular both have published techniques more efficient than brute force for finding collisions.   Rationale  Collision resistance is desirable for several reasons.     In some digital signature systems, a party attests to a document by publishing a public key signature on a hash of the document. If it is possible to produce two documents with the same hash, an attacker could get a party to attest to one, and then claim that the party had attested to the other.   In some proof-of-work systems (e.g. bitcoin mining), users provide hash collisions as proof that they have performed a certain amount of computation to find them. If there is an easier way to find collisions than brute force, users can cheat the system.   In some distributed content systems, parties compare cryptographic hashes of files in order to make sure they have the same version. An attacker who could produce two files with the same hash could trick users into believing they had the same version of a file when they in fact did not.   Algorithm     The Merkle–Damg?rd hash function first applies an MD-compliant padding function to create an input whose size is a multiple of a fixed number (e.g. 512 or 1024) — this is because compression functions cannot handle inputs of arbitrary size. The hash function then breaks the result into blocks of fixed size, and processes them one at a time with the compression function, each time combining a block of the input with the output of the previous round.[1]:146 In order to make the construction secure, Merkle and Damg?rd proposed that messages be padded with a padding that encodes the length of the original message. This is called length padding or Merkle–Damg?rd strengthening.   In the diagram, the one-way compression function is denoted by f, and transforms two fixed length inputs to an output of the same size as one of the inputs. The algorithm starts with an initial value, the initialization vector (IV). The IV is a fixed value (algorithm or implementation specific). For each message block, the compression (or compacting) function f takes the result so far, combines it with the message block, and produces an intermediate result. The last block is padded with zeros as needed and bits representing the length of the entire message are appended. (See below for a detailed length padding example.)   To harden the hash further the last result is then sometimes fed through a finalisation function. The finalisation function can have several purposes such as compressing a bigger internal state (the last result) into a smaller output hash size or to guarantee a better mixing and avalanche effect on the bits in the hash sum. The finalisation function is often built by using the compression function[citation needed] (Note that in some documents instead the act of length padding is called “finalisation”).   Merkle–Damg?rd construction     In cryptography, the Merkle–Damg?rd construction or Merkle–Damg?rd hash function is a method of building collision-resistant cryptographic hash functions from collision-resistant one-way compression functions.[1]:145 This construction was used in the design of many popular hash algorithms such as MD5, SHA1 and SHA2.   The Merkle–Damg?rd construction was described in Ralph Merkles Ph.D. thesis in 1979.[2] Ralph Merkle and Ivan Damg?rd independently proved that the structure is sound: that is, if an appropriate padding scheme is used and the compression function is collision-resistant, then the hash function will also be collision resistant.   SHA     In cryptography, SHA-1 (Secure Hash Algorithm 1) is a cryptographic hash function designed by the United States National Security Agency and is a U.S. Federal Information Processing Standard published by the United States NIST.[2] SHA-1 produces a 160-bit (20-byte) hash value known as a message digest. A SHA-1 hash value is typically rendered as a hexadecimal number, 40 digits long.   Applications     SHA-1 forms part of several widely used security applications and protocols, including TLS and SSL, PGP, SSH, S/MIME, and IPsec. Those applications can also use MD5; both MD5 and SHA-1 are descended from MD4. SHA-1 hashing is also used in distributed revision control systems like Git, Mercurial, and Monotone to identify revisions, and to detect data corruption or tampering. The algorithm has also been used on Nintendos Wii gaming console for signature verification when booting, but a significant flaw in the first implementations of the firmware allowed for an attacker to bypass the systems security scheme.   SHA-1 and SHA-2 are the secure hash algorithms required by law for use in certain U.S. Government applications, including use within other cryptographic algorithms and protocols, for the protection of sensitive unclassified information.   A prime motivation for the publication of the Secure Hash Algorithm was the Digital Signature Standard, in which it is incorporated.   Revision control systems such as Git and Mercurial use SHA-1 not for security but for ensuring that the data has not changed due to accidental corruption.   Salt (cryptography)     In cryptography, a salt is random data that is used as an additional input to a one-way function that “hashes” a password or passphrase. Salts are closely related to the concept of nonce. The primary function of salts is to defend against dictionary attacks or against its hashed equivalent, a pre-computed rainbow table attack.[1]   Salts are used to safeguard passwords in storage.   The purpose of a hash and salt process in password security is not to prevent a password from being guessed, but to prevent a leaked password database from being used in further attacks.   The idea of salt is that when the user enters the password, we dont actually use their raw password to generate the key. We first append some random bytes to the password. A new, random salt is used for every file/piece of data being encrypted. The salt bytes are not secret: they are stored unencrypted along side the encrypted data. This means that the salt bytes would add no extra security if there was only once piece of data in the world encrypted with a given password. But they prevent dictionary attacks, whereby an attacker pre-computes the keys from some common passwords and then tries those keys on the encrypted data. Without salt bytes, the dictionary attack would be worthwhile attack because we use a deliberately slow function to derive a key from a password. With the salt bytes, the attacker is forced to run the slow key derivation function for each password they want to try on each piece of data.   To generate salt bytes in Java, we just need to make sure that we use a secure random number generator. Construct an instance of SecureRandom, create (say) a 20-byte array, and call nextBytes() on that array:  Random r = new SecureRandom(); byte[] salt = new byte[20]; r.nextBytes(salt);   Secure Random  The SecureRandom class, housed in the java.security package, provides a drop-in replacement to java.lang.Random. But unlike the latter, java.security.SecureRandom is designed to be cryptographically secure.   SecureRandom is typically used in cases where:   random numbers are generated for security related purposes, such as generating an encryption key or session ID (see below); or, more generally, high-quality randomness is important and it is worth consuming CPU (or where CPU consumption is not an issue) to generate those high-quality random numbers.   Properties of SecureRandom  We said that SecureRandom is designed to be cryptographically secure. In practice, this means that the generator has the following properties:      given only a number produced by the generator, it is (to all intents and purposes) impossible to predict previous and future numbers;   the numbers produced contain no known biases;   the generator has a large period (in Suns standard implementation, based on the 160-bit SHA1 hash function, the period is 2160);   the generator can seed itself at any position within that period with equal probability (or at least, it comes so close to this goal, that we have no practical way of telling otherwise).   These properties are important in various security applications. The first is important, for eaxmple, if we use the generator to produce, say, a session ID on a web server: we donot want user n to predict user n+1 s session ID. Similarly, we donot want a user in an Internet cafe, based on the session ID or encryption key that they are given to access a web site, to be able to predict the value assigned to a previous user on that machine.   The importance of producing all values with equal probability   For example, let s say that we want to pick a 128-bit AES encryption key. The idea of a strong encryption algorithm such as AES is that in order for an adversary to guess the key by “brute force” (which we assume is the only possible means), they would have to try every single possible key in turn until they hit on the right one. By law of averages, we would expect them to find it after half as many guesses as there are possible keys. A 128-bit key has 2128 possible values, so on average, they would have to try 2127 keys. In decimal 2127 is a 39-digit number. Or put another way, trying a million million keys per second, it would take 5x1015 millennia to try 2127 keys. Not even the British government wants to decrypt your party invitations that badly. So with current mainstream technology1, a 128-bit key is in principle sufficient for most applications.   But these metrics hold true only if our key selection algorithm— i.e. our random number generator— genuinely can pick any of the possible keys. For example, we certainly should not choose the key as follows:  // This is WRONG!! Do not do this! Random ranGen = new Random(); byte[] aesKey = new byte[16]; // 16 bytes = 128 bits ranGen.nextBytes(aesKey);  The problem here is that the period of java.util.Random is only 248. Even though we are generating a 128-bit key, we will only ever pick from a subset of 248 of the possible keys. Or put another way: an attacker need only try on average 247 keys, and will find our key by trial and error in a couple of days if they try just a thousand million keys per second. And as if that wasnot bad enough, they probably donot even need to try anywhere near 247: for reasons discussed earlier, there is a good chance that an instance of java.util.Random created within a couple of minutes of bootup will actually be seeded from a about one thousandth of the 248 possible values. This time, HM Sniffing Service doesnot even need expensive hardware to find the secret location of your housewarming party: a trip to Staples will give them all the computing power they need.   So as you’ve probably guessed, our solution to the problem is to use SecureRandom instead:   import java.security.SecureRandom; .. Random ranGen = new SecureRandom(); byte[] aesKey = new byte[16]; // 16 bytes = 128 bits ranGen.nextBytes(aesKey); Now, there’s a good chance that any of the 2128 possible keys will be chosen.   Seeding of SecureRandom In order to provide this property of choosing any seed with “equal” likelihood, (or at least, with no bias that is practically detectable), SecureRandom seeds itself from sources of entropy available from the local machine, such as timings of I/O events.   ##   Rainbow table     A rainbow table is a precomputed table for reversing cryptographic hash functions, usually for cracking password hashes.   Tables are usually used in recovering a plaintext password up to a certain length consisting of a limited set of characters. It is a practical example of a space/time trade-off, using less computer processing time and more storage than a brute-force attack which calculates a hash on every attempt, but more processing time and less storage than a simple lookup table with one entry per hash.   After gathering a password hash, using said hash as a password would fail since the authentication system would hash it a second time. In order to learn a users password, a password that produces the same hashed value must be found, usually through a brute-force or dictionary attack.   Use of a key derivation function that employs a salt makes this attack infeasible.   Passphrase     A passphrase is a sequence of words or other text used to control access to a computer system, program or data. A passphrase is similar to a password in usage, but is generally longer for added security. Passphrases are often used to control both access to, and operation of, cryptographic programs and systems, especially those that derive an encryption key from a passphrase. The origin of the term is by analogy with password. The modern concept of passphrases is believed to have been invented by Sigmund N. Porter[1] in 1982.   Compared to password     Passphrases differ from passwords. A password is usually short—six to ten characters. Such passwords may be adequate for various applications (if frequently changed, if chosen using an appropriate policy, if not found in dictionaries, if sufficiently random, and/or if the system prevents online guessing, etc.)   But passwords are typically not safe to use as keys for standalone security systems (e.g., encryption systems) that expose data to enable offline password guessing by an attacker.[citation needed] Passphrases are theoretically stronger, and so should make a better choice in these cases. First, they usually are (and always should be) much longer—20 to 30 characters or more is typical—making some kinds of brute force attacks entirely impractical. Second, if well chosen, they will not be found in any phrase or quote dictionary, so such dictionary attacks will be almost impossible. Third, they can be structured to be more easily memorable than passwords without being written down, reducing the risk of hardcopy theft.[citation needed] However, if a passphrase is not protected appropriately by the authenticator and the clear-text passphrase is revealed its use is no better than other passwords. For this reason it is recommended that passphrases not be reused across different or unique sites and services.   Dictionary Attack     In cryptanalysis and computer security, a dictionary attack is a technique for defeating a cipher or authentication mechanism by trying to determine its decryption key or passphrase by trying hundreds or sometimes millions of likely possibilities, such as words in a dictionary.   A dictionary attack is based on trying all the strings in a pre-arranged listing, typically derived from a list of words such as in a dictionary (hence the phrase dictionary attack).   In contrast to a brute force attack, where a large proportion of the key space is searched systematically, a dictionary attack tries only those possibilities which are deemed most likely to succeed.   Dictionary attacks often succeed because many people have a tendency to choose short passwords that are ordinary words or common passwords, or simple variants obtained, for example, by appending a digit or punctuation character. Dictionary attacks are relatively easy to defeat, e.g. by using a passphrase or otherwise choosing a password that is not a simple variant of a word found in any dictionary or listing of commonly used passwords.   It is possible to achieve a time-space tradeoff by pre-computing a list of hashes of dictionary words, and storing these in a database using the hash as the key. This requires a considerable amount of preparation time, but allows the actual attack to be executed faster.   A more refined approach involves the use of rainbow tables, which reduce storage requirements at the cost of slightly longer lookup times.   Pre-computed dictionary attacks, or “rainbow table attacks”, can be thwarted by the use of salt, a technique that forces the hash dictionary to be recomputed for each password sought, making precomputation infeasible provided the number of possible salt values is large enough.   Avalanche effect     In cryptography, the avalanche effect is the desirable property of cryptographic algorithms, typically block ciphers and cryptographic hash functions wherein if when an input is changed slightly (for example, flipping a single bit) the output changes significantly (e.g., half the output bits flip). In the case of high-quality block ciphers, such a small change in either the key or the plaintext should cause a drastic change in the ciphertext. The actual term was first used by Horst Feistel,[1] although the concept dates back to at least Shannon’s diffusion.   The SHA-1 hash function exhibits good avalanche effect. When a single bit is changed the hash sum becomes completely different. If a block cipher or cryptographic hash function does not exhibit the avalanche effect to a significant degree, then it has poor randomization, and thus a cryptanalyst can make predictions about the input, being given only the output. This may be sufficient to partially or completely break the algorithm. Thus, the avalanche effect is a desirable condition from the point of view of the designer of the cryptographic algorithm or device.   Public-key cryptography     An unpredictable (typically large and random) number is used to begin generation of an acceptable pair of keys suitable for use by an asymmetric key algorithm.   In an asymmetric key encryption scheme, anyone can encrypt messages using the public key, but only the holder of the paired private key can decrypt. Security depends on the secrecy of the private key.   In the Diffie–Hellman key exchange scheme, each party generates a public/private key pair and distributes the public key. After obtaining an authentic copy of each other’s public keys, Alice and Bob can compute a shared secret offline. The shared secret can be used, for instance, as the key for a symmetric cipher.   Public key cryptography, or asymmetric cryptography, is any cryptographic system that uses pairs of keys: public keys which may be disseminated widely, and private keys which are known only to the owner. This accomplishes two functions: authentication, which is when the public key is used to verify that a holder of the paired private key sent the message, and encryption, whereby only the holder of the paired private key can decrypt the message encrypted with the public key.   In a public key encryption system, any person can encrypt a message using the public key of the receiver, but such a message can be decrypted only with the receiver’s private key. For this to work it must be computationally easy for a user to generate a public and private key-pair to be used for encryption and decryption. The strength of a public key cryptography system relies on the degree of difficulty (computational impracticality) for a properly generated private key to be determined from its corresponding public key. Security then depends only on keeping the private key private, and the public key may be published without compromising security.   Because of the computational complexity of asymmetric encryption, it is usually used only for small blocks of data, typically the transfer of a symmetric encryption key (e.g. a session key). This symmetric key is then used to encrypt the rest of the potentially long message sequence. The symmetric encryption/decryption is based on simpler algorithms and is much faster.   Usage of public key  Two of the best-known uses of public key cryptography are:     Public key encryption, in which a message is encrypted with a recipient’s public key. The message cannot be decrypted by anyone who does not possess the matching private key, who is thus presumed to be the owner of that key and the person associated with the public key. This is used in an attempt to ensure confidentiality.        Digital signatures, in which a message is signed with the sender’s private key and can be verified by anyone who has access to the sender’s public key. This verification proves that the sender had access to the private key, and therefore is likely to be the person associated with the public key. This also ensures that the message has not been tampered with, as a signature is mathematically bound to the message it originally was made with, and verification will fail for practically any other message, no matter how similar to the original message.       An analogy to public key encryption is that of a locked mail box with a mail slot. The mail slot is exposed and accessible to the public – its location (the street address) is, in essence, the public key. Anyone knowing the street address can go to the door and drop a written message through the slot. However, only the person who possesses the key can open the mailbox and read the message.   An analogy for digital signatures is the sealing of an envelope with a personal wax seal. The message can be opened by anyone, but the presence of the unique seal authenticates the sender.   Problems of public generated key     A central problem with the use of public key cryptography is confidence/proof that a particular public key is authentic, in that it is correct and belongs to the person or entity claimed, and has not been tampered with or replaced by a malicious third party. The usual approach to this problem is to use a public key infrastructure (PKI), in which one or more third parties – known as certificate authorities – certify ownership of key pairs. PGP, in addition to being a certificate authority structure, has used a scheme generally called the “web of trust”, which decentralizes such authentication of public keys by a central mechanism, and substitutes individual endorsements of the link between user and public key. To date, no fully satisfactory solution to the “public key authentication problem” has been found   RSA     RSA is one of the first practical public-key cryptosystems and is widely used for secure data transmission. In such a cryptosystem, the encryption key is public and differs from the decryption key which is kept secret. In RSA, this asymmetry is based on the practical difficulty of factoring the product of two large prime numbers, the factoring problem. RSA is made of the initial letters of the surnames of Ron Rivest, Adi Shamir, and Leonard Adleman, who first publicly described the algorithm in 1977.   A user of RSA creates and then publishes a public key based on two large prime numbers, along with an auxiliary value. The prime numbers must be kept secret. Anyone can use the public key to encrypt a message, but with currently published methods, if the public key is large enough, only someone with knowledge of the prime numbers can feasibly decode the message.[2] Breaking RSA encryption is known as the RSA problem; whether it is as hard as the factoring problem remains an open question.   RSA is a relatively slow algorithm, and because of this it is less commonly used to directly encrypt user data. More often, RSA passes encrypted shared keys for symmetric key cryptography which in turn can perform bulk encryption-decryption operations at much higher speed.   Operation     The RSA algorithm involves four steps: key generation, key distribution, encryption and decryption.   RSA involves a public key and a private key. The public key can be known by everyone and is used for encrypting messages. The intention is that messages encrypted with the public key can only be decrypted in a reasonable amount of time using the private key.   Symmetric-key algorithm     Symmetric-key algorithms[1] are algorithms for cryptography that use the same cryptographic keys for both encryption of plaintext and decryption of ciphertext. The keys may be identical or there may be a simple transformation to go between the two keys   The keys, in practice, represent a shared secret between two or more parties that can be used to maintain a private information link.   This requirement that both parties have access to the secret key is one of the main drawbacks of symmetric key encryption, in comparison to public-key encryption (also known as asymmetric key encryption)   Implementations     Examples of popular symmetric algorithms include Twofish, Serpent, AES (Rijndael), Blowfish, CAST5, Kuznyechik, RC4, 3DES, Skipjack, Safer+/++ (Bluetooth), and IDEA.[5][6]   References     https://www.javamex.com/tutorials/cryptography/hash_functions_algorithms.shtml   https://www.goanywhere.com/blog/2011/10/20/sftp-ftps-secure-ftp-transfers   https://en.wikipedia.org/wiki/Secure_Shell   https://en.wikipedia.org/wiki/Ssh-keygen   https://en.wikipedia.org/wiki/Cryptographic_hash_function   https://en.wikipedia.org/wiki/Rainbow_table   https://en.wikipedia.org/wiki/Merkle%E2%80%93Damg%C3%A5rd_construction   https://en.wikipedia.org/wiki/Public-key_cryptography   https://en.wikipedia.org/wiki/RSA_%28cryptosystem%29   ","categories": [],
        "tags": ["SSH","Cryptography"],
        "url": "/2017/02/06/SSH-Cryptography.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "R Language",
        "excerpt":"s&lt;-read.csv(\"C:/Users/xxx/dev/R/IRS/SHH_SCHISHG.csv\") # aggregate  s2&lt;-table(s$Original.CP) s3&lt;-as.data.frame(s2) # extract by Frequency ordered  s3[order(-s3$Freq),]  ","categories": [],
        "tags": ["R","Data Science"],
        "url": "/2017/02/09/R-Lang.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "SeriableVersionUID",
        "excerpt":"Noteworthy points about SeriableVersionUID in Java   Preventing ClassCastExceptions with SerialVersionUID Problem     Your classes were recompiled, and you’re getting ClassCastExceptions that you shouldn’t. Solution   Run serialver to generate a “serial version UUID” and paste its output into your classes before you start. Or use your IDE’s tools for this purpose. Discussion   When a class is undergoing a period of evolution—particularly a class being used in a networking context such as RMI or servlets—it may be useful to provide a serialVersionUID value in this class. This is a long that is basically a hash of the methods and fields in the class. Both the object serialization API (see Saving and Restoring Java Objects) and the JVM, when asked to cast one object to another (common when using collections, as in Chapter 7), either look up or, if not found, compute this value. If the value on the source and destination do not match, a ClassCastException is thrown. Most of the time, this is the correct thing for Java to do. However, sometimes you may want to allow a class to evolve in a compatible way, but you can’t immediately replace all instances in circulation. You must be willing to write code to account for the additional fields being discarded if restoring from the longer format to the shorter and having the default value (null for objects, 0 for numbers, and false for Boolean) if you’re restoring from the shorter format to the longer. If you are only adding fields and methods in a reasonably compatible way, you can control the compatibility by providing a long int named serialVersionUID. The initial value should be obtained from a JDK tool called serialver, which takes just the class name. Consider a simple class called SerializableUser:     public class SerializableUser implements java.io.Serializable {   public String name;   public String address;   public String country;   public String phoneNum;    // other fields, and methods, here...   static final long serialVersionUID = -7978489268769667877L; }          I first compiled it with two different compilers to ensure that the value is a product of the class structure, not of some minor differences in class file format that different compilers might emit:      $ javac SerializableUser.java $ serialver SerializableUser SerializableUser:    static final long serialVersionUID = -7978489268769667877L; $ jikes +E SerializableUser.java $ serialver SerializableUser SerializableUser:    static final long serialVersionUID = -7978489268769667877L;          Sure enough, the class file from both compilers has the same hash. Now let’s change the file. I go in with an editor and add a new field, phoneNum, right after country:       public String country; public String phoneNum;      // Added this line.           $ javac SerializableUser.java $ serialver SerializableUser SerializableUser:    static final long serialVersionUID = -8339341455288589756L;  Notice how the addition of the field changed the serialVersionUID! Now, if I had wanted this class to evolve in a compatible fashion, here’s what I should have done before I started expanding it. I copy and paste the original serialver output into the source file (again using an editor to insert a line before the last line):   // The following is the line I added to SerializableUser.java private static final long serialVersionUID = -7978489268769667877L;     $ javac SerializableUser.java $ serialver SerializableUser SerializableUser:    static final long serialVersionUID = -7978489268769667877L; $     Now all is well: I can interchange serialized versions of this file. Note that serialver is part of the “object serialization” mechanism, and, therefore, it is meaningful only on classes that implement the Serializable interface described in Saving and Restoring Java Objects. Note also that some developers use serialVersionUID values that start at 1 (a choice offered by some IDEs when they note that a class that appears to be serializable lacks a serialVersionUID), and then simply increment it by one each time the class changes in an incompatible way.  ","categories": [],
        "tags": ["java","serialVersionUID"],
        "url": "/2017/02/14/SerialVersionUID.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Java Security Notes",
        "excerpt":"Java Security     well-behaved: programs should be prevent from consuming too much system resources   Components     JCE: Java Cryptography Extension   JSSE: Java Secure Socketets Extension   JAAS: Java Authentication and Authorization Service   Anatomy of a Java Application  the bytecode verifier  The bytecode verifier ensures that Java class files follow the rules of the Java language. As the figure implies, not all classes are subject to bytecode verification.  the class loader  One or more class loaders load all Java classes. Programatically, the class loader can set permissions for each class it loads.  the access controller  The access controller allows (or prevents) most access from the core API to the operating system, based upon policies set by the end user or system administrator.  the security manager  The security manager is the primary interface between the core API and the operating system; it has the ultimate responsibility for allowing or preventing access to all system resources. However, it exists mostly for historical reasons; it defers its actions to the access controller.  The security package  the security package is a complex API. This includes discussions of:     The security provider interface −− the means by which different security implementations may be plugged into the security package   Message digests   Keys and certificates   Digital signatures   Encryption (through JCE and JSSE)   Authentication (through JAAS)   The key database  The key database is a set of keys used by the security infrastructure to create or verify digital signatures. In the Java architecture, it is part of the security package, though it may be manifested as an external file or database.   Trusted and Untrusted Classes     In Java 2, only classes in the core API are considered trusted. Other classes must be given explicit permission to perform the operations we’ve discussed.   Summary  Although the security manager is the most commonly known feature of Java’s security story, it’s often misunderstood: there is no standard security manager among Java implementations, and Java applications, by default, have no security manager at all.   Access Controller     The implementation of most security managers, however, is based entirely upon the access controller.   Permissions     The basic entity that the access controller operates on is a permission object −− an instance of the Permission class (java.security.Permission). This class, of course, is the basis of the types that are listed in a policy file for the default security policy. The Permission class itself is an abstract class that represents a particular operation. The nomenclature here is a little misleading because a permission object can reflect two things. When it is associated with a class (through a code source and a protection domain), a permission object represents an actual permission that has been granted to that class. Otherwise, a permission object allows us to ask if we have a specific permission.   For example, if we construct a permission object that represents access to a file, possession of that object does not mean we have permission to access the file. Rather, possession of the object allows us to ask if we have permission to access the file.   The access controller is built upon the four concepts     Code sources An encapsulation of the location from which certain Java classes were obtained.   Permissions An encapsulation of a request to perform a particular operation.   Policies An encapsulation of all the specific permissions that should be granted to specific code sources.   Protection domains An encapsulation of a particular code source and the permissions granted to that code source.   Byte code verifier     The verifier is often referred to as a mini−theorem prover (a term first used in several documents from Sun). This sounds somewhat more impressive than it is; it’s not a generic, all−purpose theorem prover by any means. Instead, it’s a piece of code that can prove one (and only one) thing −− that a given series of ( Java) bytecodes represents a legal set of ( Java) instructions.   Shifting  Java and JavaScript perform sign extension when shift¬ing right, filling the empty spaces with 1’s for negative numbers, so 10100110 » 5 becomes 11111101. The »&gt; operator is unique to Java and JavaScript. It does a logical shift right, filling the empty spaces with 0 no matter what the value, so 10100110 »&gt; 5 becomes 00000101.   The shift operators enable you to multiply and divide by powers of 2 very quickly. For non-negative numbers, shifting to the right one bit is equivalent to dividing by 2, and shifting to the left one bit is equivalent to multiplying by 2. For negative numbers, it obviously depends on the language being used.   Specifically, the bytecode verifier can prove the following:     The class file has the correct format. The full definition of the class file format may be found in the Java virtual machine specification; the bytecode verifier is responsible for making sure that the class file has the right length, the correct magic numbers in the correct places, and so on.   Final classes are not subclassed, and final methods are not overridden.   Every class (except for java.lang.Object) has a single superclass.   There is no illegal data conversion of primitive data types (e.g., int to Object).   No illegal data conversion of objects occurs. Because the casting of a superclass to its subclass may be a valid operation (depending on the actual type of the object being cast), the verifier cannot ensure that such casting is not attempted −− it can only ensure that before each such attempt is made, the legality of the cast is tested.   There are no operand stack overflows or underflows.   Stacks     In Java, there are two stacks for each thread. One stack holds a series of method frames, where each method frame holds the local variables and other storage for a particular method invocation. This stack is known as the data stack and is what we normally think of as the stack within a traditional program. The bytecode verifier cannot prevent overflow of this stack −− an infinitely recursive method call will cause this stack to overflow. However, each method invocation requires a second stack (which itself is allocated on the data stack) that is referred to as the operand stack; the operand stack holds the values that the Java bytecodes operate on. This secondary stack is the stack that the bytecode verifier can ensure will not overflow or underflow.   Security Manager     The implementation of the sandbox depends on three things:   The security manager, which provides the mechanism that the Java API uses to see if security−related operations are allowed.   The access controller, which provides the basis of the default implementation of the security manager.   The class loader, which encapsulates information about security policies and classes.   We’ll start by examining the security manager. From the perspective of the Java API, there is a security manager that actually is in control of the security policy of an application. The purpose of the security manager is to determine whether particular operations should be permitted or denied. In truth, the purpose of the access controller is really the same: it decides whether access to a critical system resource should be permitted or denied. Hence, the access controller can do everything the security manager can do.   The reason there is both an access controller and a security manager is mainly historical: the access controller is only available in Java 2 and subsequent releases. Before the access controller existed, the security manager relied on its internal logic to determine the security policy that should be in effect, and changing the security policy required changing the security manager itself. Starting with Java 2, the security manager defers these decisions to the access controller. Since the security policy enforced by the access controller can be specified by using policy files, this allows a much more flexible mechanism for determining policies. The access controller also gives us a much simpler method of granting fine−grained, specific permissions to specific classes. That process was theoretically possibly with the security manager alone, but it was simply too hard to implement.   The BasicPermission class     If you need to implement your own permission class, the BasicPermission class (java.security.BasicPermission) provides some useful semantics. This class implements a basic permission −− that is, a permission that doesn’t have actions. Basic permissions can be thought of as binary permissions −− you either have them or you don’t. However, this restriction does not prevent you from implementing actions in your subclasses of the BasicPermission class (as the PropertyPermission class does).        The prime benefit of this class is the manner in which it implements wildcards. Names in basic permissions are considered to be hierarchical, following a dot−separated convention. For example, if the XYZ corporation wanted to create a set of basic permissions, they might use the convention that the first word of the permission always be xyz: xyz.readDatabase, xyz.writeDatabase, xyz.runPayrollProgram, xyz.HRDepartment.accessCheck, and so on. These permissions can then be specified by their full name, or they can be specified with an asterisk wildcard: xyz.* would match each of these (no matter what depth), and * would match every possible basic permission.            http://www.qidianlife.com/index.php?m=home&amp;c=discover&amp;a=article&amp;id=2351       保护密码的最好办法是使用加盐密码哈希（ salted password hashing）。   永远不要告诉用户输错的究竟是用户名还是密码。就像通用的提示那样，始终显示：“无效的用户名或密码。”就行了。这样可以防止攻击者在不知道密码的情况下枚举出有效的用户名。   应当注意的是，用来保护密码的哈希函数，和数据结构课学到的哈希函数是不同的。例如，实现哈希表的哈希函数设计目的是快速查找，而非安全性。只有加密哈希函数（ cryptographic hash function）才可以用来进行密码哈希加密。像 SHA256 、 SHA512 、 RIPEMD 和 WHIRLPOOL 都是加密哈希函数。        破解哈希加密最简单的方法是尝试猜测密码，哈希每个猜测的密码，并对比猜测密码的哈希值是否等于被破解的哈希值。如果相等，则猜中。猜测密码攻击的两种最常见的方法是字典攻击和暴力攻击 。            字典攻击使用包含单词、短语、常用密码和其他可能用做密码的字符串的字典文件。对文件中的每个词都进行哈希加密，将这些哈希值和要破解的密码哈希值比较。如果它们相同，这个词就是密码。字典文件是通过大段文本中提取的单词构成，甚至还包括一些数据库中真实的密码。还可以对字典文件进一步处理以使其更为有效：如单词 “hello” 按网络用语写法转成 “h3110” 。            暴力攻击是对于给定的密码长度，尝试每一种可能的字符组合。这种方式会消耗大量的计算，也是破解哈希加密效率最低的办法，但最终会找出正确的密码。因此密码应该足够长，以至于遍历所有可能的字符组合，耗费的时间太长令人无法承受，从而放弃破解。            目前没有办法来组织字典攻击或暴力攻击。只能想办法让它们变得低效。如果密码哈希系统设计是安全的，破解哈希的唯一方法就是进行字典攻击或暴力攻击遍历每一个哈希值了。            我们可以通过在密码中加入一段随机字符串再进行哈希加密，这个被加的字符串称之为盐值。如上例所示，这使得相同的密码每次都被加密为完全不同的字符串。我们需要盐值来校验密码是否正确。通常和密码哈希值一同存储在帐号数据库中，或者作为哈希字符串的一部分。            盐值无需加密。由于随机化了哈希值，查表法、反向查表法和彩虹表都会失效。因为攻击者无法事先知道盐值，所以他们就没有办法预先计算查询表或彩虹表。如果每个用户的密码用不同的盐再进行哈希加密，那么反向查表法攻击也将不能奏效。            一个常见的错误是每次都使用相同的盐值进行哈希加密，这个盐值要么被硬编码到程序里，要么只在第一次使用时随机获得。这样的做法是无效的，因为如果两个用户有相同的密码，他们仍然会有相同的哈希值。攻击者仍然可以使用反向查表法对每个哈希值进行字典攻击。他们只是在哈希密码之前，将固定的盐值应用到每个猜测的密码就可以了。如果盐值被硬编码到一个流行的软件里，那么查询表和彩虹表可以内置该盐值，以使其更容易破解它产生的哈希值。            用户创建帐号或者更改密码时，都应该用新的随机盐值进行加密。            出于同样的原因，不应该将用户名用作盐值。对每一个服务来说，用户名是唯一的，但它们是可预测的，并且经常重复应用于其他服务。攻击者可以用常见用户名作为盐值来建立查询表和彩虹表来破解密码哈希。            为使攻击者无法构造包含所有可能盐值的查询表，盐值必须足够长。一个好的经验是使用和哈希函数输出的字符串等长的盐值。例如， SHA256 的输出为256位（32字节），所以该盐也应该是32个随机字节。            每个用户的每一个密码都要使用独一无二的盐值。用户每次创建帐号或更改密码时，密码应采用一个新的随机盐值。永远不要重复使用某个盐值。这个盐值也应该足够长，以使有足够多的盐值能用于哈希加密。一个经验规则是，盐值至少要跟哈希函数的输出一样长。该盐应和密码哈希一起存储在用户帐号表中。       存储密码的步骤：            使用 CSPRNG 生成足够长的随机盐值。       将盐值混入密码，并使用标准的密码哈希函数进行加密，如Argon2、 bcrypt 、 scrypt 或 PBKDF2 。       将盐值和对应的哈希值一起存入用户数据库。           校验密码的步骤：            从数据库检索出用户的盐值和对应的哈希值。       将盐值混入用户输入的密码，并且使用通用的哈希函数进行加密。       比较上一步的结果，是否和数据库存储的哈希值相同。如果它们相同，则表明密码是正确的；否则，该密码错误。           ","categories": [],
        "tags": ["java","security"],
        "url": "/2017/02/15/Java-Security.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Java JIT compiler",
        "excerpt":"This is talking about Java JIT (Just-In-Time) compiler           That trade-off is one reason that the compiler executes the interpreted code first—the compiler can figure out which methods are called frequently enough to warrant their compilation. The second reason is one of optimization: the more times that the JVM executes a particular method or loop, the more information it has about that code. This allows the JVM to make a number of optimizations when it compiles the code.            At some point in time, the suminstance variable must reside in main memory, but retrieving a value from main memory is an expensive operation that takes multiple cycles to complete. If the value of sumwere to be retrieved from (and stored back to) main memory on every iteration of this loop, performance would be dismal. Instead, the compiler will load a register with the initial value of sum, perform the loop using that value in the register, and then (at an indeterminate point in time) store the final result from the register back to main memory. This kind of optimization is very effective, but it means that the semantics of thread synchronization (see Chapter 9) are crucial to the behavior of the application            One thread cannot see the value of a variable stored in the register used by another thread; synchronization makes it possible to know exactly when the register is stored to main memory and available to other threads. Register usage is a general optimization of the compiler, and when escape analysis is enabled (see the end of this chapter), register use is quite aggressive.            If the size of your heap will be less than about 3 GB, the 32-bit version of Java will be faster and have a smaller footprint. This is because the memory references within the JVM will be only 32 bits, and manipulating those memory references is less expensive than manipulating 64-bit references (even if you have a 64-bit CPU). The 32-bit references also use less memory.            JVM developers (and even some tools) often refer to the compilers by the names C1 (compiler 1, client compiler) and C2 (compiler 2, server compiler).            The primary difference between the two compilers is their aggressiveness in compiling code. The client compiler begins compiling sooner than the server compiler does. This means that during the beginning of code execution, the client compiler will be faster, because it will have compiled correspondingly more code than the server compiler.            The engineering trade-off here is the knowledge the server compiler gains while it waits: that knowledge allows the server compiler to make better optimizations in the compiled code. Ultimately, code produced by the server compiler will be faster than that produced by the client compiler. From a user’s perspective, the benefit to that trade-off is based on how long the program will run, and how important the startup time of the program is.            The obvious question here is why there needs to be a choice at all: couldn’t the JVM start with the client compiler, and then use the server compiler as code gets hotter? That technique is known as tiered compilation. With tiered compilation, code is first compiled by the client compiler; as it becomes hot, it is recompiled by the server compiler.            To use tiered compilation, specify the server compiler (either with -server or by ensuring it is the default for the particular Java installation being used), and ensure that the Java command line includes the flag -XX:+TieredCompilation (the default value of which is false). In Java 8, tiered compilation is enabled by default.            The client compiler is most often used when fast startup is the primary objective.          The client compiler is most useful when the startup of an application is the overriding performance concern.   Tiered compilation can achieve startup times very close to those obtained from the client compiler.      It is also interesting that tiered compilation is always slightly better than the standard server compiler. In theory, once the program has run enough to compile all the hot spots, the server compiler might be expected to achieve the best (or at least equal) performance. But in any application, there will almost always be some small section of code that is infrequently executed. It is better to compile that code—even if the compilation is not the best that might be achieved—than to execute that code in interpreted mode.      For jobs that run in a fixed amount of time, choose the compiler based on which one is the fastest at executing the actual job.   Tiered compilation provides a reasonable default choice for batch jobs.      For long-running applications, always choose the server compiler, preferably in conjunction with tiered compilation.   ","categories": [],
        "tags": ["java","JIT"],
        "url": "/2017/02/21/Java-JIT.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Spring notes",
        "excerpt":"Spring Bean Life Cycle Callback Methods    A bean life cycle includes the following steps.     Within IoC container, a spring bean is created using class constructor.   Now the dependency injection is performed using setter method.   Once the dependency injection is completed, BeanNameAware.setBeanName() is called. It sets the name of bean in the bean factory that created this bean.   Now &lt; code&gt;BeanClassLoaderAware.setBeanClassLoader() is called that supplies the bean class loader to a bean instance.   Now &lt; code&gt;BeanFactoryAware.setBeanFactory() is called that provides the owning factory to a bean instance.   Now the IoC container calls BeanPostProcessor.postProcessBeforeInitialization on the bean. Using this method a wrapper can be applied on original bean.   Now the method annotated with @PostConstruct is called.   After @PostConstruct, the method InitializingBean.afterPropertiesSet() is called.   Now the method specified by init-method attribute of bean in XML configuration is called.   And then BeanPostProcessor.postProcessAfterInitialization() is called. It can also be used to apply wrapper on original bean.   Now the bean instance is ready to be used. Perform the task using the bean.   Now when the ApplicationContext shuts down such as by using registerShutdownHook() then the method annotated with @PreDestroy is called.   After that DisposableBean.destroy() method is called on the bean.   Now the method specified by destroy-method attribute of bean in XML configuration is called.   Before garbage collection, finalize() method of Object is called.   Spring framework provides following 4 ways for controlling life cycle events of bean:      InitializingBean and DisposableBean callback interfaces   Other Aware interfaces for specific behavior   Custom init() and destroy() methods in bean configuration file   @PostConstruct and @PreDestroy annotations   InitializingBean  The org.springframework.beans.factory.InitializingBean interface specifies a single method −   void afterPropertiesSet() throws Exception;   Destruction callbacks  The org.springframework.beans.factory.DisposableBean interface specifies a single method −   void destroy() throws Exception;   Custom init() and destroy() methods in bean configuration file  The default init and destroy methods in bean configuration file can be defined in two ways:   Bean local definition applicable to a single bean Global definition applicable to all beans defined in beans context   Local definition is given as below.   &lt;beans&gt;  \t&lt;bean id=\"demoBean\" class=\"com.howtodoinjava.task.DemoBean\"  \t\t\t\t\tinit-method=\"customInit\"  \t\t\t\t\tdestroy-method=\"customDestroy\"&gt;&lt;/bean&gt;  &lt;/beans&gt;   Where as global definition is given as below. These methods will be invoked for all bean definitions given under  tag. They are useful when you have a pattern of defining common method names such as init() and destroy() for all your beans consistently. This feature helps you in not mentioning the init and destroy method names for all beans independently.   &lt;beans default-init-method=\"customInit\" default-destroy-method=\"customDestroy\"&gt;         \t&lt;bean id=\"demoBean\" class=\"com.howtodoinjava.task.DemoBean\"&gt;&lt;/bean&gt;  &lt;/beans&gt;   @PostConstruct and @PreDestroy annotations  Spring 2.5 onwards, you can use annotations also for specifying life cycle methods using @PostConstruct and @PreDestroy annotations.   @PostConstruct annotated method will be invoked after the bean has been constructed using default constructor and just before it’s instance is returned to requesting object. @PreDestroy annotated method is called just before the bean is about be destroyed inside bean container. A sample implementation will look like this:   package com.howtodoinjava.task;  import javax.annotation.PostConstruct; import javax.annotation.PreDestroy;  public class DemoBean  { \t@PostConstruct \tpublic void customInit()  \t{ \t\tSystem.out.println(\"Method customInit() invoked...\"); \t} \t \t@PreDestroy \tpublic void customDestroy()  \t{ \t\tSystem.out.println(\"Method customDestroy() invoked...\"); \t} }   Spring Notes   AOP    execution(* concert.Performance.perform())  and !bean('woodstock')  @Aspect public class Audience {   @Before(\"execution(** concert.Performance.perform(..))\")   public void silenceCellPhones() { Before performance           Fortunately, there’s a way: the @Pointcut annotation defines a reusable pointcut within an @AspectJ aspect. The next listing shows the Audience aspect, updated to use @Pointcut.       Reuse pointuct     @Aspect public class Audience { @Pointcut(\"execution(** concert.Performance.perform(..))\") public void performance() {} //Define named pointcut    @Before(\"performance()\") public void silenceCellPhones() {   System.out.println(\"Silencing cell phones\"); @Before(\"performance()\") public void takeSeats() {   System.out.println(\"Taking seats\"); }           The body of the performance() method is irrelevant and, in fact, should be empty. The method itself is just a marker, giving the @Pointcut annotation something to attach itself to.   import org.aspectj.lang.ProceedingJoinPoint; public class Audience { public void watchPerformance(ProceedingJoinPoint jp) { try {  System.out.println(\"Silencing cell phones\"); System.out.println(\"Taking seats\");   &lt;aop:config&gt; &lt;aop:aspect ref=\"audience\"&gt; &lt;aop:pointcut id=\"performance\" expression=\"execution(** concert.Performance.perform(..))\" /&gt; &lt;aop:around     Declare around advice          pointcut-ref=\"performance\" method=\"watchPerformance\"/&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;              to use AspectJ’s @DeclareParents annota¬tion to magically introduce a new method into an advised bean. But AOP introduc¬tions aren’t exclusive to AspectJ. Using the  element from Spring’s aop namespace, you can do similar magic in XML.       Listing 1.7 Spring offers Java-based configuration as an alternative to XML.     package com.springinaction.knights.config; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import com.springinaction.knights.BraveKnight; import com.springinaction.knights.Knight; import com.springinaction.knights.Quest; import com.springinaction.knights.SlayDragonQuest; @Configuration public class KnightConfig { @Bean public Knight knight() { return new BraveKnight(quest()); } @Bean public Quest quest() { return new SlayDragonQuest(System.out); } }                In a Spring application, an application context loads bean definitions and wires them together. The Spring application context is fully responsible for the creation of and wiring of the objects that make up the application. Spring comes with several imple¬mentations of its application context, each primarily differing only in how it loads its configuration.            When the beans in knights.xml are declared in an XML file, an appropriate choice for application context might be ClassPathXmlApplicationContext.1            These system services are commonly referred to as cross-cut¬ting concerns because they tend to cut across multiple components in a system.            Your components are littered with code that isn’t aligned with their core func¬tionality. A method that adds an entry to an address book should only be con¬cerned with how to add the address and not with whether it’s secure or transactional.       Spring seeks to eliminate boilerplate code by encapsulating it in templates. Spring’s JdbcTemplate makes it possible to perform database operations without all the ceremony required by traditional JDBC. The container is at the core of the Spring Framework. Spring’s container uses DI to manage the components that make up an application. This includes creating associa¬tions between collaborating components. As such, these objects are cleaner and easier to understand, they support reuse, and they’re easy to unit test.   There’s no single Spring container. Spring comes with several container imple¬mentations that can be categorized into two distinct types. Bean factories (defined by the org.springframework.beans.factory.BeanFactory interface) are the simplest of containers, providing basic support for DI. Application contexts (defined by the org.springframework.context.ApplicationContext interface) build on the notion of a bean factory by providing application-framework services, such as the ability to resolve textual messages from a properties file and the ability to publish application events to interested event listeners.        Although it’s possible to work with Spring using either bean factories or applica¬tion contexts, bean factories are often too low-level for most applications. Therefore, application contexts are preferred over bean factories. We’ll focus on working with application contexts and not spend any more time talking about bean factories.            As you can see, a bean factory performs several setup steps before a bean is ready to use. Let’s break down figure 1.5 in more detail: 1 Spring instantiates the bean. 2 Spring injects values and bean references into the bean’s properties. 3 If the bean implements BeanNameAware, Spring passes the bean’s ID to the set-BeanName() method. 4 If the bean implements BeanFactoryAware, Spring calls the setBeanFactory() method, passing in the bean factory itself. 5 If the bean implements ApplicationContextAware, Spring calls the set-ApplicationContext() method, passing in a reference to the enclosing appli¬cation context. 6       If the bean implements the BeanPostProcessor interface, Spring calls its post-  ProcessBeforeInitialization() method. 7       If the bean implements the InitializingBean interface, Spring calls its after-  PropertiesSet() method. Similarly, if the bean was declared with an init-method, then the specified initialization method is called. 8 If the bean implements BeanPostProcessor, Spring calls its postProcess-AfterInitialization() method. 9 At this point, the bean is ready to be used by the application and remains in the application context until the application context is destroyed. 10 If the bean implements the DisposableBean interface, Spring calls its destroy() method. Likewise, if the bean was declared with a destroy-method, the specified method is called.       Spring Boot heavily employs automatic configuration techniques that can elimi¬nate most (and in many cases, all) Spring configuration. It also provides several starter projects to help reduce the size of your Spring project build files, whether you’re using Maven or Gradle.   ·   Spring began to support Servlet 3.0, including the ability to declare servlets and filters in Java-based configuration instead of web.xml. ·   You should now have a good idea of what Spring brings to the table. Spring aims to make enterprise Java development easier and to promote loosely coupled code. Vital to this are dependency injection and aspect-oriented programming. When it comes to expressing a bean wiring specification, Spring is incredibly flexible, offering three primary wiring mechanisms: ·   Explicit configuration in XML ·   Explicit configuration in Java ·   Implicit bean discovery and automatic wiring ·   in many cases, the choice is largely a matter of personal taste, and you’re welcome to choose the approach that feels best for you. Spring attacks automatic wiring from two angles: ·   Component scanning—Spring automatically discovers beans to be created in the application context. ·   Autowiring—Spring automatically satisfies bean dependencies. Working together, component scanning and autowiring are a powerful force and can help keep explicit configuration to a minimum.   package soundsystem; public interface CompactDisc { void play(); }      The specifics of the CompactDisc interface aren’t important. What is important is that you’ve defined it as an interface. As an interface, it defines the contract through which a CD player can operate on the CD. And it keeps the coupling between any CD player implementation and the CD itself to a minimum.   package soundsystem; import org.springframework.stereotype.Component; @Component public class SgtPeppers implements CompactDisc { private String title = \"Sgt. Pepper's Lonely Hearts Club Band\"; private String artist = \"The Beatles\"; public void play() { System.out.println(\"Playing \" + title + \" by \" + artist); } }      that SgtPeppers is annotated with @Component. This simple annotation identifies this class as a component class and serves as a clue to Spring that a bean should be created for the class. Component scanning isn’t turned on by default, however. You’ll still need to write an explicit configuration to tell Spring to seek out classes annotated with @Component and to create beans from them. The configuration class in the following listing shows the minimal configuration to make this possible.   package soundsystem; import org.springframework.context.annotation.ComponentScan; import org.springframework.context.annotation.Configuration; @Configuration @ComponentScan public class CDPlayerConfig { }      you can explicitly identify any state as the starting state by setting the start-state attri¬bute in the  element: ```xml &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;   ...    Welcome to Spizza!!!         &lt;/html&gt;   - The _eventId_ portion of the button’s name is a clue to Spring Web Flow that what follows is an event that should be fired. When the form is submitted by clicking that button, a phoneEntered event is fired, triggering a transition to lookupCustomer.      Flow execution key ```xml &lt;p&gt;The address is outside of our delivery area. You may still place the order, but you will need to pick it up yourself.&lt;/p&gt; &lt;![CDATA[ &lt;a href=\"${flowExecutionUrl}&amp;_eventId=accept\"&gt; Continue, I'll pick up the order&lt;/a&gt; | &lt;a href=\"${flowExecutionUrl}&amp;_eventId=cancel\"&gt;Never mind&lt;/a&gt; 11&gt;           Note that the customerReady end state includes an  element. This ele-ment is a flow’s equivalent of Java’s return statement. It passes back some data from a subflow to the calling flow. In this case,  returns the customer flow variable so that the identifyCustomer subflow state in the pizza flow can assign it to the order. you use the  element to pass the Order in to the flow. Here you’re using it to accept that Order object. If you think of this subflow as being analo¬gous to a method in Java, the  element used here is effectively defining the subflow’s signature. This flow requires a single parameter called order.            States, transitions, and entire flows can be secured in Spring Web Flow by using the  element as a child of those elements. For example, to secure access to a view state, you might use  like this:         As configured here, access to the view state will be restricted to only users who are granted ROLE_ADMIN access (per the attributes attribute). The attributes attribute takes a comma-separated list of authorities that the user must have to gain access to the state, transition, or flow.   @RequestBody converter   Spring uses HttpMessageConverters to render @ResponseBody (or responses from @RestController).   If a bean you add is of a type that would have been included by default anyway (such as MappingJackson2HttpMessageConverter for JSON conversions), it replaces the default value. A convenience bean of type HttpMessageConverters is provided and is always available if you use the default MVC configuration. It has some useful methods to access the default and user-enhanced message converters (For example, it can be useful if you want to manually inject them into a custom RestTemplate).   On the contrary, REST has little to do with RPC. Whereas RPC is service oriented and focused on actions and verbs, REST is resource oriented, emphasizing the things and nouns that comprise an application.   Put more succinctly, REST is about transferring the state of resources—in a representational form that is most appropriate for the client or server—from a server to a client (or vice versa).   It’s a small start, but you’ll build on this controller throughout this chapter as you learn the ins and outs of Spring’s REST programming model.   Representation is an important facet of REST. It’s how a client and a server communicate about a resource. Any given resource can be represented in virtually any form. If the consumer of the resource prefers JSON, then the resource can be   Meanwhile, a human user viewing the resource in a web browser will likely prefer seeing it in HTML (or possibly PDF, Excel, or some other human-readable form). The resource doesn’t change—only how it’s represented.   Understanding how ContentNegotiatingViewResolverworks involves getting to know the content-negotiation two-step:           Determine the requested media type(s).            Find the best view for the requested media type(s).       The @ResponseBodyannotation tells Spring that you want to send the returned object as a resource to the client, converted into some representational form that the client can accept. More specifically, DispatcherServletconsiders the request’s Acceptheader and looks for a message converter that can give the client the representation it wants.   Just as @ResponseBody tells Spring to employ a message converter when sending data to a client, the @RequestBody tells Spring to find a message converter to convert a resource representation coming from a client into an object. For example, suppose that you need a way for a client to submit a new Spittle to be saved. You can write the controller method to handle such a request like this:   The body of the POST request is expected to carry a resource representation for a Spittle. Because the Spittleparameter is annotated with @RequestBody, Spring will look at the Content-Type header of the request and try to find a message converter that can convert the request body into a Spittle.   For example, if the client sent the Spittle data in a JSON representation, then the Content-Type header might be set to application/json. In that case, DispatcherServletwill look for a message converter that can convert JSON into Java objects. If the Jackson 2 library is on the classpath, then MappingJackson2Http-MessageConverterwill get the job and will convert the JSON representation into a Spittle that’s passed into the saveSpittle()method. The method is also annotated with @ResponseBodyso that the returned Spittle will be converted into a resource representation to be returned to the client.   Notice that the @RequestMappinghas a consumesattribute set to application/json. The consumesattribute works much like the producesattribute, only with regard to the request’s Content-Typeheader. This tells Spring that this method will only handle POSTrequests to /spittles if the request’s Content-Typeheader is application/json. Otherwise, it will be up to some other method (if a suitable one exists) to handle the request.   The key thing to notice in listing 16.3is what’s not in the code. Neither of the handler methods are annotated with @ResponseBody. But because the controller is annotated with @RestController, the objects returned from those methods will still go through message conversion to produce a resource representation for the client.The @ExceptionHandler annotation can be applied to controller methods to handle specific exceptions. Here, it’s indicating that if a SpittleNotFoundException is thrown from any of the handler methods in the same controller, the spittleNotFound() method should be called to handle that exception.   @ExceptionHandler(SpittleNotFoundException.class)@ResponseStatus(HttpStatus.NOT_FOUND)public @ResponseBody Error spittleNotFound(SpittleNotFoundException e) {  long spittleId = e.getSpittleId();  return new Error(4, “Spittle [” + spittleId + “] not found”);}   @ExceptionHandler(SpittleNotFoundException.class) @ResponseStatus(HttpStatus.NOT_FOUND) public @ResponseBody Error spittleNotFound(SpittleNotFoundException e) {  long spittleId = e.getSpittleId();  return new Error(4, “Spittle [” + spittleId + “] not found”); }   Because spittleNotFound() always returns an Error, the only reason to keep Response-Entity around is so you can set the status code. But by annotating spittleNotFound() with @ResponseStatus(HttpStatus.NOT_FOUND), you can achieve the same effect and get rid of ResponseEntity.   Again, if the controller class is annotated with @RestController, you can remove the @ResponseBody annotation and clean up the code a little more:   @ExceptionHandler(SpittleNotFoundException.class) @ResponseStatus(HttpStatus.NOT_FOUND) public Error spittleNotFound(SpittleNotFoundException e) {  long spittleId = e.getSpittleId();  return new Error(4, “Spittle [” + spittleId + “] not found”); }   public Spittle fetchSpittle(long id) {  RestTemplate rest = new RestTemplate();  ResponseEntity response = rest.getForEntity(    \"http://localhost:8080/spittr-api/spittles/{id}\",  Spittle.class, id);  if(response.getStatusCode() == HttpStatus.NOT_MODIFIED) {    throw new NotModifiedException();  }  return response.getBody(); }   Just like the getForEntity() method, postForEntity() returns a Response-Entity object. From that object, you can call getBody() to get the resource object (a Spitter in this case). And the getHeaders() method gives you an HttpHeaders from which you can access the various HTTP headers returned in the response. Here, you’re calling getLocation() to retrieve the Location header as a java.net.URI.   By passing in HttpMethod.GET as the HTTP verb, you’re asking exchange() to send a GET request. The third argument is for sending a resource on the request, but because this is a GET request, it can be null. The next argument indicates that you want the response converted into a Spitter object. An   Used this way, the exchange() method is virtually identical to the previously used getForEntity(). But unlike getForEntity()—or getForObject()—exchange() lets you set headers on the request sent. Instead of passing null to exchange(), you pass in an HttpEntity created with the request headers you want.   RESTful architecture uses web standards to integrate applications, keeping the interactions simple and natural. Resources in a system are identified by URLs, manipulated with HTTP methods, and represented in one or more forms suitable for the client.   Spring’s philosophy of avoiding checked exceptions, you don’t want to let the JMSException escape this method, so you’ll catch it instead.   In the catch block, you can use the convertJmsAccessException() method from Spring’s JmsUtils class to convert the checked JMSException to an unchecked JmsException. This is effectively the same thing JmsTemplate does for you in other cases.   A message-listener container is a special bean that watches a JMS destination, waiting for a message to arrive. Once a message arrives, the bean retrieves the message and passes it on to any message listeners that are interested.        JmsInvokerServiceExporter is much like those other service exporters. In fact, note that there’s some symmetry in the names of JmsInvokerServiceExporter and HttpInvokerServiceExporter. If HttpInvokerServiceExporter exports services that communicate over HTTP, then JmsInvokerServiceExporter must export services that converse over JMS.   As it turns out, AMQP offers several advantages over JMS. First, AMQP defines a wire-level protocol for messaging, whereas JMS defines an API specification. JMS’s API specification ensures that all JMS implementations can be used through a common API but doesn’t mandate that messages sent by one JMS implementation can be consumed by a different JMS implementation. AMQP’s wire-level protocol, on the other hand, specifies the format that messages will take when en route between the producer and consumer. Consequently, AMQP is more interoperable than JMS—not only across different AMQP implementations, but also across languages and platforms.   In JMS, there are just three primary participants: the message producer, the message consumer(s), and a channel (either a queue or a topic) to carry the message between producers and consumers. These essentials of the JMS messaging model are illustrated in figures 17.3 and 17.4.   In JMS, the channel helps to decouple the producer from the consumer, but both are still coupled to the channel. A producer publishes messages to a specific queue or topic, and the consumer receives those message from a specific queue or topic. The channel has the double duty of relaying messages and determining how those messages will be routed; queues route using a point-to-point algorithm, and topics route in publish/subscribe fashion.   In contrast, AMQP producers don’t publish directly to a queue. Instead, AMQP introduces a new level of indirection between the producer and any queues that will carry the message: the exchange. This relationship is illustrated in figure 17.8.   Figure 17.8. In AMQP, message producers are decoupled from message queues by an exchange that handles message routing.   For example, to have a message routed to multiple queues with no regard for the routing key, you can configure a fanout exchange and several queues like this:               As its name implies, the RabbitMQ connection factory is used to create connections with RabbitMQ. If you want to send messages via RabbitMQ, you could inject the connectionFactory bean into your AlertServiceImpl class, use it to create a Connection, use that Connection to create a Channel, and use that Channel to publish a message to an exchange.  Yep, you could do that.  you can configure different defaults using the exchange and routing-key attributes on the  element:     it was tricky to convert domain objects into Messages for sending, it’s messy to convert received Messages to domain objects. Therefore, consider using RabbitTemplate’s receiveAndConvert() method instead:  Spittle spittle =    (Spittle) rabbit.receiveAndConvert(\"spittle.alert.queue\"); Or you can leave the queue name out of the call parameters to fall back on the template’s default queue name:  The first thing you’ll need in order to consume a Spittle object asynchronously in a message-driven POJO is the POJO itself. Here’s SpittleAlertHandler, which fills that role:  package com.habuma.spittr.alerts; import com.habuma.spittr.domain.Spittle;  public class SpittleAlertHandler {   public void handleSpittleAlert(Spittle spittle) {    // ... implementation goes here ...  } }        Do you see the difference? I’ll agree that it’s not obvious. The  and  elements appear to be similar to their JMS counterparts. These elements, however, come from the rabbit namespace instead of the JMS namespace.  I said it wasn’t obvious.   Regardless of whether you handle text messages, binary messages, or both, you might also be interested in handling the establishment and closing of connections. In that case, you can override afterConnectionEstablished() and afterConnectionClosed():  public void afterConnectionEstablished(WebSocketSession session)    throws Exception {  logger.info(\"Connection established\"); }  @Override public void afterConnectionClosed(    WebSocketSession session, CloseStatus status) throws Exception {  logger.info(\"Connection closed. Status: \" + status); }    Fortunately, you don’t have to work with raw WebSocket connections. Just as HTTP layers a request-response model on top of TCP sockets, STOMP layers a frame-based wire format to define messaging semantics on top of WebSocket.  At a quick glance, STOMP message frames look very similar in structure to HTTP requests. Much like HTTP requests and responses, STOMP frames are comprised of a command, one or more headers, and a payload. For example, here’s a STOMP frame that sends data.  SEND destination:/app/marco content-length:20  {\\\"message\\\":\\\"Marco!\\\"} In this simple example, the STOMP command is SEND, indicating that something is being sent. It’s followed by two headers: one indicates the destination where the message should be sent, and the other communicates the size of the payload. Following a blank line, the frame concludes with the payload; in this case, a JSON message.  Taking advantage of Maven’s and Gradle’s transitive dependency resolution, the starters declare several dependencies in their own pom.xml file. When you add one of these starter dependencies to your Maven or Gradle build, the starter’s dependencies are resolved transitively.  Whereas Spring Boot starters cut down the size of your build’s dependency list, Spring Boot autoconfiguration cuts down on the amount of Spring configuration.  When Spring Boot’s web autoconfiguration detects Spring MVC in the classpath, it will automatically configure several beans to support Spring MVC, including view resolvers, resource handlers, and message converters (among others). All that’s left for you to do is write the controller classes to handle the requests.  Spring Boot is an exciting new addition to the Spring family of projects. Where Spring aims to make Java development simpler, Spring Boot aims to make Spring itself simpler.  Spring Boot employs two main tricks to eliminate boilerplate configuration in a Spring project: Spring Boot starters and automatic configuration.  A single Spring Boot starter dependency can replace several common dependencies in a Maven or Gradle build. For example, adding only Spring Boot’s web starter as a dependency in a project pulls in Spring’s web and Spring MVC modules as well as the Jackson 2 databind module.  Automatic configuration takes full advantage of Spring 4.0’s conditional configuration feature to automatically configure certain Spring beans to enable a certain feature. For example, Spring Boot can detect that Thymeleaf is in the application classpath and automatically configure the beans required to enable Thymeleaf templates as Spring MVC views.  In addition to the MBean info assemblers you’ve seen thus far, Spring provides another assembler known as MetadataMBeanInfoAssembler that can be configured to use annotations to appoint bean methods as managed operations and attributes. I could show you  JTA transaction management is resource-intensive; its exception handling is based on checked exceptions and so is not developer-friendly. Moreover, unit testing is hard with EJB CMT.  Most applications just need local transactions since they do not deal with multiple servers or transactional resources such as databases, JMS, and JCA; hence, they do not need a full-blown application server. For distributed transactions spanned across multiple servers over remote calls, you need JTA, necessitating an application server, as JTA needs JNDI to look up the data source. JNDI is normally available only in an application server. Use JTATransactionManager inside application servers for JTA capabilities.  TransactionDefinition defines the critical transaction attributes such as isolation, propagation, transaction timeout, and the read-only status of a given transaction instance.   Transaction attributes determine the behavior of transaction instances. They can be set programmatically as well as declaratively. Transaction attributes are:  Isolation level: Defines how much a transaction is isolated from (can see) other transactions running in parallel. Valid values are: None, Read committed, Read uncommitted, Repeatable reads, and Serializable. Read committed cannot see dirty reads from other transactions.  Propagation: Determines the transactional scope of a database operation in relation to other operations before, after, and nested inside itself. Valid values are: REQUIRED, REQUIRES_NEW, NESTED, MANDATORY, SUPPORTS, NOT_SUPPORTED, and NEVER.  Timeout: Maximum time period that a transaction can keep running or waiting before it completes. Once at timeout, it will roll back automatically.  Read-only status: You cannot save the data read in this mode.  These transaction attributes are not specific to Spring, but reflect standard transactional concepts. The TransactionDefinition interface specifies these attributes in the Spring Transaction Management context. For multiple DataSource objects or transactional resources, you need a JtaTransactionManager with JTA capabilities, which usually delegates to a container JTA provider.  If you are using Hibernate and just a single DataSource (and no other transactional resource), then the best option is to use HibernateTransactionManager, which requires you to pass the session factory as a dependency.  Spring Transactions supports two transactional modes: proxy mode and AspectJ mode. Proxy is the default and most popular mode. In proxy mode, Spring creates an AOP proxy object, wrapping the transactional beans, and applies transactional behavior transparently around the methods using transaction aspects based on the metadata. The AOP proxy created by Spring based on transactional metadata, with the help of the configured PlatformTransactionManager, performs transactions around the transactional methods.  Spring offers two convenient approaches for declaratively defining the transactional behavior of your beans:  AOP configuration for transactions in an XML metadata file Using the @Transactional annotation                                   You can see that this AOP configuration instructs Spring how to weave transactional advices around the methods using pointcuts. It instructs TransactionManager to make all find methods of the entire service layer read-only, and to force other methods to have the transaction propagation: REQUIRED, which means that, if the caller of the method is already in a transactional context, this method joins the same transaction without creating a new one; otherwise, a new transaction is created. If you want to create a different transaction for this method, you should use the REQUIRES_NEW propagation. Also, note that the transaction isolation level is specified as DEFAULT, which means the default isolation of the database is to be used. Most databases default to READ_COMMITTED, which means a transactional thread cannot see the data of other transactions in progress (dirty reads).  Note @Transactional can be applied only to public methods. If you want to annotate over protected, private, or package-visible methods, consider using AspectJ, which uses compile-time aspect weaving. Spring recommends annotating @Transactional only on concrete classes as opposed to interfaces, as it will not work in most cases such as when you use proxy-target-class=\"true\" or mode=\"aspectj\".  You need to first enable transaction management in your application before Spring can detect the @Transactional annotation for your bean methods. You enable transaction in your XML metadata using the following notation:   The following is the Java configuration alternative for the preceding listing:  @Configuration @EnableTransactionManagement public class JpaConfiguration { } Spring scans the application context for bean methods annotated with @Transactional when it sees either of the preceding settings.   Implementing InitializingBean and DisposableBean  The Spring IoC container invokes the callback methods afterPropertiesSet() of org.springframework.beans.factory.InitializingBean and destroy() of org.springframework.beans.factory.DisposableBean on any Spring bean and implements them:  There are different bean scopes in Spring, such as singleton, prototype, request, session, and global session. We will understand each session one by one.  By default, all Spring beans are singleton. Once ApplicationContext is initialized, it looks at all the beans in XML and initializes only one bean per bean definition in Spring Container. On each call to the getBean() method, Spring Container returns the same instance of the bean.  Prototype  The prototype is second bean scope in Spring, which returns a brand-new instance of a bean on each call to the getBean() method. When a bean is defined as a prototype, Spring waits for getBean() to happen and only then does it initialize the prototype.   Spring doesn't maintain the complete life cycle of the prototype. Here, the container instantiates and configures prototype beans and returns this bean to the client with no further record of this prototype instance.   Request  The third bean scope in Spring is request, which is available only in web applications that use Spring and create an instance of bean for every HTTP request. Here, a new bean is created per Servlet request. Spring will be aware of when a new request is happening because it ties well with the Servlet APIs, and depending on the request, Spring creates a new bean. So, if the reque  Session  The session is the fourth bean scope in Spring, which is available only in web applications that use Spring and create an instance of bean for every HTTP session. Here, a new bean is created per session. As long as there is one user accessing in a single session, each call to getBean() will return same instance of the bean.     Global session  The global session is the fifth bean scope in Spring, which works only in portlet environments that use Spring and create a bean for every new portlet session.  Spring's BeanFactory manages the life cycle of beans created through the Spring IoC container. The life cycle of beans consist of callback methods, which can be categorized broadly into the following two groups:  Post-initialization callback methods Pre-destruction callback methods    Initialization  It represents a sequence of activities that take place between the bean instantiation and the handover of its reference to the client application:  The bean container finds the definition of the Spring bean in the configuration file and creates an instance of the bean If any properties are mentioned, populate the properties using setters If the Bean class implements the BeanNameAware interface, then call the setBeanName() method If the Bean class implements the BeanFactoryAware interface, then call the setBeanFactory() method If the Bean class implements the ApplicationContextAware interface, then call the setApplicationContext() method If there are any BeanPostProcessors objects associated with the BeanFactory interface that loaded the bean, then Spring will call the postProcessBeforeInitialization() method before the properties for the bean are injected If the Bean class implements the InitializingBean interface, then call the afterPropertiesSet() method once all the bean properties defined in the configuration file are injected If the bean definition in the configuration file contains the init-method attribute, then call this method after resolving the value for the attribute to a method name in the Bean class The postProcessAfterInitialization() method will be called if there are any bean post processors attached to the BeanFactory interface that loads the bean   Destruction  This represents the following sequence of activities:  If the Bean class implements the DisposableBean interface, then call the destroy() method when the application no longer needs the bean reference If the bean definition in the configuration file contains the destroy-method attribute, then call this method after resolving the value for the attribute to a method name in the Bean class.   The InitializingBean interface has afterPropertiesSet(), which needs to be implemented, and it will be called by Spring when this bean is initialized and all properties are set. This InitializingBean interface is a marker for the bean to know that the afterPropertiesSet() method of this bean needs to be called after initialization.   Using init-method in the XML configuration In the case of XML-based configuration metadata, you can use the init-method attribute to specify the name of the method that has a void no-argument signature, which is to be called on the bean immediately upon instantiation.  In the beans.xml file, you'll find the following code:  ...   ... In the EmployeeServiceImpl.java class, you'll find the following code:      ","categories": [],
        "tags": ["java","spring"],
        "url": "/2017/03/05/Spring.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Oracle",
        "excerpt":"   ORA-12899: Value Too Large for Column   SQL&gt; SELECT value$ FROM sys.props$ WHERE name = 'NLS_CHARACTERSET' ;  VALUE$ -------------------------------------------------------------------------  AL32UTF8  SELECT * FROM NLS_DATABASE_PARAMETERS  select user from dual;    u43888859@hkl105482$  lsnrctl services DHKCUSTD  LSNRCTL for Linux: Version 12.1.0.2.0 - Production on 28-FEB-2017 06:40:53  Copyright (c) 1991, 2014, Oracle.  All rights reserved.  Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=hkl105482.hk.hsbc)(PORT=2001))(CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=DHKCUSTD.hk.hsbc))) Services Summary... Service \"DHKCUSTD.hk.hsbc\" has 1 instance(s).   Instance \"DHKCUSTD\", status READY, has 1 handler(s) for this service...     Handler(s):       \"DEDICATED\" established:15 refused:0 state:ready          LOCAL SERVER The command completed successfully    ","categories": [],
        "tags": ["database"],
        "url": "/2017/03/13/oracle.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "AWS Tips",
        "excerpt":"After establishing a SSH session, you can install a default web server by executing sudo yum install httpd -y. To start the web server, type sudo service httpd start and press Return to execute the command. Your web browser should show a placeholder site if you open http://$PublicIp with $PublicIp replaced by the public IP address of your virtual server.  ","categories": [],
        "tags": ["AWS","Cloud"],
        "url": "/2017/03/21/aws.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Setup WebSphere profiles and application in command line",
        "excerpt":"Setup WebSphere profiles and application in command line   Background &amp; descriptions      Beginning with V8.5, WebSphere Application Server provides two runtime profiles. Every WebSphere Application Server package includes both profile types.            Full WebSphere Application Server       Liberty profile           What’s profile?          Simply put, a profile contains an Application Server. When an Application Server is running, the server process may read and write data to the underlying configuration files and logs. So, by using profiles, transient data is kept away from the base product. This allows us to have more than one profile using the same base binaries, and also allows us to remove certain profiles without affecting other profiles. Another reason for separating the base binaries is that we can upgrade the product with maintenance updates and fix packs without having to re-create all profiles. Sometimes you do not want a specific profile to be updated. WAS profile management has been designed for flexibility.            WAS has the ability to have multiple application server definitions using the same underlying base binaries. Each profile defines the attributes and configurations for a given application server. Each standalone application server can optionally have its own administrative console application, which you use to manage the application server. We will cover how to install a profile later in the chapter.            On distributed platforms, profiles are created after you install the product by using either the Profile Management Tool or the manageprofiles command.       WAS Concepts  Nodes  A node is an administrative grouping of application servers for configuration and operational management within one operating system instance. You can create multiple nodes inside one operating system instance, but a node cannot leave the operating system boundaries.   A stand-alone application server configuration has only one node. With Network Deployment, you can configure a distributed server environment that consists of multiple nodes that are managed from one central administration server.   Node agents  In distributed server configurations, each node has a node agent that works with the deployment manager to manage administration processes. A node agent is created automatically when you add (federate) a stand-alone application server node to a cell. Node agents are not included in the Base and Express configurations because a deployment manager is not needed in these architectures.   The node agent is an administrative server that runs on the same system as the node. It monitors the application servers on that node, routing administrative requests from the deployment manager to those application servers.   Node groups  A node group is a collection of nodes within a cell that have similar capabilities in terms of installed software, available resources, and configuration. A node group is used to define a boundary for server cluster formation so that the servers on the same node group host the same applications. A DefaultNodeGroup is created automatically. The DefaultNodeGroup contains the deployment manager and any new nodes with the same platform type. A node can be a member of more than one node group.   Cells  A cell is a grouping of nodes into a single administrative domain. A cell encompasses the entire management domain. In the Base and Express configurations, a cell contains one node, and that node contains one server. The left side of Figure 3-11 illustrates a system with two cells that are each accessed by their own administrative console. Each cell has a node and a stand-alone application server. In a Network Deployment environment (the right side of Figure 3-11), a cell can consist of multiple nodes and node groups. These nodes and groups are all administered from a single point, the deployment manager. Figure 3-11 shows a single cell that spans two systems that are accessed by a single administrative console. The deployment manager is administering the nodes.   A cell configuration that contains nodes that are running on the same operating system is called a homogeneous cell. It is also possible to configure a cell that consists of nodes on mixed operating systems. With this configuration, other operating systems can exist in the same WebSphere Application Server cell.   For example, z/OS nodes, Linux nodes, UNIX nodes, and Windows system nodes can exist in the same WebSphere Application Server cell. This configuration is called a heterogeneous cell. A heterogeneous cell requires significant planning.   Noteworthy points   Tools/utilities     create profile     /opt/IBM/WebSphere85/AppServer/bin/manageprofiles.sh           check server status     /opt/IBM/WebSphere85/AppServer/bin/serverStatus.sh -all           start server     /opt/IBM/WebSphere85/AppServer/bin/startServer.sh SERVER_NAME           start server     /opt/IBM/WebSphere85/AppServer/bin/stopServer.sh dmgr           deploy application     sudo -u wasadm /opt/IBM/WebSphere85/utilities/API/v1.0/AppMgmt -deploy -deployMechanism simpleDeploy -appId xxx_war -appEnvId xxx -file \"/tmp/xxx.ear\"           stop&amp;start application     sudo -u wasadm /opt/IBM/WebSphere85/utilities/API/v1.0/AppMgmt -operate -appId xxx_war -appEnvId xxx -stop sudo -u wasadm /opt/IBM/WebSphere85/utilities/API/v1.0/AppMgmt -operate -appId xxx_war -appEnvId xxx -start           To create profile  sudo -u wasadm /opt/IBM/WebSphere85/AppServer/bin/manageprofiles.sh -create -profileName TEST_PROFILE -profilePath /opt/IBM/WebSphere85/AppServer/profiles/appprofiles/TEST_PROFILE -templatePath /opt/IBM/WebSphere85/AppServer/profileTemplates/default -serverName testSrv01 -nodeName testNode01 -hostName testserver.com -enableAdminSecurity true -adminUserName wasadmin -adminPassword wasadmin@12   Errors &amp; troubleshooting   Server can’t started after profile creation     You may find following errors if you try to bring up server after profile created     sudo -u wasadm /opt/IBM/WebSphere85/AppServer/bin/startServer.sh TEST_PROFILE ADMU0116I: Tool information is being logged in file          /opt/IBM/WebSphere85/AppServer/profiles/dmgrprofile/logs/TEST_PROFILE/startServer.log ADMU0128I: Starting tool with the dmgrprofile profile ADMU3100I: Reading configuration for server: TEST_PROFILE ADMU0111E: Program exiting with error: java.io.FileNotFoundException:          /opt/IBM/WebSphere85/AppServer/profiles/dmgrprofile/config/cells/wascell/nodes/dmgrnode/servers/TEST_PROFILE/server.xml           Solution:     Please use the server name, rather than the profile name , e.g. should be testSrv01, rather than TEST_PROFILE   The command should be:     sudo -u wasadm /opt/IBM/WebSphere85/AppServer/bin/startServer.sh testSrv01           On the other hand, you can troubleshoot this issue by double checking the server.xml by     ll /opt/IBM/WebSphere85/AppServer/profiles/appprofiles/TEST_PROFILE/config/cells/xxxxx/nodes/testNode01/servers/testSrv01/server.xml           WebServer console can’t opened     There are few reasones, such as:     DNS errors       If the error as below: This webpage is not available The server at testserver.com can’t be found, because the DNS lookup failed. DNS is the network service that translates a website’s name to its Internet address.     Solutions:      That’s maybe something wrong for DNS as stated above, so please try to either udpate hosts file in you workstation or repalce the URL by IP address, e.g.  replace https://testserver.com:9045/ibm/console/login.do?action=secure with https://123.123.112.123:9045/ibm/console/login.do?action=secure       What’s the port number for WAS console  You should get to know it after profile created  For example, in aforesaid profile creation step, you’ll get following in output:  INSTCONFSUCCESS: Success: Profile TEST_PROFILE now exists. Please consult /opt/IBM/WebSphere85/AppServer/profiles/TEST_PROFILE/logs/AboutThisProfile.txt for more information about this profile.  To check content of this file, you’ll find following snipets:  Administrative console port: 9060 Administrative console secure port: 9043  Check the port number from serverindex.xml  grep -a2 WC_adminhost /opt/IBM/WebSphere85/AppServer/profiles/appprofiles/TEST_PROFILE/config/cells/xxxx/nodes/testNode01/serverindex.xml  You’ll find the port number listed as below:   &lt;endPoint xmi:id=\"EndPoint_1183122129645\" host=\"testserver.com\" port=\"9407\"/&gt;     &lt;/specialEndpoints&gt;     &lt;specialEndpoints xmi:id=\"NamedEndPoint_1183122129646\" endPointName=\"WC_adminhost\"&gt;       &lt;endPoint xmi:id=\"EndPoint_1183122129646\" host=\"*\" port=\"9062\"/&gt;     &lt;/specialEndpoints&gt; --       &lt;endPoint xmi:id=\"EndPoint_1183122129648\" host=\"*\" port=\"9355\"/&gt;     &lt;/specialEndpoints&gt;     &lt;specialEndpoints xmi:id=\"NamedEndPoint_1183122129649\" endPointName=\"WC_adminhost_secure\"&gt;       &lt;endPoint xmi:id=\"EndPoint_1183122129649\" host=\"*\" port=\"9045\"/&gt;     &lt;/specialEndpoints&gt;   Check networking &amp; port on WAS server   root@cn000tst1129 Thu Mar 30 17:07:11 /  #netstat -nptlev | grep java tcp        0      0 133.14.16.2:20962           0.0.0.0:*                   LISTEN      200000000  938007     252552/java tcp        0      0 127.0.0.1:9635              0.0.0.0:*                   LISTEN      200000000  1048485    254902/java tcp        0      0 0.0.0.0:9445                0.0.0.0:*                   LISTEN      200000000  1048615    254902/java tcp        0      0 0.0.0.0:9062                0.0.0.0:*                   LISTEN      200000000  1048613    254902/java tcp        0      0 0.0.0.0:9102                0.0.0.0:*                   LISTEN      200000000  1048479    254902/java tcp        0      0 0.0.0.0:8882                0.0.0.0:*                   LISTEN      200000000  1048484    254902/java tcp        0      0 0.0.0.0:9045                0.0.0.0:*                   LISTEN      200000000  1048616    254902/java tcp        0      0 0.0.0.0:20950               0.0.0.0:*                   LISTEN      200000000  938036     252552/java tcp        0      0 0.0.0.0:20953               0.0.0.0:*                   LISTEN      200000000  937999     252552/java tcp        0      0 0.0.0.0:9082                0.0.0.0:*                   LISTEN      200000000  1048614    254902/java tcp        0      0 0.0.0.0:20954               0.0.0.0:*                   LISTEN      200000000  938118     252552/java tcp        0      0 0.0.0.0:2811                0.0.0.0:*                   LISTEN      200000000  1048482    254902/java tcp        0      0 0.0.0.0:20957               0.0.0.0:*                   LISTEN      200000000  937992     252552/java tcp        0      0 0.0.0.0:9407                0.0.0.0:*                   LISTEN      200000000  1048481    254902/java tcp        0      0 0.0.0.0:20959               0.0.0.0:*                   LISTEN      200000000  938088     252552/java tcp        0      0 0.0.0.0:9408                0.0.0.0:*                   LISTEN      200000000  1048480    254902/java tcp        0      0 127.0.0.1:20960             0.0.0.0:*                   LISTEN      200000000  938089     252552/java tcp        0      0 133.14.16.2:20961           0.0.0.0:*                   LISTEN      200000000  938047     252552/java  ","categories": [],
        "tags": ["WAS","WebSphere"],
        "url": "/2017/03/30/WebSphere-setp-via-CLI.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "SSH SFTP",
        "excerpt":"Secure FTP     SFTP over FTP is the equivalant of HTTPS over HTTP, the security version    SFTP versus FTPS     An increasing number of our customers are looking to move away from standard FTP for transferring data, so we are often asked which secure FTP protocol we recommend. In the next few paragraphs, we will explain what options are available and their main differences.   The two mainstream protocols available for Secure FTP transfers are named SFTP (FTP over SSH) and FTPS (FTP over SSL). Both SFTP and FTPS offer a high level of protection since they implement strong algorithms such as AES and Triple DES to encrypt any data transferred. Both options also support a wide variety of functionality with a broad command set for transferring and working with files. So the most notable differences between SFTP and FTPS is how connections are authenticated and managed.   Authentication: SFTP vs. FTPS     With SFTP (FTP over SSH), a connection can be authenticated using a couple different techniques.  For basic authentication, you (or your trading partner) may just require a user id and password to connect to the SFTP server. Its important to note that any user ids and passwords supplied over the SFTP connection will be encrypted, which is a big advantage over standard FTP.   SSH keys can also be used to authenticate SFTP connections in addition to, or instead of, passwords. With key-based authentication, you will first need to generate a SSH private key and public key beforehand. If you need to connect to a trading partner’s SFTP server, you would send your SSH public key to them, which they will load onto their server and associate with your account. When you connect to their SFTP server, your client software will transmit your public key to the server for authentication. If the keys match, along with any user/password supplied, then the authentication will succeed.   With FTPS (FTP over SSL), a connection is authenticated using a user id, password and certificate(s).  Like SFTP, the users and passwords for FTPS connections will also be encrypted. When connecting to a trading partner’s FTPS server, your FTPS client will first check if the server’s certificate is trusted. The certificate is considered trusted if either the certificate was signed off by a known certificate authority (CA), like Verisign, or if the certificate was self-signed (by your partner) and you have a copy of their public certificate in your trusted key store.   Your partner may also require that you supply a certificate when you connect to them.  Your certificate may be signed off by a 3rd party CA or your partner may allow you to just self-sign your certificate, as long as you send them the public portion of your certificate beforehand (which they will load in their trusted key store).   Implementation: SFTP vs. FTPS     In regards to how easy each of the secure FTP protocols are to implement, SFTP is the clear winner since it is very firewall friendly. SFTP only needs a single port number (default of 22) to be opened through the firewall.  This port will be used for all SFTP communications, including the initial authentication, any commands issued, as well as any data transferred.   On the other hand, FTPS can be very difficult to patch through a tightly secured firewall since FTPS uses multiple port numbers. The initial port number (default of 21) is used for authentication and passing any commands.  However, every time a file transfer request (get, put) or directory listing request is made, another port number needs to be opened.  You and your trading partners will therefore have to open a range of ports in your firewalls to allow for FTPS connections, which can be a security risk for your network.   In summary, SFTP and FTPS are both very secure with strong authentication options.  However since SFTP is much easier to port through firewalls, and we are seeing an increasing percentage of trading partners adopting SFTP, we believe SFTP is the clear winner for your secure FTP needs.   SSH     There are several ways to use SSH; one is to use automatically generated public-private key pairs to simply encrypt a network connection, and then use password authentication to log on.   Another is to use a manually generated public-private key pair to perform the authentication, allowing users or programs to log in without having to specify a password. In this scenario, anyone can produce a matching pair of different keys (public and private). The public key is placed on all computers that must allow access to the owner of the matching private key (the owner keeps the private key secret). While authentication is based on the private key, the key itself is never transferred through the network during authentication. SSH only verifies whether the same person offering the public key also owns the matching private key. In all versions of SSH it is important to verify unknown public keys, i.e. associate the public keys with identities, before accepting them as valid. Accepting an attacker’s public key without validation will authorize an unauthorized attacker as a valid user.   Key management     On Unix-like systems, the list of authorized public keys is typically stored in the home directory of the user that is allowed to log in remotely, in the file ~/.ssh/authorized_keys.   WinFTP     cryptographic protocol is SSH-2   SSH implementation is OpenSSH_5.3   Server fingerprint: File transfer protocol = SFTP-3 Cryptographic protocol = SSH-2 SSH implementation = OpenSSH_5.3 Encryption algorithm = aes Compression = No ———————————————————— Server host key fingerprint ssh-rsa 2048 86:54:d9:09:25:c0:9b:f8:17:8c:c0:52:13:0c:9c:cc ————————————————————   References     https://www.goanywhere.com/blog/2011/10/20/sftp-ftps-secure-ftp-transfers   https://en.wikipedia.org/wiki/Secure_Shell  ","categories": [],
        "tags": ["SFTP","SSH"],
        "url": "/2017/04/06/SSH.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "JDK source",
        "excerpt":"interface RandomAccess  Marker interface used by List implementations to indicate that they support fast (generally constant time) random access. The primary purpose of this interface is to allow generic algorithms to alter their behavior to provide good performance when applied to either random or sequential access lists.   Such a List implementation should generally implement this interface. As a rule of thumb, a List implementation should implement this interface if, for typical instances of the class, this loop:          for (int i=0, n=list.size(); i &lt; n; i++)            list.get(i);     // runs faster than this loop:        for (Iterator i=list.iterator(); i.hasNext(); )            i.next();    ","categories": [],
        "tags": ["java"],
        "url": "/2017/06/24/JDK-sources.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "google analysis",
        "excerpt":"How Page Value is calculated   At a glance  Page Value is the average value for a page that a user visited before landing on the goal page or completing an Ecommerce transaction (or both). This value is intended to give you an idea of which page in your site contributed more to your site’s revenue. If the page wasn’t involved in an ecommerce transaction for your website in any way, then the Page Value for that page will be $0 since the page was never visited in a session where a transaction occurred.   Below is the equation you can follow to calculate Page Value. Please note that the unique pageview statistic represents the number of individual users who have loaded a given page per session. Each user is counted only once per session, no matter how many pages are opened by the same user.   Ecommerce Revenue + Total Goal Value Number of Unique Pageviews for Given Page   In depth   The first example above illustrates how Page Value works. Let’s say you want to know the Page Value for Page B, and you know the following factors:   Goal page D: $10 (Remember, you assign the value of the Goal page when you first create a goal in the Analytics Settings page) Receipt Page E: $100 (This page is where the user makes an ecommerce transaction of $100) Unique pageview for Page B: One   You would then set up your Page Value equation like this:   Ecommerce Revenue ($100) + Total Goal Value ($10)  Number of Unique Pageviews for Page B (1)   Page Value for Page B is $110 since a user visits Page B only once before the goal page during this session.   Now let’s explore how Page Value for Page B is affected when we combine the data from two different sessions. You can see that Page B is viewed only once during Session 1, but during Session 2 it gets two pageviews (we’re assuming the two pageviews are from the same user). The total Ecommerce revenue stays the same during both sessions. Although there were two unique pageviews, there was still only one Ecommerce transaction total for both sessions.   Goal page D: $10 Receipt Page E: $100 Unique pageview for Page B: Two   Your Page Value calculation should be adjusted to look like this:   Ecommerce Revenue ($100) + Total Goal Value ($10 x 2 sessions)  Number of Unique Pageviews for Page B (2)   Page Value for Page B across two sessions is then $60, or $120 divided by two sessions.   Code snippet about setting up GA in your site   ```html                        var hostName = window.location.host;                             var ga_script = document.createElement('script');                                     ga_script.setAttribute('async',true);                                             if (['localhost:9000','test','uat'].some(function(hostItem){return hostName.indexOf(hostItem)&gt;-1})) {                                                             // non-production                                                                         ga_script.setAttribute('src','https://www.google-analytics.com/analytics_debug.js');                                                                                     window.ga_debug = {trace: true};                                                                                             }else {                                                                                                             ga_script.setAttribute('src','https://www.google-analytics.com/analytics.js');                                                                                                                     }                                                                                                                             document.head.appendChild(ga_script);                                                                                                                                     ga('send', 'pageview');                                                                                                                                         &lt;/script&gt;                                                                                                                                         ```                                                                                                                                          ```javascript                                                                                                                                         $rootScope.$on('$stateChangeSuccess', function() {                                                                                                                                                             $window.ga('send', 'pageview', $location.path());                                                                                                                                                                             $window.scrollTo(0, 0);                                                                                                                                                                                         });                                                                                                                                         ```                                                                                                                                           Alternative async tracking snippet                                                                                                                                         While the JavaScript tracking snippet described above ensures the script will be loaded and executed asynchronously on all browsers, it has the disadvantage of not allowing modern browsers to preload the script.                                                                                                                                         The alternative async tracking snippet below adds support for preloading, which will provide a small performance boost on modern browsers, but can degrade to synchronous loading and execution on IE 9 and older mobile browsers that do not recognize the async script attribute. Only use this tracking snippet if your visitors primarily use modern                                                                                                                                         browsers to access your site.                                                                                                                                          &lt;!-- Google Analytics --&gt;                                                                                                                                         &lt;script&gt;                                                                                                                                         window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;                                                                                                                                         ga('create', 'UA-XXXXX-Y', 'auto');                                                                                                                                         ga('send', 'pageview');                                                                                                                                         &lt;/script&gt;                                                                                                                                         &lt;script async src='https://www.google-analytics.com/analytics.js'&gt;&lt;/script&gt;                                                                                                                                         &lt;!-- End Google Analytics --&gt;                                                                                                                                          From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/&gt;                                                                                                                                          What data does the tracking snippet capture?                                                                                                                                         When you add either of these tracking snippets to your website, you send a pageview for each page your users visit. Google Analytics processes this data and can infer a great deal of information including:                                                                                                                                         • The total time a user spends on your site.                                                                                                                                         • The time a user spends on each page and in what order those pages were visited.                                                                                                                                         • What internal links were clicked (based on the URL of the next pageview).                                                                                                                                         In addition, the IP address, user agent string, and initial page inspection analytics.js does when creating a new tracker is used to determine things like the following:                                                                                                                                         • The geographic location of the user.                                                                                                                                         • What browser and operating system are being used.                                                                                                                                         • Screen size and whether Flash or Java is installed.                                                                                                                                         • The referring site.                                                                                                                                          From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/#alternative_async_tracking_snippet&gt;                                                                                                                                            How analytics.js Works                                                                                                                                          From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/how-analyticsjs-works&gt;                                                                                                                                           The ga command queue                                                                                                                                         The JavaScript tracking snippet defines a global ga function known as the \"command queue\". It's called the command queue because rather than executing the commands it receives immediately, it adds them to a queue that delays execution until the analytics.js library is fully loaded.                                                                                                                                         In JavaScript, functions are also objects, which means they can contain properties. The tracking snippet defines a q property on the ga function object as an empty array. Prior to the analytics.js library being loaded, calling the ga() function appends the list of arguments passed to the ga()function to the end of the q array.                                                                                                                                         For example, if you were to run the tracking snippet and then immediately log the contents of ga.qto the console, you'd see an array, two items in length, containing the two sets of arguments already passed to the ga() function:                                                                                                                                          console.log(ga.q);                                                                                                                                          // Outputs the following:                                                                                                                                         // [                                                                                                                                         //   ['create', 'UA-XXXXX-Y', 'auto'],                                                                                                                                         //   ['send', 'pageview']                                                                                                                                         // ]                                                                                                                                         Once the analytics.js library is loaded, it inspects the contents of the ga.q array and executes each command in order. After that, the ga() function is redefined, so all subsequent calls execute immediately.                                                                                                                                         This pattern allows developers to use the ga() command queue without having to worry about whether or not the analytics.js library has finished loading. It provides a simple, synchronous-looking interface that abstracts away most of the complexities of asynchronous code.                                                                                                                                          From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/how-analyticsjs-works&gt;                                                                                                                                           Adding commands to the queue                                                                                                                                         All calls to the ga() command queue share a common signature. The first parameter, the \"command\", is a string that identifies a particular analytics.js method. Any additional parameters are the arguments that get passed to that method.                                                                                                                                         The method a particular command refers to can be a global method, like create, a method on the ga object, or it can be an instance method on a tracker object, like send. If the ga() command queue receives a command it doesn't recognize, it simply ignores it, making calls to the ga()function very safe, as they will almost never result in an error.                                                                                                                                         For a comprehensive list of all commands that can be executed via the command queue, see the ga() command queue reference.                                                                                                                                          From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/how-analyticsjs-works&gt;                                                                                                                                           Command parameters                                                                                                                                         Most analytics.js commands (and their corresponding methods) accept parameters in a number of different formats. This is done as a convenience to make it easier to pass commonly used fields to certain methods.                                                                                                                                         As an example, consider the two commands in the JavaScript tracking snippet:                                                                                                                                          ga('create', 'UA-XXXXX-Y', 'auto');                                                                                                                                         ga('send', 'pageview');                                                                                                                                         In the first command, create accepts the fields trackingId, cookieDomain, and name to optionally be specified as the second, third, and fourth parameters, respectively. The sendcommand accepts an optional hitType second parameter.                                                                                                                                         All commands accept a final fieldsObject parameter that can be used to specify any fields as well. For example, the above two commands in the tracking snippet could be rewritten as:                                                                                                                                          ga('create', {                                                                                                                                               trackingId: 'UA-XXXXX-Y',                                                                                                                                                 cookieDomain: 'auto'                                                                                                                                         });                                                                                                                                         ga('send', {                                                                                                                                               hitType: 'pageview'                                                                                                                                         });                                                                                                                                         See the ga() command queue reference for a comprehensive list of the optional parameters allowed for each of the commands.                                                                                                                                          From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/how-analyticsjs-works&gt;                                                                                                                                            Creating Trackers                                                                                                                                         • Contents                                                                                                                                         • The create method                                                                                                                                         • Naming trackers                                                                                                                                         • Specifying fields at creation time                                                                                                                                         • Working with multiple trackers                                                                                                                                         • Running commands for a specific tracker                                                                                                                                         • Next steps                                                                                                                                         Tracker objects (also known as \"trackers\") are objects that can collect and store data and then send that data to Google Analytics.                                                                                                                                         When creating a new tracker, you must specify a tracking ID (which is the same as the property ID that corresponds to one of your Google Analytics properties) as well as a cookie domain, which specifies how cookies are stored. (The recommended value 'auto' specifies automatic cookie domain configuration.)                                                                                                                                         If a cookie does not exist for the specified domain, a client ID is generated and stored in the cookie, and the user is identified as new. If a cookie exists containing a client ID value, that client ID is set on the tracker, and the user is identified as returning.                                                                                                                                         Upon creation, tracker objects also gather information about the current browsing context such as the page title and URL, and information about the device such as screen resolution, viewport size, and document encoding. When it's time to send data to Google Analytics, all of the information currently stored on the tracker gets sent.                                                                                                                                          From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/creating-trackers&gt;                                                                                                                                           Running commands for a specific tracker                                                                                                                                         To run analytics.js commands for a specific tracker, you prefix the command name with the tracker name, followed by a dot. When you don't specify a tracker name, the command is run on the default tracker.                                                                                                                                         To send pageviews for the above two trackers, you'd run the following two commands:                                                                                                                                          ga('send', 'pageview');                                                                                                                                         ga('clientTracker.send', 'pageview');                                                                                                                                         Future guides will go into more detail on the syntax for running specific commands. You can also refer to the command queue reference to see the full command syntax for all analytics.js commands.                                                                                                                                          From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/creating-trackers&gt;                                                                                                                                           Getting trackers via ga Object methods                                                                                                                                         If you're not using a default tracker, or if you have more than one tracker on the page, you can access those trackers via one of the ga object methods.                                                                                                                                         Once the analytics.js library is fully loaded, it adds additional methods to the ga object itself. Two of those methods, getByName and getAll, are used to access tracker objects.                                                                                                                                         Note: ga object methods are only available when analytics.js has fully loaded, so you should only reference them inside a ready callback.                                                                                                                                         getByName                                                                                                                                         If you know the name of the tracker you want to access, you can do so using the getByNamemethod:                                                                                                                                          ga('create', 'UA-XXXXX-Y', 'auto', 'myTracker');                                                                                                                                          ga(function() {                                                                                                                                               // Logs the \"myTracker\" tracker object to the console.                                                                                                                                                 console.log(ga.getByName('myTracker'));                                                                                                                                         });                                                                                                                                          From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/accessing-trackers&gt;                                                                                                                                           The last line of the JavaScript tracking snippet adds a send command to the ga() command queue to send a pageview to Google Analytics:                                                                                                                                          ga('create', 'UA-XXXXX-Y', 'auto');                                                                                                                                         ga('send', 'pageview');                                                                                                                                         The object that is doing the sending is the tracker that was scheduled for creation in the previous line of code, and the data that gets sent is the data stored on that tracker.                                                                                                                                         This guide describes the various ways to send data to Google Analytics and explains how to control what data gets sent.                                                                                                                                          From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/sending-hits&gt;                                                                                                                                           Hits, hit types, and the Measurement Protocol                                                                                                                                         When a tracker sends data to Google Analytics it's called sending a hit, and every hit must have a hit type. The JavaScript tracking snippet sends a hit of type pageview; other hit types include screenview, event, transaction, item, social, exception, and timing. This guide outlines the concepts and methods common to all hit types. Individual guides                                                                                                                                         for each hit type can be found under the section Tracking common user interactions in the left-side navigation.                                                                                                                                             The hit is an HTTP request, consisting of field and value pairs encoded as a query string, and sent to the Measurement Protocol.                                                                                                                                              From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/sending-hits&gt;                                                                                                                                               The simplest way to use the send command, that works for all hit types, is to pass all fields using the fieldsObjectparameter. For example:                                                                                                                                              ga('send', {                                                                                                                                                   hitType: 'event',                                                                                                                                                     eventCategory: 'Video',                                                                                                                                                       eventAction: 'play',                                                                                                                                                         eventLabel: 'cats.mp4'                                                                                                                                             });                                                                                                                                             For convenience, certain hit types allow commonly used fields to be passed directly as arguments to the sendcommand. For example, the above send command for the \"event\" hit type could be rewritten as:                                                                                                                                              ga('send', 'event', 'Video', 'play', 'cats.mp4');                                                                                                                                             For a complete list of what fields can be passed as arguments for the various hit types, see the \"parameters\" section of the send method reference.                                                                                                                                               From &lt;https://developers.google.com/analytics/devguides/collection/analyticsjs/sending-hits&gt;    ","categories": [],
        "tags": [],
        "url": "/2017/09/04/Google-Analytics.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "TypeScript noteworthy notes",
        "excerpt":"Async Await keywords  Async Await Support in TypeScript Async - Await has been supported by TypeScript since version 1.7. Asynchronous functions are prefixed with the async keyword; await suspends the execution until an asynchronous function return promise is fulfilled and unwraps the value from the Promise returned. It was only supported for target es6 transpiling directly to ES6 generators.   Troubleshotting   Unexpected token …   That’s because your node version is lower (e.g. node v4.x), which don’t support spread operator. You’d firstly check your node version  node -v   If above result say node is v4.x, then you should run following commands to upgrade your node. Normally you can leverage Node Package Manager n as below:  sudo npm install -g n sudo n stable   ","categories": [],
        "tags": ["Angular","JavaScript"],
        "url": "/2017/11/27/TypeScript.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Openshift tips",
        "excerpt":"Commands bible   install cli in Mac  brew install openshift-cli   Frequently used commands   Start mini shift  minishift start   OC commands  oc config view   Start a new application  oc new-app https://github.com/openshift/nodejs-ex -l name=toddapp   To switch project  oc project    Show a high level overview of the current project  oc status # Export the overview of the current project in an svg file.   oc status -o dot | dot -T svg -o project.svg      # See an overview of the current project including details for any identified issues.   oc status -v   This command will show services, deployment configs, build configurations, and active deployments. If you have any misconfigured components information about them will be shown. For more information about individual items, use the describe command (e.g. oc describe buildConfig, oc describe deploymentConfig, oc describe service).   You can specify an output format of “-o dot” to have this command output the generated status graph in DOT format that is suitable for use by the “dot” command.   OpenShift command-line tool  The OpenShift command-line tool oc is the primary way most users interact with OpenShift. The command-line tool talks via a REST API exposed by the OpenShift cluster.   Pod  The most basic unit in OpenShift are pods. A pod is one or more containers guaranteed to be running on the same host. The containers within a pod share a unique IP address. They can communicate with each other via the “localhost” and also all share any volumes (persistent storage). The containers themselves are started from an image, which in our case is a Docker image.   OpenShift Origin leverages the Kubernetes concept of a pod, which is one or more containers deployed together on one host, and the smallest compute unit that can be defined, deployed, and managed.   Pods are the rough equivalent of a machine instance (physical or virtual) to a container. Each pod is allocated its own internal IP address, therefore owning its entire port space, and containers within pods can share their local storage and networking.   Pods have a lifecycle; they are defined, then they are assigned to run on a node, then they run until their container(s) exit or they are removed for some other reason. Pods, depending on policy and exit code, may be removed after exiting, or may be retained in order to enable access to the logs of their containers.   Pods have a lifecycle; they are defined, then they are assigned to run on a node, then they run until their container(s) exit or they are removed for some other reason. Pods, depending on policy and exit code, may be removed after exiting, or may be retained in order to enable access to the logs of their containers.   OpenShift Origin treats pods as largely immutable; changes cannot be made to a pod definition while it is running. OpenShift Origin implements changes by terminating an existing pod and recreating it with modified configuration, base image(s), or both. Pods are also treated as expendable, and do not maintain state when recreated. Therefore pods should usually be managed by higher-level controllers, rather than directly by users.   Scale up  When scaled up, an application will have more than one copy of itself, and each copy will have its own local state. Each copy corresponds to a different instance of a pod with the pods being managed by the replication controller. As each pod has a unique IP, we need an easy way to address the set of all pods as a whole. This is where a service comes into play. The service gets its own IP and a DNS name. When making a connection to a service, OpenShift will automatically route the connection to one of the pods associated with that service.   caling from the Web Console Scaling up the number of instances of your application running can be done from the Overview page for your application in the OpenShift web console (the page with those tell-tale up and down arrows we saw previously). Jump to that page and click the up arrow key twice to increase the replica count to 3 If your application is a web application that adheres to the 12-factor methodology, or what might also be called a cloud native application, then it would generally be safe to scale up.   Applications that can’t usually be able to be scaled up include traditional relational databases backed by persistent storage. Databases cannot be scaled in the traditional way as only the primary instance of the database should have the ability to update data. Scaling can still be performed, but usually only on read-only instances of the database.   Kubernetes and Openshift  The basic concepts of Kubernetes and discuss how OpenShift builds on them. In general, you can view Kubernetes as being aimed at Ops teams, providing them with a tool for running containers at scale in production. OpenShift adds to this by also supporting the work of a Dev team and others by making the job of the Ops team easier, which helps to bridge the gap between Dev and Ops and thus enable the latest DevOps philosophy.OpenShift provides a number of different ways to interact with an OpenShift cluster. The OpenShift command-line tool oc is the primary way most users interact with OpenShift. The command-line tool talks via a REST API exposed by the OpenShift cluster.  If you want to avoid using the command line tool, or you want to automate your interactions with the OpenShift cluster, you can always use the REST API directly.   You may be wondering: “Is a namespace the same thing as an application?” OpenShift has no formal concept of an application, thereby allowing an application to be flexible depending on a user’s needs.   You can dedicate one to everything related to just one application. Or, so long as you label all the resources making up an application so you know what goes with what, you can also use the namespace to hold more than one.   Secrets   This topic discusses important properties of secrets and provides an overview on how developers can use them.   The Secret object type provides a mechanism to hold sensitive information such as passwords, OpenShift Origin client configuration files, dockercfg files, private source repository credentials, and so on. Secrets decouple sensitive content from the pods. You can mount secrets into containers using a volume plug-in or the system can use secrets to perform actions on behalf of a pod.   Route  Although a service has a DNS name, it is still only accessible within the OpenShift cluster and is not accessible externally. To make a service externally accessible, a route needs to be created. Creating a route automatically sets up haproxy or a hardware-based router, with an externally addressable DNS name, exposing the service and load-balancing inbound traffic across the pods.   image  The output of the build process is an image, which is stored in an integrated Docker registry ready for distribution out to nodes when the application is deployed. The image stream is how the image and its versions are tracked by OpenShift. If you already have an existing Docker image on an external registry such as Docker Hub, it can also be referenced by an image stream instead of building it locally.   use the handy online cheat sheet available by running the oc types command. It gives you a quick summary of the different conceptual types and definitions used in OpenShift like those we covered here:   Vagrant  Vagrant is a software tool that allows users to create and configure lightweight, reproducible, and portable development environments. It works in conjunction with virtualization (both VMs and IaaS) to automate all the steps necessary to get your dev environment going   One of the advantages of creating and deploying applications from the web console is that a route is automatically created for you. When deploying new containers while using the oc tool, you will need to expose the service manually   Volumes   One of the great features of the OpenShift platform is the ability to provide persistent volumes for your running pods. This ensures that data in your database doesn’t suddenly disappear if the container is restarted. Another important aspect of persistent volumes is the ability to run both stateful and stateless applications on the platform. This is sometimes referred to as mode 1 and mode 2 applications as well as legacy and 12-factor applications.   webhooks  Automatic Deployments Using Webhooks A webhook (also called a web callback or HTTP push API) is a way an application can provide other applications with real-time information or notifications.   We can configure the GitHub code hosting service to trigger a webhook each time we push a set of changes to your project code repository. Using this tool, we can notify OpenShift when you have made code changes and thus initiate rebuild and redeployment of our application.   Deployment Strategies  A deployment strategy defines the process by which a new version of your application is started and the existing instances shut down. By default OpenShift uses a rolling deployment strategy that enables you to perform an update with no apparent down time.  ","categories": [],
        "tags": ["cloud","DevOps"],
        "url": "/2017/11/29/OpenShift.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "promise vs observiable",
        "excerpt":"The drawback of using Promises is that they’re unable to handle data sources that produce more than one value, like mouse movements or sequences of bytes in a file stream. Also, they lack the ability to retry from failure—all present in RxJS.   The most important downside, moreover, is that because Promises are immutable, they can’t be cancelled. So, for instance, if you use a Promise to wrap the value of a remote HTTP call, there’s no hook or mechanism for you to cancel that work. This is unfortunate because HTTP calls, based on the XmlHttpRequest object, can be aborted,[3] but this feature isn’t honored through the Promise interface   ","categories": [],
        "tags": ["Angular","JavaScript"],
        "url": "/2017/12/01/Promise-vs-Observable.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Container",
        "excerpt":"The Docker project was responsible for popularizing container development in Linux systems. The original project defined a command and service (both named docker) and a format in which containers are structured. This chapter provides a hands-on approach to using the docker command and service to begin working with containers in Red Hat Enterprise Linux 7 and RHEL Atomic Host by getting and using container images and working with running containers.   Containers provide a means of packaging applications in lightweight, portable entities. Running applications within containers offers the following advantages:      Smaller than Virtual Machines: Because container images include only the content needed to run an application, saving and sharing is much more efficient with containers than it is with virtual machines (which include entire operating systems)   Improved performance: Likewise, since you are not running an entirely separate operating system, a container will typically run faster than an application that carries with it the overhead of a whole new virtual machine.   Secure: Because a container typically has its own network interfaces, file system, and memory, the application running in that container can be isolated and secured from other activities on a host computer.   Flexible: With an application’s run time requirements included with the application in the container, a container is capable of being run in multiple environments.   RHEL Atomic Host is a light-weight Linux operating system distribution that was designed specifically for running containers. It contains two different versions of the docker service, as well as some services that can be used to orchestrate and manage Docker containers, such as Kubernetes. Only one version of the docker service can be running at a time.   Images   Containers in OpenShift Origin are based on Docker-formatted container images. An image is a binary that includes all of the requirements for running a single container, as well as metadata describing its needs and capabilities.   You can think of it as a packaging technology. Containers only have access to resources defined in the image unless you give the container additional access when creating it. By deploying the same image in multiple containers across multiple hosts and load balancing between them, OpenShift Origin can provide redundancy and horizontal scaling for a service packaged into an image.   Container Registries  A container registry is a service for storing and retrieving Docker-formatted container images. A registry contains a collection of one or more image repositories. Each image repository contains one or more tagged images. Docker provides its own registry, the Docker Hub, and you can also use private or third-party registries. Red Hat provides a registry at registry.access.redhat.com for subscribers. OpenShift Origin can also supply its own internal registry for managing custom container images.   Reference     https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/single/getting_started_with_containers/index#introduction_to_linux_containers  ","categories": [],
        "tags": ["DevOps"],
        "url": "/2017/12/02/Container.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "common errors in NPM or node",
        "excerpt":"code E503  code E503 when run npm install packages, e.g.   npm install pretty-data  Get following error: ` npm ERR! code E503 npm ERR! 503 Service Unavailable: pretty-data@latest   npm ERR! A complete log of this run can be found in: npm ERR!     xxxx\\nodejs\\npm-cache_logs\\2017-12-07T04_16_53_679Z-debug.log   Solution:  You maybe behind corporate proxy, so try execute following command  npm config set proxy http://127.0.0.1:53128  ","categories": [],
        "tags": ["nodejs","javascript","anugar"],
        "url": "/2017/12/07/Errors-In-NPM.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "RxJS reactive extension javascript",
        "excerpt":"Streams   Traditionally, the term stream was used in programming languages as an abstract object related to I/O operations such as reading a file, reading a socket, or requesting data from an HTTP server. For instance, Node.js implements readable, writable, and duplex streams for doing just this. In the RP world, we expand the definition of a stream to mean any data source that can be consumed.  A$ = [20];                                        1 B$ = [22];                                        2 C$ = A$.concat(B$).reduce(adder); //-&gt; [42]       3  A$.push(100);                                     4 C$ = ?   1 Creates a stream initialized with the value 20 2 Creates a stream initialized with the value 22 3 Concatenates both streams and applies an adder function to get a new container with 42 4 Pushes a new value into A$ First, we’ll explain some of the notation we use here. Streams are containers or wrappers of data very similar to arrays, so we used the array literal notation [] to symbolize this. Also, it’s common to use the $ suffix to qualify variables that point to streams. In the RxJS community, this is known as Finnish Notation, attributed to Andre Staltz, who is one of the main contributors of RxJS and Finnish.   Array extras   JavaScript ES5 introduced new array methods, known as the array extras, which enable some level of native support for FP. These include map, reduce, filter, some, every, and others Reactive programming is oriented around data flows and propagation. In this case, you can think of C$ as an always-on variable that reacts to any change and causes actions to ripple through it when any constituent part changes. Now let’s see how RxJS implements this concept.   If you were to visit the main website for the Reactive Extensions project (http://reactivex.io/), you’d find it defined as “an API for asynchronous programming with observable streams.”   Definition   A stream is nothing more than a sequence of events over time. Everything is a stream The concept of a stream can be applied to any data point that holds a value; this ranges from a single integer to bytes of data received from a remote HTTP call. RxJS provides lightweight data types to subscribe to and manage streams as a whole that can be passed around as first-class objects and combined with other streams.  RxJS provides lightweight data types to subscribe to and manage streams as a whole that can be passed around as first-class objects and combined with other streams. Learning how to manipulate and use streams is one of the central topics of this book. At this point, we haven’t talked about any specific RxJS objects; for now, we’ll assume that an abstract data type, a container called Stream, exists. You can create one from a single value as such:   Stream(42); At this point, this stream remains dormant and nothing has actually happened, until there’s a subscriber (or observer) that listens for it. This is very different from Promises, which execute their operations as soon as they’re created. Instead, streams are lazy data types, which means that they execute only after a subscriber is attached. In this case, the value 42, which was lifted into the stream context, navigates or propagates out to at least one subscriber. After it receives the value, the stream is completed:   Stream(42).subscribe(    val =&gt; {                                             1       console.log(val); //-&gt; prints 42    } );   This creates two important challenges: scalability and latency.   As more and more data is received, the amount of memory that your application consumes or requires will grow linearly or, in worst cases, exponentially; this is the classic problem of scalability, and trying to process it all at once will certainly cause the user interface (UI) to become unresponsive. Buttons may no longer appear to work, fancy animations will lag, and the browser may even flag the page to terminate, which is an unacceptable notion for modern web users.   This problem is not new, though in recent years there has been exponential growth in the sheer scale of the number of events and data that JavaScript applications are required to process. This quantity of data is too big to be held readily available and stored in memory for use. Instead, we must create ways to fetch it from remote locations asynchronously, resulting in another big challenge of interconnected software systems: latency, which can be difficult to express in code. you’ll first learn about the fundamental principles of two emerging paradigms: functional programming (FP) and reactive programming (RP). This exhilarating composition is what gives rise to functional reactive programming (FRP), encoded in a library called RxJS (or rx.js), which is the best prescription to deal with asynchronous and event-based data sources effectively. By subscribing to a stream, your code expresses an interest in receiving the stream’s elements. During subscription, you specify the code to be invoked when the next element is emitted, and optionally the code for error processing and stream completion. Often you’ll specify a number of chained operators and then invoke the subscribe() method   The subscribe() method creates the instance of Observer, which in this case passes each value from the stream generated by the searchInput to the getStockQuoteFromServer() method. In a real-world scenario, this method would issue a request to the server, No matter how many operators you chain together, none of them will be invoked on the stream until you invoke subscribe(). If you prefer to generate an observable stream based on another event (such as on keyup), you can use the RxJS Observable.fromEvent() API (see the RxJS  One of the benefits of observables over promises is that the former can be canceled.  Observable1 → switchMap(function) → Observable2 → subscribe()   You’re switching over from the first observable to the second one. If Observable1 pushes the new value but the function that creates Observable2 hasn’t finished yet, it’s killed; switchMap() unsubscribes and resubscribes to Observable1 and starts handling the new value from this stream.   If the observable stream from the UI pushes the next value before getWeather() has returned its observable value, switchMap() kills the running getWeather(), gets the new value for the city from the UI, and invokes getWeather() again. While killing getWeather(), it also aborts the HTTP request that was slow and didn’t complete in time.   The first argument of subscribe() contains a callback for handling data coming from the server. The code in this arrow expression is specific to the API provided by the weather service. You just extract the temperature and humidity from the returned JSON. The API offered by this particular weather service stores the error codes in the response, so you manually handle the status 404 here and not in the error-handler callback.   Now let’s verify that canceling previous requests works. Typing the word London takes more than the 200 milliseconds specified in debounceTime(), which means the valueChanges event will emit the observable data more than once. To ensure that the request to the server takes more than 200 milliseconds, you need a slow internet connection.   Note   Listing 5.5 has lots of code in the constructor, which may look like a red flag to developers who prefer using constructors only to initialize variables and not to execute any code that takes time to complete. If you take a closer look, though, you’ll notice that it just creates a subscription to two observable streams (UI events and HTTP service). No actual processing is done until the user starts entering the name of a city, which happens after the component is already rendered.   We ran the preceding example and then turned on throttling in Chrome Developer Tools, emulating a slow GPRS connection. Typing the word London resulted in four getWeather() invocations: for Lo, Lon, Lond, and London. Accordingly, four HTTP requests were sent over the slow connection, and three of them were automatically canceled by the switchMap() operator, as shown in figure 5.10.   Figure 5.10. Running observable_events_http.ts   With very little programming, you saved bandwidth by eliminating the need for the server to send four HTTP responses for cities you’re not interested in and that may not even exist. As we stated in chapter 1, a good framework is one that allows you to write less code. Angular comes with a number of predefined pipes, and each pipe has a class that implements its functionality (such as DatePipe) as well as the name you can use in the template (such as date):   UpperCasePipe allows you to convert an input string into uppercase by using | uppercase in the template. DatePipe lets you display a date in different formats by using | date. CurrencyPipe transforms a number into a desired currency by using | currency. AsyncPipe will unwrap the data from the provided observable stream by using | async. You’ll see a code sample that uses async in chapter 8. Some pipes don’t require input parameters (such as uppercase), and some do (such as date:’medium’). You can chain as many pipes as you want. The next code snippet shows how to display the value of the birthday variable in a medium date format and in uppercase (for example, JUN 15, 2001, 9:43:11 PM):   Custom pipes In addition to predefined pipes, Angular offers a simple way to create custom pipes, which can include code specific to your application. You need to create a @Pipe annotated class that implements the PipeTransform interface. The PipeTransform interface has the following signature:   export interface PipeTransform {   transform(value: any, …args: any[]): any; } This tells you that a custom pipe class must implement just one method with the preceding signature. The first parameter of transform takes a value to be transformed, and the second defines zero or more parameters required for your transformation algorithm. The @Pipe annotation is where you specify the name of the pipe to be used in the template. If your component uses custom pipes, they have to be explicitly listed in its @Component annotation in the pipes property.   In the previous section, the weather example displayed the temperature in London in Fahrenheit. But most countries use the metric system and show temperature in Celsius. Let’s create a custom pipe that can convert the temperature from Fahrenheit to Celsius and back. The code of the custom TemperaturePipe pipe (see the following listing) can be used in a template as temperature.   Listing 5.6. temperature-pipe.ts   Next comes the code of the component (pipe-tester.ts) that uses the temperature pipe. Initially this program will convert the temperature from Fahrenheit to Celsius (the FtoC format). By clicking the toggle button, you can change the direction of the temperature conversion Event emitters Event emitters are popular mechanisms for asynchronous event-based architectures. The DOM, for instance, is probably one of the most widely known event emitters. On a server like Node.js, certain kinds of objects periodically produce events that cause functions to be called. In Node.js, the EventEmitter class is used to implement APIs for things like WebSocket I/O or file reading/writing so that if you’re iterating through directories and you find a file of interest, an object can emit an event referencing this file for you to execute any additional code. ajax(‘/items',    items =&gt; {        for (let item of items) {           ajax(`/items/${item.getId()}/info`,           dataInfo =&gt; {           ajax(`/files/${dataInfo.files}`,           processFiles);        });     } }); —is known to be continuation-passing style (CPS), because none of the functions are explicitly waiting for a return value. But as we mentioned, abusing this makes code hard to reason about. What you can do is to make continuations first-class citizens and actually define a concrete interpretation of what it means to “continue.” So, we introduce the notion of then: “Do X, then do Y,” to create code that reads like this:   Fetch all items, then                                          1    For-each item fetch all files, then                         1       Process each file 1 The key term “then” suggests time and sequence. This is where Promises come in. A Promise is a data type that wraps an asynchronous or long-running operation, a future value, with the ability for you to subscribe to its result or its error. A Promise is considered to be fulfilled when its underlying operation completes, at which point subscribers will receive the computed result.   Because we can’t alter the value of a Promise once it’s been executed, it’s actually an immutable type, which is a functional quality we seek in our programs.  ajax(‘/items')   .then(items =&gt;     items.forEach(item =&gt;       ajax(`/data/${item.getId()}/info`)        .then(dataInfo =&gt;          ajax(`/data/files/${dataInfo.files}`)        )        .then(processFiles);     )   ); This looks similar to the previous statement! Being a more recent addition to the language with ES6 and inspired in FP design, Promises are more versatile and idiomatic than callbacks. Applying these functions declaratively—meaning your code expresses the what and not the how of what you’re trying to accomplish—into then blocks allows you to express side effects in a pure manner.   let getItems = () =&gt; ajax(‘/items'); let getInfo  = item =&gt; ajax(`/data/${item.getId()}/info`); let getFiles = dataInfo =&gt; ajax(`/data/files/${dataInfo.files}`); and then use Promises to stitch together our asynchronous flow. We use the Promise.all() function to map an array of separate Promises into a single one containing an array of results:   getItems()   .then(items =&gt; items.map(getInfo))   .then(promises =&gt; Promise.all(promises))   .then(infos =&gt; infos.map(getFiles))   .then(promises =&gt; Promise.all(promises))   .then(processFiles); The use of then() explicitly implies that there’s time involved among these calls, which is a really good thing. If any step fails, we can also have matching catch() blocks to handle errors and potentially continue the chain of command if necessary, a   Figure 1.7. Promises create a flow of calls chained by then methods. If the Promise is fulfilled, the chain of functions continues; otherwise, the error is delegated to the Promise catch block.   The drawback of using Promises is that they’re unable to handle data sources that produce more than one value, like mouse movements or sequences of bytes in a file stream. Also, they lack the ability to retry from failure—all present in RxJS.  The most important downside, moreover, is that because Promises are immutable, they can’t be cancelled. So, for instance, if you use a Promise to wrap the value of a remote HTTP call, there’s no hook or mechanism for you to cancel that work. This is unfortunate because HTTP calls, based on the XmlHttpRequest object, can be aborted,[3] but this feature isn’t honored through the Promise interface.   It’s difficult to detect when events or long-running operations go rogue and need to be cancelled. Consider the case of a remote HTTP request that’s taking too long to process. Is the script unresponsive or is the server just slow? It would be ideal to have an easy mechanism to cancel events cleanly after some predetermined amount of time. Implementing your own cancellation mechanism can be very challenging and error prone even with the help of third-party libraries.   One good quality of responsive design is to always throttle a user’s interaction with any UI components, so that the system isn’t unnecessarily overloaded. In chapter 4, you’ll learn how to use throttling and debouncing to your advantage. Manual solutions for achieving this are typically very hard to get right and involve functions that access data outside their local scope, which breaks the stability of your entire program You learned that Promises certainly move the needle in the right direction (and RxJS integrates with Promises seamlessly if you feel the need to do so).   But what you really need is a solution that abstracts out the notion of latency away from your code while allowing you to model your solutions using a linear sequence of steps through which data can flow over time   THE REACTIVE EXTENSIONS FOR JAVASCRIPT Reactive Extensions for JavaScript (RxJS) is an elegant replacement for callback or Promise-based libraries, using a single programming model that treats any ubiquitous source of events—whether it be reading a file, making an HTTP call, clicking a button, or moving the mouse—in the exact same manner. For example, instead of handling each mouse event independently with a callback, with RxJS you handle all of them combined.      Reference     https://angular.io/guide/router  ","categories": [],
        "tags": ["Angular","JavaScript","NodeJs"],
        "url": "/2017/12/08/Router-In-Angular.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Router in angular",
        "excerpt":"Lettable operators  RxJS 5.5, piping all the things   So now we want a way to use those operators, how could we do that?   Well, we said those operators are “lettable” that means we can use them by calling the let method on an observable:   And if we want to chain multiple lettable operators we can keep dot chaining:   import { Observable } from ‘rxjs/Rx’; import { filter, map, reduce } from ‘rxjs/operators’;   const filterOutEvens = filter(x =&gt; x % 2); const sum = reduce((acc, next) =&gt; acc + next, 0); const doubleBy = x =&gt; map(value =&gt; value * x);   const source$ = Observable.range(0, 10);   source$   .let(filterOutEvens)   .let(doubleBy(2))   .let(sum)   .subscribe(x =&gt; console.log(x)); // 50    Meaning we can easily compose a bunch of pure function operators and pass them as a single operator to an observable!   Conclusion With those tools in hand, you can write RxJS code that is much more re-usable by just piping your (pure functions) operators together and easily re-use shared logic.   import { Observable, pipe } from ‘rxjs/Rx’; import { filter, map, reduce } from ‘rxjs/operators’;   const filterOutEvens = filter(x =&gt; x % 2); const sum = reduce((acc, next) =&gt; acc + next, 0); const doubleBy = x =&gt; map(value =&gt; value * x);   const complicatedLogic = pipe(   filterOutEvens,   doubleBy(2),   sum );   const source$ = Observable.range(0, 10);   source$.let(complicatedLogic).subscribe(x =&gt; console.log(x)); // 50   https://github.com/ReactiveX/rxjs/blob/master/doc/pipeable-operators.md What? What is a pipeable operator? Simply put, a function that can be used with the current let operator. It used to be the origin of the name (“lettable”), but that was confusing and we call them “pipeable” now because they’re intended to be used with the pipe utility. A pipeable operator is basically any function that returns a function with the signature: &lt;T, R&gt;(source: Observable&lt;T&gt;) =&gt; Observable&lt;R&gt;.   There is a pipe method built into Observable now at Observable.prototype.pipe that сan be used to compose the operators in similar manner to what you’re used to with dot-chaining (shown below).   There is also a pipe utility function at rxjs/util/pipe that can be used to build reusable pipeable operators from other pipeable operators.   Usage  You pull in any operator you need from one spot, under ‘rxjs/operators’ (plural!). It’s also recommended to pull in the Observable creation methods you need directly as shown below with range:   import { range } from 'rxjs/observable/range'; import { map, filter, scan } from 'rxjs/operators';  const source$ = range(0, 10);  source$.pipe(   filter(x =&gt; x % 2 === 0),   map(x =&gt; x + x),   scan((acc, x) =&gt; acc + x, 0) ) .subscribe(x =&gt; console.log(x))   https://blog.angularindepth.com/rxjs-understanding-lettable-operators-fe74dda186d3 RxJS: Understanding Lettable Operators   What are lettable operators and what does lettable mean?  If lettable operators are used with a method named pipe, you might wonder why they are referred to as lettable. The term is derived from RxJS’s let operator.   The let operator is conceptually similar to the map operator, but instead of taking a projection function that receives and returns a value, let takes a function that receives and returns an observable.  It’s unfortunate that let is one of the less-well-known operators, as it’s very useful for composing reusable functionality.   import * as Rx from \"rxjs\";  export function retry&lt;T&gt;(   count: number,   wait: number ): (source: Rx.Observable&lt;T&gt;) =&gt; Rx.Observable&lt;T&gt; {    return (source: Rx.Observable&lt;T&gt;) =&gt; source     .retryWhen(errors =&gt; errors       // Each time an error occurs, increment the accumulator.       // When the maximum number of retries have been attempted, throw the error.       .scan((acc, error) =&gt; {         if (acc &gt;= count) { throw error; }         return acc + 1;       }, 0)       // Wait the specified number of milliseconds between retries.       .delay(wait)     ); }   When retry is called, it’s passed the number of retry attempts that should be made and the number of milliseconds to wait between attempts, and it returns a function that receives an observable and returns another observable into which the retry logic is composed. The returned function can be passed to the let operator, like this:   import * as Rx from \"rxjs\"; import { retry } from \"./retry\";  const name = Rx.Observable.ajax   .getJSON&lt;{ name: string }&gt;(\"/api/employees/alice\")   .let(retry(3, 1000))   .map(employee =&gt; employee.name)   .catch(error =&gt; Rx.Observable.of(null));   Using the let operator, we’ve been able to create a reusable function much more simply than we would have been able to create a prototype-patching operator. What we’ve created is a lettable operator.   Lettable operators are a higher-order functions. Lettable operators return functions that receive and return observables; and those functions can be passed to the let operator.   We can also use our lettable retry operator with pipe, like this:  import { ajax } from \"rxjs/observable/dom/ajax\"; import { of } from \"rxjs/observable/of\"; import { catchError, map } from \"rxjs/operators\"; import { retry } from \"./retry\";  const name = ajax   .getJSON&lt;{ name: string }&gt;(\"/api/employees/alice\")   .pipe(     retry(3, 1000),     map(employee =&gt; employee.name),     catchError(error =&gt; of(null))   );   Let’s return to our retry function and replace the chained methods with lettable operators and a pipe call, so that it looks like this:  import { Observable } from \"rxjs/Observable\"; import { delay, retryWhen, scan } from \"rxjs/operators\";  export function retry&lt;T&gt;(   count: number,   wait: number ): (source: Observable&lt;T&gt;) =&gt; Observable&lt;T&gt; {    return retryWhen(errors =&gt; errors.pipe(     // Each time an error occurs, increment the accumulator.     // When the maximum number of retries have been attempted, throw the error.     scan((acc, error) =&gt; {       if (acc &gt;= count) { throw error; }       return acc + 1;     }, 0),     // Wait the specified number of milliseconds between retries.     delay(wait)   )); }  With the chained methods replaced, we now have a proper, reusable lettable operator that imports only what it requires.   Why should lettable operators should be preferred?  For application developers, lettable operators are much easier to manage:      Rather then relying upon operators being patched into Observable.prototype, lettable operators are explicitly imported into the modules in which they are used.   It’s easy for TypeScript and bundlers to determine whether the lettable operators imported into a module are actually used. And if they are not, they can be left unbundled. If prototype patching is used, this task is manual and tedious.   For library authors, lettable operators are much less verbose than call-based alternative, but it’s the correct inference of types that is — at least for me — the biggest advantage.    Agreed, the pipe is awesome for composing custom rx operators. But why do we see more and more people using it even when not combining re-usable variables — instead of just chaining methods?   Meaning, we use to write e.g…   const source$ = Observable.range(0, 10);  source$   .filter(x =&gt; x % 2)   .reduce((acc, next) =&gt; acc + next, 0)   .map(value =&gt; value * 2)   .subscribe(x =&gt; console.log(x));   Above is imho much cleaner than what I see more nowadays:   const source$ = Observable.range(0, 10); source$.pipe(   filter(x =&gt; x % 2),   reduce((acc, next) =&gt; acc + next, 0),   map(value =&gt; value * 2) ).subscribe(x =&gt; console.log(x));  Are there performance advantages by using the standalone operators instead of chaining?   debounceInput is a function that takes and returns an observable, so it can be passed to the Observable.prototype.pipe function, like this: valueChanges.pipe(debounceInput).   So, whenever you find yourself using the same combination of operators in many places, you could consider using the static pipe function to create a reusable operator combination.   The static pipe function also makes something else much simpler: dealing with pipe-like overload signatures. Let’s look at that next.   Tree Shaking   https://webpack.js.org/guides/tree-shaking/ Tree shaking is a term commonly used in the JavaScript context for dead-code elimination. It relies on the static structure of ES2015 module syntax, i.e. import and export. The name and concept have been popularized by the ES2015 module bundler rollup.   So, what we’ve learned is that in order to take advantage of tree shaking, you must…   Use ES2015 module syntax (i.e. import and export). Add a “sideEffects” entry to your project’s package.json file. Include a minifier that supports dead code removal (e.g. the UglifyJSPlugin).   mergeMap vs flatMap vs concatMap vs switchMap   Today we’re going to look at the difference between these four three RxJS operators. The simple part is that flatMap is just an alias for mergeMap. Other operators have a difference that might be important in some cases.   When do you need them?  All these operators are used with so-called higher order Observables. This is when items of an Observable are Observables themselves (or they are mapped to Observables) and we need to flatten all of them into one final Observable. You can easily identify this situation when you subscribe to an Observable inside subscription to another Observable (Note: this is not a recommended approach):  outerObservable.subscribe(outerItem =&gt; {     outerItem.subsribe(innerItem =&gt; {             foo(innerItem);         })});  In my examples, initial Observable is called outer Observable. And items of the outer Observable are called inner Observables. Technically inner and outer Observables are just plain Observables.   A usage example for such operators can be a search box (search box text changes as outer Observable) with a request being sent to a server for each search text change (HTTP responses as inner Observables). Another example is mouse button clicks (outer Observable) that trigger an interval timer for each mouse click (timer events as inner Observables).   Learning plan To tackle these operators you need to understand more basic ones first. In this article I won’t give any definitions or explanations per se, but rather just a small learning plan with links:   Map, Merge, Concat. Basic Observable operators. MergeAll, ConcatAll, Switch. These are for higher order Observables already. After that MergeMap, ConcatMap, and SwitchMap should be easy for you. Operators from the third group are two step operators. First, they map outer Observable items to inner Observables. The second step is to merge a result set of inner Observables in some way. The way they are merged depends on the operator you use.  mergeMap = map + mergeAll concatMap = map + concatAll switchMap = map + switch   Here are some reworked reactivex.io diagrams. At first some explanations for the diagrams:   MergeMap  Outer (initial) Observable emits circles. Each circle is then mapped to its own inner Observable - collection of rhombuses. Collections are identified by color; each collection has its own color. All those inner Observables are then merged into one final Observable - resulting collection of rhombuses.   MergeMap mergeMap emits items into the resulting Observable just as they are emitted from inner Observables. It doesn’t wait for anything. mergeMap doesn’t preserve the order from outer Observable. Collections of rhombuses interleave. mergeMap doesn’t cancel any inner Observables. All rhombuses from inner Observables get to final collection.   ConcatMap  concatMap waits for inner Observable to complete before taking items from the next inner Observable. concatMap does preserve the order from outer Observable. Collections of rhombuses don’t interleave. Just as mergeMap, concatMap doesn’t cancel any inner Observables. All rhombuses from inner Observables get to the final collection.   SwitchMap  switchMap emits items only from the most recent inner Observable. switchMap cancels previous inner Observables when a new inner Observable appears. Items of inner Observable that were emitted after the Observable was canceled will be lost (not included in the resulting Observable).   “Talk is cheap. Show me the code.” I still wasn’t sure I had cracked the difference between the discussed operators even after reading all the results from Google’s first page :) I’ve set up a small example on JsFiddle to see the difference in practice.   In the example, outer Observable emits three items with a one-second interval. Each item is then mapped to an inner Observable, which in its turn emits another three items with a one-second interval. The final result depends on the operator you use. Hope this small example will help you understand these operators without too much googling.   Router  const appRoutes: Routes = [   { path: 'crisis-center', component: CrisisListComponent },   { path: 'hero/:id',      component: HeroDetailComponent },   {     path: 'heroes',     component: HeroListComponent,     data: { title: 'Heroes List' }   },   { path: '',     redirectTo: '/heroes',     pathMatch: 'full'   },   { path: '**', component: PageNotFoundComponent } ];  @NgModule({   imports: [     RouterModule.forRoot(       appRoutes,       { enableTracing: true } // &lt;-- debugging purposes only     )     // other imports here   ],   ... }) export class AppModule { }      The data property in the third route is a place to store arbitrary data associated with this specific route. The data property is accessible within each activated route. Use it to store items such as page titles, breadcrumb text, and other read-only, static data. You’ll use the resolve guard to retrieve dynamic data   The empty path in the fourth route represents the default path for the application, the place to go when the path in the URL is empty, as it typically is at the start. This default route redirects to the route for the /heroes URL and, therefore, will display the HeroesListComponent.   The ** path in the last route is a wildcard. The router will select this route if the requested URL doesn’t match any paths for routes defined earlier in the configuration. This is useful for displaying a “404 - Not Found” page or redirecting to another route.   The order of the routes in the configuration matters and this is by design. The router uses a first-match wins strategy when matching routes, so more specific routes should be placed above less specific routes.   Router outlet   The router matches that URL to the route path  and displays the Component after a RouterOutlet that you’ve placed in the host view’s HTML.   content_copy &lt;router-outlet&gt;&lt;/router-outlet&gt; &lt;!-- Routed views go here --&gt;   Define routes  A router must be configured with a list of route definitions.   mechanism     When the application requests navigation to the path /crisis-center, the router activates an instance of CrisisListComponent, displays its view, and updates the browser’s address location and history with the URL for that path.   Pass the array of routes, appRoutes, to the RouterModule.forRoot method. It returns a module, containing the configured Router service provider, plus other providers that the routing library requires. Once the application is bootstrapped, the Router performs the initial navigation based on the current browser URL   Some key points of setting up router     Load the router library.   Add a nav bar to the shell template with anchor tags, routerLink and routerLinkActive directives.   Add a router-outlet to the shell template where views will be displayed.   Configure the router module with RouterModule.forRoot.   Set the router to compose HTML5 browser URLs.   handle invalid routes with a wildcard route.   navigate to the default route when the app launches with an empty path.   Difference between forRoot and forChild  Only call RouterModule.forRoot in the root AppRoutingModule (or the AppModule if that’s where you register top level application routes). In any other module, you must call the RouterModule.forChild method to register additional routes.   Leave the default and the wildcard routes! These are concerns at the top level of the application itself.   Reference     https://angular.io/guide/router  ","categories": [],
        "tags": ["angular","rxjs","typescript","javascript","node"],
        "url": "/2017/12/10/RxJS.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "reactive programing",
        "excerpt":"The second advantage to a lazy subscription is that the observable doesn’t hold onto data by default. In the previous example, each event generated by the interval will be processed and then dropped. This is what we mean when we say that the observable is streaming in nature rather than pooled. This discard-by-default semantic means that you never have to worry about unbounded memory growth sneaking up on you, causing memory leaks. When writing native event-driven JavaScript code, especially in older browsers, memory leaks can occur if you neglect event management and disposal.  ","categories": [],
        "tags": ["JavaScript"],
        "url": "/2017/12/11/Why-Reactive-programming-is-outpermant.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Redux",
        "excerpt":"whats @Effects  You can almost think of your Effects as special kinds of reducer functions that are meant to be a place for you to put your async calls in such a way that the returned data can then be easily inserted into the store’s internal state for the application.   rule of a thumb for actions  As a good rule of thumb, try not to make one reducer that handles all the actions, but also do not make a separate reducer for each action. Group them in a way that makes sense for the application structure.  ","categories": [],
        "tags": ["javascript","angular"],
        "url": "/2017/12/12/Redux.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "cloud computering",
        "excerpt":"Concepts  Cloud computing is the on-demand demand delivery of compute database storage applications and other IT resources through a cloud services platform via the Internet with pay-as-you-go pricing.   The 6 advantages of cloud computering   Trade Capital Expense For Variable Expense  Benefit from massive economies of scale  Stop guessing about capacity  Increase speed and agility  Stop spending money running and maintaining data centers  Go global in minutes   3 types of cloud computing     IAAS   PAAS   SAAS   “Certification is the beginning of your journey, not the end”   Edge location  Edge locations are CDN endpoints for CloudFront.   IAM  Identity Access Management, to control who can access the service. IAM is apply to “Global”, it’s not releated to any single region.   MFA  Multiple Factors Authentication, which can be virtual (software) or hardware devices.   How to access AWS     Web console   CLI (command line)   SDK   Account  By logging into webconsole with your email acess, you are in root account. The root account alwyas has full administrator access. You should not give these account credentials away to anyone. You should always secure this root account using multi factor authentication.   Group  A group is simply a place to store your users. Your users will inherit all permissions that the group has. Examples of groups might be developers, system administrators, etc.   Policy  To set the permissions in a group you need to apply a policy to that group. Policies consist of JSON.   S3     Its a safe place to store your files. They are object-based storage.   S3 is a global name space, so you won’t have same name with others.   There is unlimited storage.   Files are stored in Buckets, you can think buckets as folder.   There are versioning, encryptiong for file storage   Secure your data using Access Control Lists(Object level) and Bucket policies(folder level)   S3 solutions     S3 (99.99% availability , 99.999999999% durability)-&gt; S3 IA (infrequently accessed) -&gt; Reduced Redundancy Storage -&gt; Glacier (very cheap , used for archival, it takes 3-5 hours to restore from Glacier, its $0.01/GB/Month)   S3 is for current data while Glacier is for archived data.   S3 Transfer Acceleration  It enable fast, easy and secure transfer of files over long distances between your end users and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.   Cloudfront  It’s the CDN services. Amazon CloudFront can be used to deliever your entire website, including dynamic, static contents using a global network of edge locations. There are two types of distributions available:     Web Distribution: typically used for websites.   RTMP distribution: used for media streaming   Edge location  This is the location where content will be cached. This is separate to an AWS region/AZ (Avaiability Zones)   Origin  This is the origin of all the files that the CDN will distribute. This can be an S3 bucket, an EC2 instance, and Elastic Load Balancer or Route53.   EC2  Amazon Elastic Compute Cloud (EC2) is a web service that provies resizable compute capacity in the cloud. Basically EC2 are just virtual machines. EC2 reduces the time required to obtain and boot new server instances to minutes, allowing you to quickly scale capacity, both up and down, as your computing requirements changes.   EC2 options     On demand   reserved   Spot. Users with urgent computing needs for large amounts of additional capacity.   Dedicated   EBS  Elastic Block Storage. Amazon EBS allows you to create storage volumes and attach them to Amazon EC2 instances. Once attached, you can create a file system on top of these volumes, run a database, or use them in any other way you would use in a block device. It’s simply a virtual disk that you install your operating system on and all relevant files.   openshift vs openstack  “How does OpenShift relate to OpenStack?”, I answer “OpenShift Origin can run on top of OpenStack. They are complementary projects that work well together. OpenShift Origin is not presently part of OpenStack, and does not compete with OpenStack. If you stand up your own OpenStack system, you can make it even more useful by installing OpenShift Origin on top of it.”   OpenStack provides “Infrastructure-as-a-Service”, or “IaaS”. It provides bootable virtual machines, networking, block storage, object storage, and so forth. Some IaaS service providers based on OpenStack are HP Cloud and Rackspace Cloud.   The OpenShift hosted service provides “Platform-as-a-Service” or “PaaS”. It provides the necessary parts to quickly deploy and run a LAMP application: the web server, application server, application runtimes and libraries, database service, and so forth.   ","categories": [],
        "tags": ["cloud"],
        "url": "/2017/12/18/Clouding.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "cloud computering",
        "excerpt":"openshift vs openstack  The shoft and direct answer is `OpenShift Origin can run on top of OpenStack. They are complementary projects that work well together. OpenShift Origin is not presently part of OpenStack, and does not compete with OpenStack. If you stand up your own OpenStack system, you can make it even more useful by installing OpenShift Origin on top of it.’   OpenStack is IAAS   OpenStack provides “Infrastructure-as-a-Service”, or “IaaS”. It provides bootable virtual machines, networking, block storage, object storage, and so forth. Some IaaS service providers based on OpenStack are HP Cloud and Rackspace Cloud.   OpenShift is PAAS   The OpenShift hosted service provides “Platform-as-a-Service” or “PaaS”. It provides the necessary parts to quickly deploy and run a LAMP application: the web server, application server, application runtimes and libraries, database service, and so forth.   ","categories": [],
        "tags": ["cloud","OpenStack","OpenShift"],
        "url": "/2017/12/18/OpenStack-vs-OpenShift.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "iOS programming",
        "excerpt":"View  A view is also a responder (UIView is a subclass of UIResponder). This means that a view is subject to user interactions, such as taps and swipes. Thus, views are the basis not only of the interface that the user sees, but also of the interface that the user touches   The Window and Root View  The top of the view hierarchy is the app’s window. It is an instance of UIWindow (or your own subclass thereof), which is a UIView subclass. Your app should have exactly one main window. It is created at launch time and is never destroyed or replaced   Object  In Swift, the syntax of message-sending is dot-notation. We start with the object; then there’s a dot (a period); then there’s the message. (Some messages are also followed by parentheses, but ignore them for now; the full syntax of message-sending is one of those details we’ll be filling in later.) This is valid Swift syntax:   fido.bark() rover.sit()   The idea of everything being an object is a way of suggesting that even “primitive” linguistic entities can be sent messages. Take, for example, 1. It appears to be a literal digit and no more. It will not surprise you, if you’ve ever used any programming language, that you can say things like this in Swift:   let sum = 1 + 2   But it is surprising to find that 1 can be followed by a dot and a message. This is legal and meaningful in Swift (don’t worry about what it actually means):   let s = 1.description   Just as 1 is actually an object, + is actually a message; but it’s a message with special syntax (operator syntax). In Swift, every noun is an object, and every verb is a message.   extension Int {     func sayHello() {         print(“Hello, I’m (self)”)     } } 1.sayHello() // outputs: “Hello, I’m 1”   In Swift, then, 1 is an object. In some languages, such as Objective-C, it clearly is not; it is a “primitive” or scalar built-in data type. So the distinction being drawn here is between object types on the one hand and scalars on the other. In Swift, there are no scalars; all types are ultimately object types. That’s what “everything is an object” really means.   Class   Swift has classes, but 1 in Swift is not a class or an instance of a class: the type of 1, namely Int, is a struct, and 1 is an instance of a struct. And Swift has yet another kind of thing you can send messages to, called an enum.   So Swift has three kinds of object type: classes, structs, and enums. I like to refer to these as the three flavors of object type. Exactly how they differ from one another will emerge in due course. But they are all very definitely object types, and their similarities to one another are far stronger than their differences. For now, just bear in mind that these three flavors exist.   Variables   A variable is a name for an object. Technically, it refers to an object; it is an object reference. Nontechnically, you can think of it as a shoebox into which an object is placed. The object may undergo changes, or it may be replaced inside the shoebox by another object, but the name has an integrity all its own. The object to which the variable refers is the variable’s value.   In Swift, no variable comes implicitly into existence; all variables must be declared. If you need a name for something, you must say “I’m creating a name.” You do this with one of two keywords: let or var. In Swift, declaration is usually accompanied by initialization — you use an equal sign to give the variable a value immediately, as part of the declaration. These are both variable declarations (and initializations):   let one = 1 var two = 2   main.swift   Swift also has a special rule that a file called main.swift, exceptionally, can have executable code at its top level, outside any function body, and this is the code that actually runs when the program runs. You can construct your app with a main.swift file, but in general you won’t need to.  ","categories": [],
        "tags": ["iOS","swift"],
        "url": "/2018/01/06/iOS.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "ngrx",
        "excerpt":"Why @Effects?  In a simple ngrx/store project without ngrx/effects there is really no good place to put your async calls. Suppose a user clicks on a button or types into an input box and then we need to make an asynchronous call. The dumb component will be the first to know about this action from the user, and it’s handler will be called when the button is actually clicked. However, we don’t want to put the logic to do our async call right in the dumb component since we want to keep it dumb! The only thing in the dumb component’s handler is it’s @Output emitter emitting an event to the smart component telling it that the button was clicked. Then the smart component gets the event and it’s handler function is triggered, but we don’t want to put the async login right in there because we want to keep it lean and only dipatching actions to our store so that the store can modify the state! Ok… but the store only handles actions in the reducer, and reducer are meant to be pure functions so where are we supposed to logically put our async calls so that we can put their response data in the store? The answer, friends, is @Effects! You can almost think of your Effects as special kinds of reducer functions that are meant to be a place for you to put your async calls in such a way that the returned data can then be easily inserted into the store’s internal state for the application.   what’s effects  At it’s core, the Effects Class in simply just an Angular 2 Service:   In Angular 2 a service is just a regular old TypeScript class with the @Injectable metadata, and when working with @Effects you make a single “Effect Class” or “Effect Service” that then contains various @Effect functions, each corresponding to an action dispatched by your ngrx store.   sample  @Effect() update$ = this.action$     .ofType('SUPER_SIMPLE_EFFECT')     .switchMap( () =&gt;       Observable.of({type: \"SUPER_SIMPLE_EFFECT_HAS_FINISHED\"})     );  Were using the TypeScript metadata to label our variable update$ (the $ is commonly used as a suffix for variables whose value is an observable) as an “ngrx effect” that will be triggered when we dispatch actions with the store (the same was we always send actions to the reducer or reducers). Then we see “this.action$.ofType(‘SUPER_SIMPLE_EFFECT’)”. Remeber, we’re translating the dispatched event into an observable, so .ofType means your taking in an observable and then returning the observable only if it is of that type.  ","categories": [],
        "tags": ["Angular","javascript","ngrx"],
        "url": "/2018/01/08/ngrx.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "CORS :Cross-Origin Resource Sharing",
        "excerpt":"Cross-Origin Request Sharing - CORS (A.K.A. Cross-Domain AJAX request) is an issue that most web developers might encounter, according to Same-Origin-Policy, browsers restrict client JavaScript in a security sandbox, usually JS cannot directly communicate with a remote server from a different domain. In the past developers created many tricky ways to achieve Cross-Domain resource request, most commonly using ways are:   Use Flash/Silverlight or server side as a “proxy” to communicate with remote. JSON With Padding (JSONP). Embeds remote server in an iframe and communicate through fragment or window.name, refer here. Those tricky ways have more or less some issues, for example JSONP might result in security hole if developers simply “eval” it, and #3 above, although it works, both domains should build strict contract between each other, it neither flexible nor elegant IMHO:)   W3C had introduced Cross-Origin Resource Sharing (CORS) as a standard solution to provide a safe, flexible and a recommended standard way to solve this issue.   The Mechanism   From a high level we can simply deem CORS is a contract between client AJAX call from domain A and a page hosted on domain B, a typical Cross-Origin request/response would be:   DomainA AJAX request headers   Host DomainB.com User-Agent Mozilla/5.0 (Windows NT 6.1; WOW64; rv:2.0) Gecko/20100101 Firefox/4.0 Accept text/html,application/xhtml+xml,application/xml;q=0.9,/;q=0.8,application/json Accept-Language en-us; Accept-Encoding gzip, deflate Keep-Alive 115 Origin http://DomainA.com  DomainB response headers   Cache-Control private Content-Type application/json; charset=utf-8 Access-Control-Allow-Origin DomainA.com Content-Length 87 Proxy-Connection Keep-Alive Connection Keep-Alive The blue parts I marked above were the kernal facts, “Origin” request header “indicates where the cross-origin request or preflight request originates from”, the “Access-Control-Allow-Origin” response header indicates this page allows remote request from DomainA (if the value is * indicate allows remote requests from any domain).   As I mentioned above, W3 recommended browser to implement a “preflight request” before submiting the actually Cross-Origin HTTP request, in a nutshell it is an HTTP OPTIONS request:   OPTIONS DomainB.com/foo.aspx HTTP/1.1 If foo.aspx supports OPTIONS HTTP verb, it might return response like below:   HTTP/1.1 200 OK Date: Wed, 01 Mar 2011 15:38:19 GMT Access-Control-Allow-Origin: http://DomainA.com Access-Control-Allow-Methods: POST, GET, OPTIONS, HEAD Access-Control-Allow-Headers: X-Requested-With Access-Control-Max-Age: 1728000 Connection: Keep-Alive Content-Type: application/json Only if the response contains “Access-Control-Allow-Origin” AND its value is “*” or contain the domain who submitted the CORS request, by satisfying this mandtory condition browser will submit the actual Cross-Domain request, and cache the result in “Preflight-Result-Cache”.   I blogged about CORS three years ago: AJAX Cross-Origin HTTP request  ","categories": [],
        "tags": ["DevOps"],
        "url": "/2018/01/10/CORS.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "What is difference between declarations, providers and import in NgModule",
        "excerpt":"What is difference between declarations, providers and import in NgModule      imports: is used to import supporting modules likes FormsModule, RouterModule, CommonModule, or any other custom-made feature module. makes the exported declarations of other modules available in the current module   declarations are to make directives (including components and pipes) from the current module available to other directives in the current module. Selectors of directives, components or pipes are only matched against the HTML if they are declared or imported. declaration is used to declare components, directives, pipes that belongs to the current module. Everything inside declarations knows each other. For example, if we have a component, say UsernameComponent, which display list of the usernames, and we also have a pipe, say toupperPipe, which transform string to uppercase letter string. Now If we want to show usernames in uppercase letters in our UsernameComponent, we can use the toupperPipe which we had created before but how UsernameComponent know that the toupperPipe exist and how we can access and use it, here comes the declarations, we can declare UsernameComponent and toupperPipe.   providers are to make services and values known to DI. They are added to the root scope and they are injected to other services or directives that have them as dependency.provider is used to inject the services required by components, directives, pipes in our module.  ","categories": [],
        "tags": ["Angular","JavaScript","NodeJs"],
        "url": "/2018/01/11/Angular-Module-Declaration-Import.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Minium Viable Product",
        "excerpt":"https://blog.leanstack.com/minimum-viable-product-mvp-7e280b0b9418   What is a Minimum Viable Product (MVP) A Minimum Viable Product is the smallest thing you can build that delivers customer value (and as a bonus captures some of that value back i.e. gets you paid).  ","categories": [],
        "tags": [],
        "url": "/2018/02/27/MVP.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "NodeJs Notes",
        "excerpt":"commands to read files  var lineReader = require(‘readline’).createInterface({     input: require(‘fs’).createReadStream(‘C:\\dev\\node\\input\\git_reset_files.txt’)  });   lineReader.on(‘line’, function(line){     console.log(‘git checkout ‘+line);  });   ","categories": [],
        "tags": ["NodeJs","Angular"],
        "url": "/2018/03/18/NodeJs-Notes.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Node errors troubleshooting",
        "excerpt":"Here is the typical erros log:   node_modules/@types/node/index.d.ts(6202,55): error TS2304: Cannot find name 'Map'. node_modules/@types/node/index.d.ts(6209,55): error TS2304: Cannot find name 'Set'. node_modules/@types/node/index.d.ts(6213,64): error TS2304: Cannot find name 'Symbol'. node_modules/@types/node/index.d.ts(6219,59): error TS2304: Cannot find name 'WeakMap'. node_modules/@types/node/index.d.ts(6220,59): error TS2304: Cannot find name 'WeakSet'.  The main reason is above stuff are new to ES6, which are unavaiable in ES5. Hold no, you don’t need to change your typescript target to ES6, which may break projects and leads to tons of new regression testing. Firsty, try to add following in tsconfig.json  \"lib\": [\"es2016\", \"dom\"],  If no luck, try following in command line, it should resolve this issue.  $ tsc index.ts --lib \"es6\"   certificate error   Typical errors  events.js:183       throw er; // Unhandled 'error' event       ^  Error: unable to verify the first certificate     at TLSSocket.&lt;anonymous&gt; (_tls_wrap.js:1103:38)     at emitNone (events.js:106:13)     at TLSSocket.emit (events.js:208:7)     at TLSSocket._finishInit (_tls_wrap.js:637:8)     at TLSWrap.ssl.onhandshakedone (_tls_wrap.js:467:38)  Solution   add following to https request options  ,       rejectUnauthorized: false,         requestCert: true,         agent: false   error ECONNREFUSED  Error: connect ECONNREFUSED 127.0.0.1:443     at Object._errnoException (util.js:1022:11)     at _exceptionWithHostPort (util.js:1044:20)     at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1182:14)  Solution:  in the http request, do not use ‘url’ but ‘host’ and path host: xxx.com,     port: 443,     path: /login,  ","categories": [],
        "tags": ["NodeJs","Angular"],
        "url": "/2018/05/11/Node-errors-DB.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Strategy-Of-Openshift-Releases",
        "excerpt":"Release &amp; Testing Strategy  There are various methods for safely releasing changes to Production. Each team must select what is appropriate for their own use case with consideration to risk, rollback approaches and testing approaches.   The following are options:   Canary Release  This is the lowest risk strategy since it allows for testing on a subset of users, and it allows for fast rollback:      Build a new Environment   Use a routing tool (eg. Apigee) to test with a specific set of users   Bleed traffic across   Remove old environment   Blue Green  This is the classic zero-downtime deployment model that involves flipping traffic between two environments:     Ensure your router has two entry points, one for Production Testing and one for Production traffic   Have two environments: blue and green   If Production traffic is pointing to blue then deploy your changes on green   Point your Production Testing traffic to green   When verified, point Production traffic to green   On the next release, deploy changes to blue   ","categories": [],
        "tags": ["clud","Openshift"],
        "url": "/2018/05/14/Strategy-Of-Openshift-Releases.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "How to setup nodejs to install package from intranet",
        "excerpt":"Error of ‘ECONNRESET’  You may face error ECONNRESET from intranet, even appropriate proxy tools (e.g. cntlm) is running. The errors may looks like  $ npm install -g @angular/cli@latest npm WARN registry Unexpected warning for http://registry.npmjs.org/: Miscellaneous Warning ECONNRESET: request to http://registry.npmjs.org/@angular%2fcli failed, reason: read ECONNRESET npm WARN registry Using stale package data from http://registry.npmjs.org/ due to a request error during revalidation. npm ERR! code ECONNRESET npm ERR! errno ECONNRESET npm ERR! network request to http://registry.npmjs.org/@angular-devkit%2farchitect failed, reason: read ECONNRESET npm ERR! network This is a problem related to network connectivity. npm ERR! network In most cases you are behind a proxy or have bad network settings. npm ERR! network npm ERR! network If you are behind a proxy, please make sure that the npm ERR! network 'proxy' config is set properly.  See: 'npm help config'  npm ERR! A complete log of this run can be found in: npm ERR!     C:\\Users\\xxx\\AppData\\Roaming\\npm-cache\\_logs\\2018-05-15T05_04_39_505Z-debug.log   resson  You need to update npm configuration to make sure below config presence     registry and config https-proxy should NOT exist.   You can run following command to remove it     https-proxy   You can check your current config as  npm config list   To fix above issues, please run following commands  npm config delete https-proxy   Error of ‘code E503’   Sometimes when you run npm install, you can see error ‘E503’.  $ npm install -g @angular/cli@latest                                            npm WARN registry Using stale package data from http://registry.npmjs.org/ due to a request error during revalidation. npm ERR! code E503 npm ERR! 503 Service Unavailable: @angular-devkit/architect@0.6.0  npm ERR! A complete log of this run can be found in: npm ERR!     C:\\Users\\xxx\\AppData\\Roaming\\npm-cache\\_logs\\2018-05-15T05_40_53_127Z-debug.log   That’s because property ‘proxy’ missing for npm. Please run below command to check wether it’s exist or not.   npm config list | grep 'proxy'   If not exist, run below command to add it back.  npm config set proxy http://127.0.0.1:53128  ","categories": [],
        "tags": ["nodejs","angular","javascript"],
        "url": "/2018/05/15/how-to-setup-node-config-in-intranet.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "rxjs pipe in depth",
        "excerpt":"https://stormforger.com/blog/2016/07/08/types-of-performance-testing/   Learn more about load testing, scalability testing, stress, spike and soak testing, configuration testing as well as availability and resilience testing.   ———–15/05/2018 notes ————— https://blog.hackages.io/rxjs-5-5-piping-all-the-things-9d469d1b3f44 RxJS 5.5, piping all the things   So now we want a way to use those operators, how could we do that?   Well, we said those operators are “lettable” that means we can use them by calling the let method on an observable:   And if we want to chain multiple lettable operators we can keep dot chaining:   import { Observable } from ‘rxjs/Rx’; import { filter, map, reduce } from ‘rxjs/operators’;   const filterOutEvens = filter(x =&gt; x % 2); const sum = reduce((acc, next) =&gt; acc + next, 0); const doubleBy = x =&gt; map(value =&gt; value * x);   const source$ = Observable.range(0, 10);   source$   .let(filterOutEvens)   .let(doubleBy(2))   .let(sum)   .subscribe(x =&gt; console.log(x)); // 50    Meaning we can easily compose a bunch of pure function operators and pass them as a single operator to an observable!   Conclusion With those tools in hand, you can write RxJS code that is much more re-usable by just piping your (pure functions) operators together and easily re-use shared logic.   import { Observable, pipe } from ‘rxjs/Rx’; import { filter, map, reduce } from ‘rxjs/operators’;   const filterOutEvens = filter(x =&gt; x % 2); const sum = reduce((acc, next) =&gt; acc + next, 0); const doubleBy = x =&gt; map(value =&gt; value * x);   const complicatedLogic = pipe(   filterOutEvens,   doubleBy(2),   sum );   const source$ = Observable.range(0, 10);   source$.let(complicatedLogic).subscribe(x =&gt; console.log(x)); // 50   https://github.com/ReactiveX/rxjs/blob/master/doc/pipeable-operators.md What? What is a pipeable operator? Simply put, a function that can be used with the current let operator. It used to be the origin of the name (“lettable”), but that was confusing and we call them “pipeable” now because they’re intended to be used with the pipe utility. A pipeable operator is basically any function that returns a function with the signature: &lt;T, R&gt;(source: Observable&lt;T&gt;) =&gt; Observable&lt;R&gt;.   There is a pipe method built into Observable now at Observable.prototype.pipe that сan be used to compose the operators in similar manner to what you’re used to with dot-chaining (shown below).   There is also a pipe utility function at rxjs/util/pipe that can be used to build reusable pipeable operators from other pipeable operators.   Usage  You pull in any operator you need from one spot, under ‘rxjs/operators’ (plural!). It’s also recommended to pull in the Observable creation methods you need directly as shown below with range:   import { range } from 'rxjs/observable/range'; import { map, filter, scan } from 'rxjs/operators';  const source$ = range(0, 10);  source$.pipe(   filter(x =&gt; x % 2 === 0),   map(x =&gt; x + x),   scan((acc, x) =&gt; acc + x, 0) ) .subscribe(x =&gt; console.log(x))   https://blog.angularindepth.com/rxjs-understanding-lettable-operators-fe74dda186d3 RxJS: Understanding Lettable Operators   What are lettable operators and what does lettable mean?  If lettable operators are used with a method named pipe, you might wonder why they are referred to as lettable. The term is derived from RxJS’s let operator.   The let operator is conceptually similar to the map operator, but instead of taking a projection function that receives and returns a value, let takes a function that receives and returns an observable.  It’s unfortunate that let is one of the less-well-known operators, as it’s very useful for composing reusable functionality.   import * as Rx from \"rxjs\";  export function retry&lt;T&gt;(   count: number,   wait: number ): (source: Rx.Observable&lt;T&gt;) =&gt; Rx.Observable&lt;T&gt; {    return (source: Rx.Observable&lt;T&gt;) =&gt; source     .retryWhen(errors =&gt; errors       // Each time an error occurs, increment the accumulator.       // When the maximum number of retries have been attempted, throw the error.       .scan((acc, error) =&gt; {         if (acc &gt;= count) { throw error; }         return acc + 1;       }, 0)       // Wait the specified number of milliseconds between retries.       .delay(wait)     ); }  When retry is called, it’s passed the number of retry attempts that should be made and the number of milliseconds to wait between attempts, and it returns a function that receives an observable and returns another observable into which the retry logic is composed. The returned function can be passed to the let operator, like this:   import * as Rx from \"rxjs\"; import { retry } from \"./retry\";  const name = Rx.Observable.ajax   .getJSON&lt;{ name: string }&gt;(\"/api/employees/alice\")   .let(retry(3, 1000))   .map(employee =&gt; employee.name)   .catch(error =&gt; Rx.Observable.of(null));   Using the let operator, we’ve been able to create a reusable function much more simply than we would have been able to create a prototype-patching operator. What we’ve created is a lettable operator.   Lettable operators are a higher-order functions. Lettable operators return functions that receive and return observables; and those functions can be passed to the let operator.   We can also use our lettable retry operator with pipe, like this:  import { ajax } from \"rxjs/observable/dom/ajax\"; import { of } from \"rxjs/observable/of\"; import { catchError, map } from \"rxjs/operators\"; import { retry } from \"./retry\";  const name = ajax   .getJSON&lt;{ name: string }&gt;(\"/api/employees/alice\")   .pipe(     retry(3, 1000),     map(employee =&gt; employee.name),     catchError(error =&gt; of(null))   );   Let’s return to our retry function and replace the chained methods with lettable operators and a pipe call, so that it looks like this:  import { Observable } from \"rxjs/Observable\"; import { delay, retryWhen, scan } from \"rxjs/operators\";  export function retry&lt;T&gt;(   count: number,   wait: number ): (source: Observable&lt;T&gt;) =&gt; Observable&lt;T&gt; {    return retryWhen(errors =&gt; errors.pipe(     // Each time an error occurs, increment the accumulator.     // When the maximum number of retries have been attempted, throw the error.     scan((acc, error) =&gt; {       if (acc &gt;= count) { throw error; }       return acc + 1;     }, 0),     // Wait the specified number of milliseconds between retries.     delay(wait)   )); }  With the chained methods replaced, we now have a proper, reusable lettable operator that imports only what it requires.   Why should lettable operators should be preferred?  For application developers, lettable operators are much easier to manage:      Rather then relying upon operators being patched into Observable.prototype, lettable operators are explicitly imported into the modules in which they are used.   It’s easy for TypeScript and bundlers to determine whether the lettable operators imported into a module are actually used. And if they are not, they can be left unbundled. If prototype patching is used, this task is manual and tedious.   For library authors, lettable operators are much less verbose than call-based alternative, but it’s the correct inference of types that is — at least for me — the biggest advantage.    Agreed, the pipe is awesome for composing custom rx operators. But why do we see more and more people using it even when not combining re-usable variables — instead of just chaining methods?   Meaning, we use to write e.g…   const source$ = Observable.range(0, 10);  source$   .filter(x =&gt; x % 2)   .reduce((acc, next) =&gt; acc + next, 0)   .map(value =&gt; value * 2)   .subscribe(x =&gt; console.log(x)); Above is imho much cleaner than what I see more nowadays:   const source$ = Observable.range(0, 10); source$.pipe(   filter(x =&gt; x % 2),   reduce((acc, next) =&gt; acc + next, 0),   map(value =&gt; value * 2) ).subscribe(x =&gt; console.log(x)); Are there performance advantages by using the standalone operators instead of chaining?    https://webpack.js.org/guides/tree-shaking/ Tree shaking is a term commonly used in the JavaScript context for dead-code elimination. It relies on the static structure of ES2015 module syntax, i.e. import and export. The name and concept have been popularized by the ES2015 module bundler rollup.   So, what we’ve learned is that in order to take advantage of tree shaking, you must…   Use ES2015 module syntax (i.e. import and export). Add a “sideEffects” entry to your project’s package.json file. Include a minifier that supports dead code removal (e.g. the UglifyJSPlugin).   — english— it can safely prune unused exports.   Trim (a tree, shrub, or bush) by cutting away dead or overgrown branches or stems, especially to encourage growth.   ‘now is the time to prune roses’  ","categories": [],
        "tags": ["RxJS","NodeJs","Angular"],
        "url": "/2018/05/15/rxjs-pipe-in-depth.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "DevOps-Philosophy",
        "excerpt":":100:DevOps Model Defined   DevOps is the combination of cultural philosophies, practices, and tools that increases an organization’s ability to deliver applications and services at high velocity: evolving and improving products at a faster pace than organizations using traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market.      What’s means to team   These two teams are merged into a single team where the engineers work across the entire application lifecycle, from development and test to deployment to operations, and develop a range of skills not limited to a single function.   DevSecOps   In some DevOps models, quality assurance and security teams may also become more tightly integrated with development and operations and throughout the application lifecycle. When security is the focus of everyone on a DevOps team, this is sometimes referred to as DevSecOps.   # Benefits     Velocity: microservices and continuous delivery let teams take ownership of services and then release updates to them quicker.   Reliablity: CI, CD, automated testing   Governmence:  using infrastructure as code and policy as code, you can define and then track compliance at scale.   mindset   DevOps Cultural Philosophy Transitioning to DevOps requires a change in culture and mindset. At its simplest, DevOps is about removing the barriers between two traditionally siloed teams, development and operations. In some organizations, there may not even be separate development and operations teams; engineers may do both.   Practice   very frequent but small release  One fundamental practice is to perform very frequent but small updates. This is how organizations innovate faster for their customers. These updates are usually more incremental in nature than the occasional updates performed under traditional release practices. Frequent but small updates make each deployment less risky. They help teams address bugs faster because teams can identify the last deployment that caused the error.   microservices  use a microservices architecture to make their applications more flexible and enable quicker innovation.This architecture reduces the coordination overhead of updating applications, and when each service is paired with small, agile teams who take ownership of each service, organizations can move more quickly.   Tech Concepts   Continuous Integration  Continuous integration is a software development practice where developers regularly merge their code changes into a central repository, after which automated builds and tests are run. The key goals of continuous integration are to find and address bugs quicker, improve software quality, and reduce the time it takes to validate and release new software updates.   Continuous Delivery  Continuous delivery is a software development practice where code changes are automatically built, tested, and prepared for a release to production. It expands upon continuous integration by deploying all code changes to a testing environment and/or a production environment after the build stage. When continuous delivery is implemented properly, developers will always have a deployment-ready build artifact that has passed through a standardized test process.   Monitoring and Logging  Organizations monitor metrics and logs to see how application and infrastructure performance impacts the experience of their product’s end user. By capturing, categorizing, and then analyzing data and logs generated by applications and infrastructure, organizations understand how changes or updates impact users, shedding insights into the root causes of problems or unexpected changes. Active monitoring becomes increasingly important as services must be available 24/7 and as application and infrastructure update frequency increases. Creating alerts or performing real-time analysis of this data also helps organizations more proactively monitor their services.   Communication and Collaboration  Increased communication and collaboration in an organization is one of the key cultural aspects of DevOps. The use of DevOps tooling and automation of the software delivery process establishes collaboration by physically bringing together the workflows and responsibilities of development and operations.  ","categories": [],
        "tags": [],
        "url": "/2018/06/02/DevOps-Philosophy.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Agile and SCRUM",
        "excerpt":"Key concept  In Scrum, a team is cross functional, meaning everyone is needed to take a feature from idea to implementation.   Within agile development, Scrum teams are supported by two specific roles. The first is a ScrumMaster, who can be thought of as a coach for the team, helping team members use the Scrum process to perform at the highest level.   The product owner (PO) is the other role, and in Scrum software development, represents the business, customers or users, and guides the team toward building the right product.   ","categories": [],
        "tags": [],
        "url": "/2018/06/06/Scrum-Agile.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Portactor",
        "excerpt":"Better to use smart wait   If possible, you’d better to use smart wait in protractor e2e testing. Which could increase end to end testing efficiency. Normally dev tend to use sleep or wait to insert some stop during execution. What’s the difference of these two? The difference between browser.sleep() and browser.wait() is that browser.wait() expects a specific value/condition. This wait condition is valid of Protractor or any WebDriver framework.   An Expectation for checking an element is visible and enabled such that you can click it.   browser.wait(EC.elementToBeClickable($(‘#abc’)),5000);   Here are some protractor functions utilities.   textToBePresentInElement   An expectation for checking if the given text is present in the element.  browser.wait(EC.textToBePresentInElement($(‘#abc’),’foo’),5000);   presenceOf   An expectation for checking that an element is present on the DOM of a page.  browser.wait(EC.presenceOf($(‘#abc’)),5000);   import { go, click, see, below, slow, type } from 'blue-harvest'; import { browser, ExpectedConditions } from 'protractor';  const client = 'TEST CLIENT'; const clientFullName = 'TEST CLIENT LTD'; const timeOut = 3000;  describe('Should show bell potter', () =&gt; {     beforeEach(async () =&gt; {         await go('http://localhost:4200/#/dashboard')     });      it('should be able to search bell potter', async () =&gt; {          await slow.click('Search clients or accounts')         await type(`TEST`);          browser.wait(see(client), timeOut)         await click(client)          browser.wait(see(clientFullName), timeOut)         expect(await below(clientFullName).see('GO')).toBeTruthy();     }) })    Reference     https://medium.com/@ited.ro/how-to-use-smart-waits-with-protractor-how-to-use-expected-conditions-with-protractor-10c545c670be  ","categories": [],
        "tags": [],
        "url": "/2018/06/07/Protractor.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Foreign Exchange",
        "excerpt":"Foreign Exchange markets   Generally, FX Swap is the biggest portion of FX market, followed by FX Spot, outright forward, Options.   Forward   forward contract or simply a forward is a non-standardized contract between two parties to buy or to sell an asset at a specified future time at a price agreed upon today, making it a type of derivative instrument   A foreign exchange swap has two legs - a spot transaction and a forward transaction - that are executed simultaneously for the same quantity, and therefore offset each other.   ","categories": [],
        "tags": ["FX","Financial"],
        "url": "/2018/06/09/FX.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "KDB",
        "excerpt":"KDB  However kdb+ evaluates expressions right-to-left. There are no precedence rules. The reason commonly given for this behaviour is that it is a much simpler to understand system, many q-gods would agree, beginners however may not.   The kdb+ built-in commands are mostly a single letter. If a system command that is not built-in is entered, that command will be passed to the underlying operating system.   Created with financial institutions in mind, the database was developed as a central repository to store time series data that supports real-time analysis of billions of records.   Columnar databases return answers to some queries in a more efficient way than row-based database management systems.   kdb+ dictionaries, tables and nanosecond time stamps are native data types and are used to store time series data.   In 1998, Kx Systems released kdb, a database built on the language K written by Arthur Whitney. In 2003, kdb+ was released as a 64-bit version of kdb.   kdb/q/kdb+ is both a database (kdb) and a vector language (q). It’s used by almost every major financial institution   Kdb+ is an in-memory column-oriented database based on the concept of ordered lists. In-memory means it primarily stores its data in RAM. This makes it extremely fast with a much simplified database engine but it requires a lot of RAM (Which no longer poses a problem as servers with massive amounts of RAM are now inexpensive). Column oriented database means that each column of data is stored sequentially in memory   Column DB  The main reason why indexes dramatically improve performance on large datasets is that database indexes on one or more columns are typically sorted by value, which makes range queries operations (like the above “find all records with salaries between 40,000 and 50,000” example) very fast (lower time-complexity).   Kdb+ the Database - Column Oriented DB allowing fast timeseries analysis   Column-oriented systems  A column-oriented database serializes all of the values of a column together, then the values of the next column, and so on. For our example table, the data would be stored in this fashion: 10:001,12:002,11:003,22:004; Smith:001,Jones:002,Johnson:003,Jones:004; Joe:001,Mary:002,Cathy:003,Bob:004; 40000:001,50000:002,44000:003,55000:004; In this layout, any one of the columns more closely matches the structure of an index in a row-based system. This may cause confusion that can lead to the mistaken belief a column-oriented store “is really just” a row-store with an index on every column. However, it is the mapping of the data that differs dramatically. In a row-oriented indexed system, the primary key is the rowid that is mapped from indexed data. In the column-oriented system, the primary key is the data, which is mapped from rowids.[2] This may seem subtle, but the difference can be seen in this common modification to the same store:      …;Smith:001;Jones:002,004;Johnson:003;…    Whether or not a column-oriented system will be more efficient in operation depends heavily on the workload being automated. Operations that retrieve all the data for a given object (the entire row) are slower. A row-based system can retrieve the row in a single disk read, whereas numerous disk operations to collect data from multiple columns are required from a columnar database. However, these whole-row operations are generally rare. In the majority of cases, only a limited subset of data is retrieved. In a rolodex application, for instance, collecting the first and last names from many rows to build a list of contacts is far more common than reading all data for any single address. This is even more true for writing data into the database, especially if the data tends to be “sparse” with many optional columns. For this reason, column stores have demonstrated excellent real-world performance in spite of many theoretical disadvantages.   Why are most databases row-oriented?  Imagine we want to add one row somewhere in the middle of our data for 2011-02-26, on the row oriented database no problem, column oriented we will have to move almost all the data! Lucky since we mostly deal with time series new data only appends to the end of our table.   Difference vs row based DB  a subtle point is that unlike most standard SQL which is based on set theory, kdb+ is based on vectors of ordered lists. Where standard SQL has struggled with queries like find the top 3 stocks by price, find the bottom 3 by market cap because it has no concept of order, kdb’s ordering significantly simplifies many queries. This ordered concept allows kdb+ to provide unique timeseries joins that would be be extremely difficult in other variations of SQL and require the use of slow cursors.   Language Q  the primary design objectives of q are expressiveness, speed and efficiency. In these, it is beyond compare. The design trade-off is a terseness that can be disconcerting to programmers coming from verbose traditional database programming environments – e.g., C++, Java, C# or Python – and a relational DBMS   Q evolved from APL (A Programming Language), which was first invented as a mathematical notation by Kenneth Iverson at Harvard University in the 1950s. APL was introduced in the 1960s by IBM as a vector programming language, meaning that it processes a list of numbers in a single operation. It was successful in finance and other industries that required heavy number crunching.   q is Kx’s proprietary language. It’s a powerful, concise and elegant array language, which means that a production system could just be a single page of code, not pages and pages of code and a nightmare to maintain. Clearly there is an initial investment in learning it, but the power it gives you to manipulate streaming, real-time and historical data makes that initial investment really worthwhile.   q language - fast, interpreted vector based language   Q is a interpreted vector based dynamically typed language built for speed and expressiveness.   Since q is interpreted you can enter commands straight into the console there is no waiting for compilation, feedback is instantaneous.   Key features of Q  Interpreted Q is interpreted, not compiled. During execution, data and functions live in an in-memory workspace. Iterations of the development cycle tend to be quick because all run-time information needed to test and debug is immediately available in the workspace. Q programs are stored and executed as simple text files called scripts. The interpreter’s eval and parse routines are exposed so that you can dynamically generate code in a controlled manner.   Types Q is a dynamically typed language, in which type checking is mostly unobtrusive. Each variable has the type of its currently assigned value and type promotion is automatic for most numeric operations. Types are checked on operations to homogenous lists.   Evaluation Order While q is entered left-to-right, expressions are evaluated right-to-left or, as the q gods prefer, left of right – meaning that a function is applied to the argument on its right. There is no operator precedence and function application can be written without brackets. Punctuation noise is significantly reduced.   Null and Infinity Values In classical SQL, the value NULL represents missing data for a field of any type and takes no storage space. In q, null values are typed and take the same space as non-nulls. Numeric types also have infinity values. Infinite and null values can participate in arithmetic and other operations with (mostly) predictable results.   Integrated I/O I/O is done through function handles that act as windows to the outside world. Once such a handle is initialized, passing a value to the handle is a write.   Table Oriented Give up objects, ye who enter here. In contrast to traditional languages, you’ll find no classes, objects, inheritance and virtual methods in q. Instead, q has tables as first class entities. The lack of objects is not as severe as might first appear. Objects are essentially glorified records (i.e., entities with named fields), which are modeled by q dictionaries. A table can be viewed as a list of record dictionaries.   Ordered Lists Because classical SQL is the algebra of sets – which are unordered with no duplicates – row order and column order are not defined, making time series processing cumbersome and slow. In q, data structures are based on ordered lists, so time series maintain the order in which they are created. Moreover, simple lists occupy contiguous storage, so processing big data is fast. Very fast.   Column Oriented SQL tables are organized as rows distributed across storage and operations apply to fields within a row. Q tables are column lists in contiguous storage and operations apply on entire columns.   In-Memory Database One can think of kdb+ as an in-memory database with persistent backing. Since data manipulation is performed with q, there is no separate stored procedure language. In fact, kdb+ comprises serialized q column lists written to the file system and then mapped into memory.   In q, data structures are based on ordered lists, so time series maintain the order in which they are created. Moreover, simple lists occupy contiguous storage, so processing big data is fast. Very fast.   Q tables are column lists in contiguous storage and operations apply on entire columns.   Sample of Q code   q)l:10 12 14 16 18 22 32 45 q)sum l 169 q)avg l 21.125 q)l*10 100 120 140 160 180 220 320 450  q)k:til 8 q)k 0 1 2 3 4 5 6 7 q)l+k 10 13 16 19 22 27 38 52            Notice in the example code above the absence of loops, no for/while/do yet we could easily express adding one array to another. This is because the vector/list is the primary unit of data in kdb+. Operations are intended to be performed and expressed as being on an entire set of data. Dictionaries can be defined using lists, they provide a hashmap datastructure for quick lookups. Tables are constructed from dictionaries and lists. This brevity of data structures is actually one of the attributes that gives q its ability to express concisely what would take many lines in other languages.   Some of the practical applications of being able to combine both streaming and real-time data together with historical, all in the same database?   The most important thing is the simplicity, which translates into speed and ease of doing analysis. For example, it allows you to do complicated, time-critical analysis, such as pre-trade risk. This means that you are likely to see interesting trading opportunities before those who are using the same off-the-shelf solution as everybody else. So you’re there first and you’re there so early, you can afford to do comprehensive pre-trade risk analysis, and you’re able to look for patterns you’ve seen in the past.   In addition to capturing market data, firms are using kdb+ for order-book management, algorithmic trading, and risk assessment. “They are using kdb+/q for queries being performed on both streaming or historical data — the latter easily accommodating research and back-testing,”   DeltaFlow, a platform from First Derivatives that’s based on kdb+, is used by traders for high volume, low-latency algorithmic trading and by regulators for real-time detection of market abuse and unauthorized trading activity across multiple asset classes.   Combined power of kdb+/q   What’s beautiful about kdb+ is that since tables are columns of vectors, all the power of the q language can be used as easily on table data as it was on lists. Where we had sum[l],avg[l],weightedAvg[l1;l2] of lists we can write similar qSQL:  select avg price, sum volume, weightedAvg[time;price] from trade  Want to apply a function to a timeseries, simply place it inline:  select {a:avg x; sqrt avg (xx)-aa} price from trade   Q commadns   To obtain official console display of any q value, apply the built-in function show to it.   q)show a:42   comments   At least one whitespace character must separate / intended to begin a comment from any text to the left of it on a line.   boolean  Boolean values in q are stored in a single byte and are denoted as the binary values they really are with an explicit type suffix b. One way to generate boolean values is to test for equality.   q)42=40+2 1b   date  One interesting and useful feature of q temporal values is that, as integral values under the covers, they naturally participate in arithmetic. For example, to advance a date five days, add 5.   q)2000.01.01+5 _ Or to advance a time by one microsecond (i.e., 1000 nanoseconds) add 1000.   q)12:00:00.000000000+1000 _ Or to verify that temporal values are indeed their underlying values, test for equality.   q)2000.01.01=0   Symbols  Symbols are denoted by a leading back-quote (called “back tick” in q-speak) followed by characters. Symbols without embedded blanks or other special characters can be entered literally into the console. q)`aapl _   Since symbols are atoms, any two can be tested for equality.   q)aapl=apl _   list  The fundamental q data structure is a list, which is an ordered collection of items sequenced from left to right. The notation for a general list encloses items with ( and ) and uses ; as separator. Spaces after the semi-colons are optional but can improve readability.   q)(1; 1.2; `one) _   In the case of a homogenous list of atoms, called a simple list, q adopts a simplified format for both storage and display. The parentheses and semicolons are dropped. For example, a list of underlying numeric type separates its items with a space.   q)(1; 2; 3) 1 2 3   A simple list of booleans is juxtaposed with no spaces and has a trailing b type indicator.   q)(1b; 0b; 1b) 101b A simple list of symbols is displayed with no separating spaces.   q)(one; two; three) onetwothree   basic operations  to construct and manipulate lists. The most fundamental is til, which takes a non-negative integer n and returns the first n integers starting at 0 (n itself is not included in the result).   q)til 10 0 1 2 3 4 5 6 7 8 9   til list tips  Be mindful that q always evaluates expressions from right to left and that operations work on vectors whenever possible.   q)1+til 10 1 2 3 4 5 6 7 8 9 10   Similarly, we obtain the first 10 even numbers and the first ten odd numbers.   q)2til 10 _ q)1+2til 10 _ Finally, we obtain the first 10 even numbers starting at 42.   q)42+2*til 10 _   Another frequently used list primitive is join , that returns the list obtained by concatenating its right operand to its left operand.   q)1 2 3,4 5 1 2 3 4 5  extract  To extract items from the front or back of a list, use the take operator #. Positive argument means take from the front, negative from the back.   q)2#til 10 0 1 q)-2#til 10      Applying # always results in a list.    In particular, the idiom 0# returns an empty list of the same type as the first item in its argument. Using an atom argument is a succinct way to create a typed empty list of the type of the atom.   q)0#1 2 3 `long$()   Should you extract more items than there are in the list, # restarts at the beginning and continues extracting. It does this until the specified number of items is reached.   q)5#1 2 3 1 2 3 1 2   As with atoms, a list can be assigned to a variable.   q)L:10 20 30 The items of a list can be accessed via indexing, which uses square brackets and is relative to 0.   q)L[0] 10   Function  Conceptually, a q function is a sequence of steps that produces an output result from an input value. Since q is not purely functional, these rules can interact with the world by reaching outside the context of the function. Such actions are called side effects and should be carefully controlled.   Function definition is delimited by matching curly braces { and }. Immediately after the opening brace, the formal parameters are names enclosed in square brackets [ and ] and separated by semi-colons. These parameters presumably appear in the body of the function, which follows the formal parameters and is a succession of expressions sequenced by semi-colons.   Following is a simple function that returns the square of its input. On the next line we assign the same function to the variable sq. The whitespace is optional.   q){[x] xx} _ q)sq:{[x] xx} _   Here is a function that takes two input values and returns the sum of their squares.   q){[x;y] a:xx; b:yy; a+b} _ q)pyth:{[x;y] a:xx; b:yy; a+b} _   Here are the previous functions applied to arguments.   q){[x] xx}[5] 25 q)sq[5] _ q){[x;y] a:xx; b:y*y; a+b}[3;4] 25 q)pyth[3;4] _   monadic function  In q, as in most functional languages, we don’t need no stinkin’ brackets for application of a monadic function – i.e., with one parameter. Simply separate the function from its argument by whitespace. This is called function juxtaposition.   q){xx} 5 _ q)f:{xx} q)f 5 _   x,y,z  It is common in mathematics to use function parameters x, y, or z. If you are content with these names (in the belief that descriptive names provide no useful information to the poor soul reading your code), you can omit their declaration and q will understand that you mean the implicit parameters x, y, and z in that order.   q){xx}[5] 25 q){a:xx; b:y*y; a+b}[3;4] 25   verbs  higher order functions, or as they are called in q, adverbs.   In words, we tell q to start with the initial value of 0 in the accumulator and then modify + with the adverb / so that it adds across the list.   q)0 +/ 1 2 3 4 5 15   In this situation we don’t really need the flexibility to specify the initial value of the accumulator. It suffices to start with the first item of the list and proceed across the rest of the list. There is an even simpler form for this case.   q)(+/) 1 2 3 4 5   for loop  If you are new to functional programming, you may think, “Big deal, I write for loops in my sleep.”   More importantly, you can focus on what you want done without the irrelevant scaffolding of how to set up control structures. This is called declarative programming.   What else can we do with our newfound adverb? Change addition to multiplication for factorial.   q)(*/) 1+til 10 3628800   larger vs smaller  The fun isn’t limited to arithmetic primitives. We introduce |, which returns the larger of its operands and &amp;, which returns the smaller of its operands.   q)42|98 98 q)42&amp;98                  Use       or &amp; with over and you have maximum or minimum.           q)(|/) 20 10 40 30 40 q)(&amp;/) 20 10 40 30   command ‘over’ adverb  Some applications of / are so common that they have their own names.   q)sum 1+til 10  55 q)prd 1+til 10 “o” _ q)max 20 10 40 30  _ q)min 20 10 40 30   At this point the / pattern should be clear: it takes a given function and produces a new function that accumulates across the original list, producing a single result. In particular, / converts a dyadic function to a monadic aggregate function – i.e., one that collapses a list to an atom.   data type  long¶   In q versions 3.0 and later, the basic integer type is a signed eight-byte integer, called long. A literal is identified as a long by the fact that it contains only numeric digits, with an optional leading minus sign, and no decimal point. It may also have an optional trailing type indicator j indicating it is a long and not another integer type. Here is a typical long integer value.   q)42 42 Observe that the type indicator j is accepted but redundant.   q)42j 42   The short type represents a two-byte signed integer and requires the trailing type indicator h. For example,   q)-123h _ Similarly, the int type represents a four-byte signed integer and requires the trailing type indicator i.   The float type represents an IEEE standard eight-byte floating-point number, often called “double” in traditional languages. A float can hold (at least) 15 decimal digits of precision. It is denoted by optionally signed numeric digits with either a decimal point or an optional trailing type indicator f. Observe that the console shortens the display of floats with no significant digits to the right of the decimal.   You can change this by using the \\P command (note upper case) to specify a display width up to 16 digits. If you issue \\P 0 the console will display all 17 decimal digits of the underlying binary representation, although the last digit is unreliable.   boolean¶   The boolean type uses one byte to store a bit and is denoted by the bit value with the trailing type indicator b. There are no keywords for ‘true’ or ‘false’, nor are there separate logical operators for booleans.   q)0b _ q)1b   Text Data¶   There are two atomic text types in q. They are more akin to the SQL types CHAR and VARCHAR than the character types of traditional languages.   2.4.1 char¶   A char holds an individual ASCII or 8-bit Unicode character that is stored in one byte. It corresponds to a SQL CHAR. It is denoted by a single character enclosed in double quotes.   q)”q”   Some keyboard characters – e.g., the double-quote – cannot be entered directly into a char since they have special meaning in the q console. As in C, special characters are escaped with a preceding back-slash . The console display somewhat confusingly displays the escape, but the following are all actually single characters.   q)”\"”  “\"” q)”\\”  _ q)”\\n”  _ q)”\\r”  _ q)”\\t”  _ Also as in C, you can escape any ASCII character by specifying its underlying numeric value as three octal digits.   q)”\\142” “b”   symbol¶   A symbol is an atom holding text. It is denoted by a leading back-quote, read “back tick” in q-speak.   q)q _ q)zaphod _   a symbol is not a collection of char. The symbol `a and the char “a” are not the same, as we can see by asking q if they are identical.   q)`a~”a” 0b   list   Index Notation¶   To access the item at index i in a list, follow the list immediately with [i]. This is called item indexing. For example,   q)(100; 200; 300)[0]   Indexed Assignment¶   Items in a list can also be assigned via item indexing. Thus,   q)L:1 2 3 q)L[1]:42 q)L 1 42 3   An omitted index returns the entire list.   q)L:10 20 30 40 q)L[] 10 20 30 40  ","categories": [],
        "tags": [],
        "url": "/2018/06/12/KDB.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Locking and multithreading",
        "excerpt":"Single Writer principle   There is a lot of research in computer science for managing this contention that boils down to 2 basic approaches.  One is to provide mutual exclusion to the contended resource while the mutation takes place; the other is to take an optimistic strategy and swap in the changes if the underlying resource has not changed while you created the new copy.   Memory Barier   Memory barriers, or fences, are a set of processor instructions used to apply ordering limitations on memory operations.   The keyword volatile prevents this problem because it establishes a happens before relationship between the write to the turn variable and the write to the intentFirst variable. The compiler cannot re-order these write operations and if necessary it must forbid the processor from doing so with a memory barrier.   A memory barrier, also known as a membar, memory fence or fence instruction, is a type of barrier instruction that causes a central processing unit (CPU) or compiler to enforce an ordering constraint on memory operations issued before and after the barrier instruction. This typically means that operations issued prior to the barrier are guaranteed to be performed before operations issued after the barrier.   Memory barriers are necessary because most modern CPUs employ performance optimizations that can result in out-of-order execution. This reordering of memory operations (loads and stores) normally goes unnoticed within a single thread of execution, but can cause unpredictable behaviour in concurrent programs and device drivers unless carefully controlled.   Non blocking programing  Implementation With few exceptions, non-blocking algorithms use atomic read-modify-write primitives that the hardware must provide, the most notable of which is compare and swap (CAS).   Compare And Swap   It compares the contents of a memory location with a given value and, only if they are the same, modifies the contents of that memory location to a new given value. This is done as a single atomic operation. The atomicity guarantees that the new value is calculated based on up-to-date information; if the value had been updated by another thread in the meantime, the write would fail. The result of the operation must indicate whether it performed the substitution; this can be done either with a simple boolean response (this variant is often called compare-and-set), or by returning the value read from the memory location (not the value written to it).   Here is the pseudo code  function cas(p : pointer to int, old : int, new : int) returns bool {     if *p ≠ old {         return false     }     *p ← new     return true }   This operation is used to implement synchronization primitives like semaphores and mutexes, as well as more sophisticated lock-free and wait-free algorithms.   Algorithms built around CAS typically read some key memory location and remember the old value. Based on that old value, they compute some new value. Then they try to swap in the new value using CAS, where the comparison checks for the location still being equal to the old value. If CAS indicates that the attempt has failed, it has to be repeated from the beginning: the location is re-read, a new value is re-computed and the CAS is tried again.   A common workaround is to add extra “tag” or “stamp” bits to the quantity being considered. For example, an algorithm using compare and swap on a pointer might use the low bits of the address to indicate how many times the pointer has been successfully modified. Because of this, the next compare-and-swap will fail, even if the addresses are the same, because the tag bits will not match. This does not completely solve the problem, as the tag bits will eventually wrap around, but helps to avoid it. Some architectures provide a double-word compare and swap, which allows for a larger tag. This is sometimes called ABAʹ since the second A is made slightly different from the first. Such tagged state references are also used in transactional memory.   Priority Inversion   Consider two tasks H and L, of high and low priority respectively, either of which can acquire exclusive use of a shared resource R. If H attempts to acquire R after L has acquired it, then H becomes blocked until L relinquishes the resource. Sharing an exclusive-use resource (R in this case) in a well-designed system typically involves L relinquishing R promptly so that H (a higher priority task) does not stay blocked for excessive periods of time. Despite good design, however, it is possible that a third task M of medium priority (p(L) &lt; p(M) &lt; p(H), where p(x) represents the priority for task (x)) becomes runnable during L’s use of R. At this point, M being higher in priority than L, preempts L, causing L to not be able to relinquish R promptly, in turn causing H—the highest priority process—to be unable to run. This is called priority inversion where a higher priority task is preempted by a lower priority one.   RCU  In computer science, read-copy-update (RCU) is a synchronization mechanism based on mutual exclusion. It is used when performance of reads is crucial and is an example of space–time tradeoff, enabling fast operations at the cost of more space.   Read-copy-update allows multiple threads to efficiently read from shared memory by deferring updates after pre-existing reads to a later time while simultaneously marking the data, ensuring new readers will read the updated data. This makes all readers proceed as if there were no synchronization involved, hence they will be fast, but also making updates more difficult.   package java/util/concurrent/atomic   A small toolkit of classes that support lock-free thread-safe programming on single variables. In essence, the classes in this package extend the notion of volatile   values, fields, and array elements to those that also provide an atomic conditional update operation of the form:     boolean compareAndSet(expectedValue, updateValue);   This method (which varies in argument types across different classes) atomically sets a variable to the updateValue if it currently holds the expectedValue, reporting true on success. The classes in this package also contain methods to get and unconditionally set values, as well as a weaker conditional atomic update operation weakCompareAndSet described below.   The specifications of these methods enable implementations to employ efficient machine-level atomic instructions that are available on contemporary processors. However on some platforms, support may entail some form of internal locking. Thus the methods are not strictly guaranteed to be non-blocking – a thread may block transiently before performing the operation.   Instances of classes AtomicBoolean, AtomicInteger, AtomicLong, and AtomicReference each provide access and updates to a single variable of the corresponding type. Each class also provides appropriate utility methods for that type. For example, classes AtomicLong and AtomicInteger provide atomic increment methods. One application is to generate sequence numbers, as in:   class Sequencer {    private final AtomicLong sequenceNumber      = new AtomicLong(0);    public long next() {      return sequenceNumber.getAndIncrement();    }  }   The AtomicIntegerArray, AtomicLongArray, and AtomicReferenceArray classes further extend atomic operation support to arrays of these types. These classes are also notable in providing volatile access semantics for their array elements, which is not supported for ordinary arrays.   Volatile   The Java volatile keyword is used to mark a Java variable as “being stored in main memory”. More precisely that means, that every read of a volatile variable will be read from the computer’s main memory, and not from the CPU cache, and that every write to a volatile variable will be written to main memory, and not just to the CPU cache.   What’s wrong to volatile?   The Java volatile keyword guarantees visibility of changes to variables across threads. This may sound a bit abstract, so let me elaborate.   In a multithreaded application where the threads operate on non-volatile variables, each thread may copy variables from main memory into a CPU cache while working on them, for performance reasons. If your computer contains more than one CPU, each thread may run on a different CPU. That means, that each thread may copy the variables into the CPU cache of different CPUs.   With non-volatile variables there are no guarantees about when the Java Virtual Machine (JVM) reads data from main memory into CPU caches, or writes data from CPU caches to main memory. This can cause several problems.   visibility problem  The problem with threads not seeing the latest value of a variable because it has not yet been written back to main memory by another thread, is called a “visibility” problem. The updates of one thread are not visible to other threads.   The Java volatile Visibility Guarantee   The Java volatile keyword is intended to address variable visibility problems. By declaring the counter variable volatile all writes to the counter variable will be written back to main memory immediately. Also, all reads of the counter variable will be read directly from main memory.   Full volatile Visibility Guarantee   Actually, the visibility guarantee of Java volatile goes beyond the volatile variable itself. The visibility guarantee is as follows:   If Thread A writes to a volatile variable and Thread B subsequently reads the same volatile variable, then all variables visible to Thread A before writing the volatile variable, will also be visible to Thread B after it has read the volatile variable. If Thread A reads a volatile variable, then all all variables visible to Thread A when reading the volatile variable will also be re-read from main memory.   The Java volatile Happens-Before Guarantee   To address the instruction reordering challenge, the Java volatile keyword gives a “happens-before” guarantee, in addition to the visibility guarantee. The happens-before guarantee guarantees that:      Reads from and writes to other variables cannot be reordered to occur after a write to a volatile variable, if the reads / writes originally occurred before the write to the volatile variable.   The reads / writes before a write to a volatile variable are guaranteed to \"happen before\" the write to the volatile variable. Notice that it is still possible for e.g. reads / writes of other variables located after a write to a volatile to be reordered to occur before that write to the volatile. Just not the other way around. From after to before is allowed, but from before to after is not allowed.           Reads from and writes to other variables cannot be reordered to occur before a read of a volatile variable, if the reads / writes originally occurred after the read of the volatile variable. Notice that it is possible for reads of other variables that occur before the read of a volatile variable can be reordered to occur after the read of the volatile. Just not the other way around. From before to after is allowed, but from after to before is not allowed.            In short: ==before write,  after read==.       Limitations of volatile   Even if the volatile keyword guarantees that all reads of a volatile variable are read directly from main memory, and all writes to a volatile variable are written directly to main memory, there are still situations where it is not enough to declare a variable volatile   Performance Considerations of volatile   Reading and writing of volatile variables causes the variable to be read or written to main memory. Reading from and writing to main memory is more expensive than accessing the CPU cache. Accessing volatile variables also prevent instruction reordering which is a normal performance enhancement technique. Thus, you should only use volatile variables when you really need to enforce visibility of variables.  ","categories": [],
        "tags": ["CAS","Concurrent"],
        "url": "/2018/06/12/Locking-And-Multithreading.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Jboss tips",
        "excerpt":"commands:   to list all deployed applications  jboss\\jboss-eap-6.4\\bin\\jboss-cli.bat –connect –controller=localhost:7373 –command=/deployment=*:read-attribute(name=name)   to list JNDI tree:   cd /subsystem=mail ls /subsystem=mail/mail-session=java:jboss/mail/payment_mail    /subsystem=naming:jndi-view()   }, \"mail\" =&gt; {     \"class-name\" =&gt; \"javax.naming.Context\",     \"children\" =&gt; {         \"Default\" =&gt; {             \"class-name\" =&gt; \"javax.mail.Session\",             \"value\" =&gt; \"javax.mail.Session@22951e8f\"         },         \"payment_mail\" =&gt; {             \"class-name\" =&gt; \"javax.mail.Session\",             \"value\" =&gt; \"javax.mail.Session@548df9e2\"         }     } },    &lt;management-interfaces&gt;             &lt;native-interface security-realm=\"ManagementRealm\"&gt;                 &lt;socket-binding native=\"management-native\"/&gt;             &lt;/native-interface&gt;             &lt;http-interface security-realm=\"ManagementRealm\"&gt;                 &lt;socket-binding http=\"management-http\"/&gt;             &lt;/http-interface&gt;         &lt;/management-interfaces&gt;     &lt;/management&gt; xxx &lt;socket-binding-group name=\"standard-sockets\" default-interface=\"public\" port-offset=\"${jboss.socket.binding.port-offset:0}\"&gt;         &lt;socket-binding name=\"management-native\" interface=\"management\" port=\"${jboss.management.native.port:7373}\"/&gt;         &lt;socket-binding name=\"management-http\" interface=\"management\" port=\"${jboss.management.http.port:7371}\"/&gt; \t\t \t\t  ","categories": [],
        "tags": ["Jboss","Java"],
        "url": "/2018/06/14/JBoss-Console.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "flexbox",
        "excerpt":"How Flexbox works — explained with big, colorful, animated gifs   Flexbox promises to save us from the evils of plain CSS (like vertical alignment).   Well, Flexbox does deliver on that goal. But mastering its new mental model can be challenging.   So let’s take an animated look at how Flexbox works, so we can use it to build better layouts.   Flexbox’s underlying principle is to make layouts flexible and intuitive.   To accomplish this, it lets containers decide for themselves how to evenly distribute their children — including their size and the space between them.   This all sounds good in principle. But let’s see what it looks like in practice.   In this article, we’ll dive into the 5 most common Flexbox properties. We’ll explore what they do, how you can use them, and what their results will actually look like.   Property #1: Display: Flex Here’s our example webpage:   You have four colored divs of various sizes, held within a grey container div. As of now, each div has defaulted to display: block. Each square thus takes up the full width of its line.   In order to get started with Flexbox, you need to make your container into a flex container. This is as easy as:   #container {   display: flex; }   Not a lot has changed — your divs are displayed inline now, but that’s about it. But behind the scenes, you’ve done something powerful. You gave your squares something called a flex context.   You can now start to position them within that context, with far less difficulty than traditional CSS.   Property #2: Flex Direction A Flexbox container has two axes: a main axis and a cross axis, which default to looking like this:   By default, items are arranged along the main axis, from left to right. This is why your squares defaulted to a horizontal line once you applied display: flex.   Flex-direction, however, let’s you rotate the main axis.   #container {   display: flex;   flex-direction: column; }   There’s an important distinction to make here: flex-direction: column doesn’t align the squares on the cross axis instead of the main axis. It makes the main axis itself go from horizontal to vertical.   There are a couple of other options for flex-direction, as well: row-reverse and column-reverse.   Property #3: Justify Content Justify-content controls how you align items on the main axis.   Here, you’ll dive a bit deeper into the main/cross axis distinction. First, let’s go back to flex-direction: row.   #container {   display: flex;   flex-direction: row;   justify-content: flex-start; } You have five commands at your disposal to use justify-content:   Flex-start Flex-end Center Space-between Space-around   Space-around and space-between are the least intuitive. Space-between gives equal space between each square, but not between it and the container.   Space-around puts an equal cushion of space on either side of the square — which means the space between the outermost squares and the container is half as much as the space between two squares (each square contributing a non-overlapping equal amount of margin, thus doubling the space).   A final note: remember that justify-content works along the main-axis, and flex-direction switches the main-axis. This will be important as you move to…   Property #4: Align Items If you ‘get’ justify-content, align-items will be a breeze.   As justify-content works along the main axis, align-items applies to the cross axis.   Let’s reset our flex-direction to row, so our axes look the same as the above image.   Then, let’s dive into the align-items commands.   flex-start flex-end center stretch baseline The first three are exactly the same as justify-content, so nothing too fancy here.   The next two are a bit different, however.   You have stretch, in which the items take up the entirety of the cross-axis, and baseline, in which the bottom of the paragraph tags are aligned.   (Note that for align-items: stretch, I had to set the height of the squares to auto. Otherwise the height property would override the stretch.)   For baseline, be aware that if you take away the paragraph tags, it aligns the bottom of the squares instead, like so:   To demonstrate the main and cross axes better, let’s combine justify-content and align-items and see how centering works different for the two flex-direction commands:   With row, the squares are set up along a horizontal main axis. With column, they fall along a vertical main axis.   Even if the squares are centered both vertically and horizontally in both cases, the two are not interchangeable!   Property #5: Align Self Align-self allows you to manually manipulate the alignment of one particular element.   It’s basically overriding align-items for one square. All the properties are the same, though it defaults to auto, in which it follows the align-items of the container.   #container {   align-items: flex-start; } .square#one {   align-self: center; } // Only this square will be centered. Let’s see what this looks like. You’ll apply align-self to two squares, and for the rest apply align-items: center and flex-direction: row.   Conclusion Even though we’ve just scratched the surface of Flexbox, these commands should be enough for you to handle most basic alignments — and to vertically align to your heart’s content.      https://github.com/angular/flex-layout   https://medium.freecodecamp.org/an-animated-guide-to-flexbox-d280cf6afc35  ","categories": [],
        "tags": [],
        "url": "/2018/06/14/flex-box.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Ansible",
        "excerpt":"Ansible: What Is It Good For?  Ansible is often described as a configuration management tool, and is typically mentioned in the same breath as Chef, Puppet, and Salt. When we talk about configuration management, we are typically talking about writing some kind of state description for our servers, and then using a tool to enforce that the servers are, indeed, in that state: the right packages are installed, configuration files contain the expected values and have the expected permissions, the right services are running, and so on. Like other configuration management tools, Ansible exposes a domain-specific language (DSL) that you use to describe the state of your servers.   These tools also can be used for doing deployment as well. When people talk about deployment, they are usually referring to the process of taking software that was written in-house, generating binaries or static assets (if necessary), copying the required files to the server(s), and then starting up the services. Capistrano and Fabric are two examples of open-source deployment tools. Ansible is a great tool for doing deployment as well as configuration management. Using a single tool for both configuration management and deployment makes life simpler for the folks responsible for operations.   Some people talk about the need for orchestration of deployment. This is where multiple remote servers are involved, and things have to happen in a specific order. For example, you need to bring up the database before bringing up the web servers, or you need to take web servers out of the load balancer one at a time in order to upgrade them without downtime. Ansible’s good at this as well, and is designed from the ground up for performing actions on multiple servers. Ansible has a refreshingly simple model for controlling the order that actions happen in.   Finally, you’ll hear people talk about provisioning new servers. In the context of public clouds such as Amazon EC2, this refers to spinning up a new virtual machine instance. Ansible’s got you covered here, with a number of modules for talking to clouds, including EC2, Azure, Digital Ocean, Google Compute Engine, Linode, and Rackspace, as well as any clouds that support the OpenStack APIs.   Architecture  As with most configuration management software, Ansible has two types of servers: controlling machines and nodes. First, there is a single controlling machine which is where orchestration begins. Nodes are managed by a controlling machine over SSH. The controlling machine describes the location of nodes through its inventory.   Agentless  In contrast with popular configuration management software — such as Chef, Puppet, and CFEngine — Ansible uses an agentless architecture.[14] With an agent-based architecture, nodes must have a locally installed daemon that communicates with a controlling machine. With an agentless architecture, nodes are not required to install and run background daemons to connect with a controlling machine. This type of architecture reduces the overhead on the network by preventing the nodes from polling the controlling machine.   Playbook  Playbooks are Ansible’s configuration, deployment, and orchestration language. They can describe a policy you want your remote systems to enforce, or a set of steps in a general IT process.   I like to think of Ansible playbooks as executable documentation. It’s like the README file that describes the commands you had to type out to deploy your software, except that the instructions will never go out-of-date because they are also the code that gets executed directly.   If Ansible modules are the tools in your workshop, playbooks are your instruction manuals, and your inventory of hosts are your raw material.   In Ansible, a script is called a playbook. A playbook describes which hosts (what Ansible calls remote servers) to configure, and an ordered list of tasks to perform on those hosts.   To execute the playbook using the ansible-playbook command. In the example, the playbook is named webservers.yml, and is executed by typing:  $ ansible-playbook webservers.yml  Ansible will make SSH connections in parallel to web1, web2, and web3. It will execute the first task on the list on all three hosts simultaneously. In this example, the first task is installing the nginx apt package (since Ubuntu uses the apt package manager), so the task in the playbook would look something like this:  - name: install nginx   apt: name=nginx   Ansible will:      Generate a Python script that installs the nginx package.   Copy the script to web1, web2, and web3.   Execute the script on web1, web2, web3.   Wait for the script to complete execution on all hosts.   Ansible will then move to the next task in the list, and go through these same four steps. It’s important to note that:     Ansible runs each task in parallel across all hosts.   Ansible waits until all hosts have completed a task before moving to the next task.   Ansible runs the tasks in the order that you specify them.   Variable  Variable names should be letters, numbers, and underscores. Variables should always start with a letter.   foo_port is a great variable. foo5 is fine too. foo-port, foo port, foo.port and 12 are not valid variable names.   Defining Variables in Playbooks The simplest way to define variables is to put a vars section in your playbook with the names and values of variables.   Ansible also allows you to put variables into one or more files, using a section called vars_files.   We would replace the vars section with a vars_files that looks like this:   vars_files:  - nginx.yml   ## nginx.yml key_file: /etc/nginx/ssl/nginx.key cert_file: /etc/nginx/ssl/nginx.crt conf_file: /etc/nginx/sites-available/default server_name: localhost  To debug variable  - debug: var=myvarname   Registering Variables  Often, you’ll find that you need to set the value of a variable based on the result of a task. To do so, we create a registered variable using the register clause when invoking a module.   In order to use the login variable later, we need to know what type of value to expect. The value of a variable set using the register clause is always a dictionary, but the specific keys of the dictionary are different, depending on the module that was invoked.   ACCESSING DICTIONARY KEYS IN A VARIABLE If a variable contains a dictionary, then you can access the keys of the dictionary using either a dot (.) or a subscript ([]).   facts  When Ansible gathers facts, it connects to the host and queries the host for all kinds of details about the host: CPU architecture, operating system, IP addresses, memory info, disk info, and more. This information is stored in variables that are called facts, and they behave just like any other variable does.   Here’s a simple playbook that will print out the operating system of each server:  - name: print out operating system   hosts: all   gather_facts: True   tasks:   - debug: var=ansible_distribution   Viewing All Facts Associated with a Server Ansible implements fact collecting through the use of a special module called the setup module. You don’t need to call this module in your playbooks because Ansible does that automatically when it gathers facts. However, if you invoke it manually with the ansible command-line tool, like this:   $ ansible server1 -m setup   interactive mode  If Ansible did not succeed, add the -vvvv flag to see more details about the error:   $ ansible testserver -i hosts -m ping -vvvv   We can see that the module succeeded. The “changed”: false part of the output tells us that executing the module did not change the state of the server. The “ping”: “pong” text is output that is specific to the ping module.   Simplifying with the ansible.cfg File  We had to type a lot of text in the inventory file to tell Ansible about our test server. Fortunately, Ansible has a number of ways you can specify these sorts of variables so we don’t have to put them all in one place.   Right now, we’ll use one such mechanism, the ansible.cfg file, to set some defaults so we don’t need to type as much.   WHERE SHOULD I PUT MY ANSIBLE.CFG FILE?  Ansible looks for an ansible.cfg file in the following places, in this order:   File specified by the ANSIBLE_CONFIG environment variable   ./ansible.cfg (ansible.cfg in the current directory)   ~/.ansible.cfg (.ansible.cfg in your home directory)   /etc/ansible/ansible.cfg   I typically put an ansible.cfg in the current directory, alongside my playbooks. That way, I can check it into the same version control repository my playbooks are in.   Run command remotely  I like to use the ansible command-line tool to run arbitrary commands on remote machines, like parallel SSH. You can execute arbitrary commands with the command module. When invoking this module, you also need to pass an argument to the module with the -a flag, which is the command to run.   For example, to check the uptime of our server, we can use:   $ ansible testserver -m command -a uptime   The command module is so commonly used that it’s the default module, so we can omit it:   $ ansible testserver -a uptime $ ansible testserver -a “tail /var/log/dmesg”   inventory  WARNING Although Ansible adds the localhost to your inventory automatically, you have to have at least one other host in your inventory file; otherwise, ansible-playbook will terminate with the error:   ERROR: provided hosts list is empty   property “Changed”  The changed key is present in the return value of all Ansible modules, and Ansible uses it to determine whether a state change has occurred. For the command and shell module, this will always be set to true unless overridden with the changed_when clause   ignore error   Ignoring when a module returns an error  - name: Run myprog   command: /opt/myprog   register: result   ignore_errors: True - debug: var=result   Data type  All members of a list are lines beginning at the same indentation level starting with a “- “ (a dash and a space):   --- # A list of tasty fruits fruits:     - Apple     - Orange     - Strawberry     - Mango ...   A dictionary is represented in a simple key: value form (the colon must be followed by a space):   # An employee record martin:     name: Martin D'vloper     job: Developer     skill: Elite   More complicated data structures are possible, such as lists of dictionaries, dictionaries whose values are lists or a mix of both:   # Employee records -  martin:     name: Martin D'vloper     job: Developer     skills:       - python       - perl       - pascal -  tabitha:     name: Tabitha Bitumen     job: Developer     skills:       - lisp       - fortran       - erlang   Dictionaries and lists can also be represented in an abbreviated form if you really want to:  --- martin: {name: Martin D'vloper, job: Developer, skill: Elite} fruits: ['Apple', 'Orange', 'Strawberry', 'Mango']  These are called “Flow collections”.   span multiple lines  Values can span multiple lines using | or &gt;. Spanning multiple lines using a “Literal Block Scalar” | will include the newlines and any trailing spaces. Using a “Folded Block Scalar” &gt; will fold newlines to spaces; it’s used to make what would otherwise be a very long line easier to read and edit. In either case the indentation will be ignored. Examples are:  include_newlines: |             exactly as you see             will appear these three             lines of poetry  fold_newlines: &gt;             this is really a             single line of text             despite appearances   commands   file   file - Sets attributes of files   Sets attributes of files, symlinks, and directories, or removes files/symlinks/directories. Many other modules support the same options as the file module - including copy, template, and assemble.   # change file ownership, group and mode. When specifying mode using octal numbers, first digit should always be 0. - file:     path: /etc/foo.conf     owner: foo     group: foo     mode: 0644 - file:     path: /work     owner: root     group: root     mode: 01777   delegation  This isn’t actually rolling update specific but comes up frequently in those cases.   If you want to perform a task on one host with reference to other hosts, use the ‘delegate_to’ keyword on a task. This is ideal for placing nodes in a load balanced pool, or removing them. It is also very useful for controlling outage windows. Be aware that it does not make sense to delegate all tasks, debug, add_host, include, etc always get executed on the controller. Using this with the ‘serial’ keyword to control the number of hosts executing at one time is also a good idea:  ---  - hosts: webservers   serial: 5    tasks:    - name: take out of load balancer pool     command: /usr/bin/take_out_of_pool      delegate_to: 127.0.0.1    - name: actual steps would go here     yum:       name: acme-web-stack       state: latest    - name: add back to load balancer pool     command: /usr/bin/add_back_to_pool      delegate_to: 127.0.0.1   These commands will run on 127.0.0.1, which is the machine running Ansible. There is also a shorthand syntax that you can use on a per-task basis: ‘local_action’. Here is the same playbook as above, but using the shorthand syntax for delegating to 127.0.0.1:  ---  # ...    tasks:    - name: take out of load balancer pool     local_action: command /usr/bin/take_out_of_pool   # ...    - name: add back to load balancer pool     local_action: command /usr/bin/add_back_to_pool    A common pattern is to use a local action to call ‘rsync’ to recursively copy files to the managed servers. Here is an example:  --- # ...   tasks:    - name: recursively copy files from management server to target     local_action: command rsync -a /path/to/files :/path/to/target/  Note that you must have passphrase-less SSH keys or an ssh-agent configured for this to work, otherwise rsync will need to ask for a passphrase.   dev experience lead  ","categories": [],
        "tags": [],
        "url": "/2018/06/21/Ansible.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Apigee",
        "excerpt":"App deployment, configuration management and orchestration - all from one system. Ansible is powerful IT automation that you can learn quickly.   Architecture  APIs are the glue that connect apps and act as the foundation for what we call the digital economy. Through every connection there is at least one API interacting with other applications and exchanging data. A business needs API management solutions that can support diverse users and not compromise the stability and reliability of back-end systems. Apigees intelligent API management platform allows companies to provide connected, seamless, digital experiences and increase the speed at which they innovate and adapt.   Apigee Edge, Apigee’s API Management platform, provides a unified solution that:   makes your APIs fail-proof helps you grow your developer and partner ecosystem enables you to run your APIs at scale provides deep insights into your APIs and your business Apigee Edge is the API management tool that offers the solutions today’s digital economy demands. For more information, download our Definitive Guide to API Management.   Build RESTful APIs  You have data, you have services, and you want to develop new business solutions quickly, both internally and externally.   With Apigee, you can build API proxies—RESTful, HTTP-based APIs that interact with your services. With easy-to-use APIs, developers can be more productive, increasing your speed to market.   API proxies give you the full power of Apigee’s API platform to secure API calls, throttle traffic, mediate messages, control error handling, cache things, build developer portals, document APIs, analyze API traffic data, make money on the use of your APIs, protect against bad bots, and more.   paybook   playbooks contains plays, plays contains tasks, while taks call modules. tasks run sequentially handlers are trigered by tasks and are un once, at the end of plays.   Configuration management  Ansible configurations are simple data descriptions of your infrastructure (both human-readable and machine-parsable) - ensuring everyone on your team will be able to understand the meaning of each configuration task. New team members will be able to quickly dive in and make an impact. Existing team members can get work done faster - freeing up cycles to attend to more critical and strategic work instead of configuration management.   Ansible requires nothing more than a password or SSH key in order to start managing systems and can start managing them without installing any agent software, avoiding the problem of “managing the management” common in many automation systems. There’s no more wondering why configuration management daemons are down, when to upgrade management agents, or when to patch security vulnerabilities in those agents.   GOAL-ORIENTED, NOT SCRIPTED  Ansible features an state-driven resource model that describes the desired state of computer systems and services, not the paths to get them to this state. No matter what state a system is in, Ansible understands how to transform it to the desired state (and also supports a “dry run” mode to preview needed changes). This allows reliable and repeatable IT infrastructure configuration, avoiding the potential failures from scripting and script-based solutions that describe explicit and often irreversible actions rather than the end goal.   SECURE &amp; AGENTLESS  Ansible relies on the most secure remote configuration management system available as its default transport layer: OpenSSH. OpenSSH is available for a wide variety of platforms, is very lightweight and when security issues in OpenSSH are discovered, they are patched quickly.   Further, Ansible does not require any remote agents. Ansible delivers all modules to remote systems and executes tasks, as needed, to enact the desired configuration. These modules run with user-supplied credentials, including support for sudo and even Kerberos and clean up after themselves when complete. Ansible does not require root login privileges, specific SSH keys, or dedicated users and respects the security model of the system under management.   As a result, Ansible has a very low attack surface area and is quite easy to deploy into new environments.   EFFICIENT ARCHITECTURE  Ansible works by connecting to your nodes and pushing out small programs, called “Ansible modules” to them. These programs are written to be resource models of the desired state of the system. Ansible then executes these modules (over SSH by default), and removes them when finished.   Your library of modules can reside on any machine, and there are no servers, daemons, or databases required. Typically you’ll work with your favorite terminal program, a text editor, and probably a version control system to keep track of changes to your content.   MANAGE YOUR INVENTORY IN SIMPLE TEXT FILES   By default, Ansible represents what machines it manages using a very simple INI file that puts all of your managed machines in groups of your own choosing.   To add new machines, there is no additional SSL signing server involved, so there’s never any hassle deciding why a particular machine didn’t get linked up due to obscure NTP or DNS issues.   [webservers] www1.example.com www2.example.com  [dbservers] db0.example.com db1.example.com   installation  Once Ansible is installed, it will not add a database, and there will be no daemons to start or keep running. You only need to install it on one machine (which could easily be a laptop) and it can manage an entire fleet of remote machines from that central point. When Ansible manages remote machines, it does not leave software installed or running on them, so there’s no real question about how to upgrade Ansible when moving to a new version.  ","categories": [],
        "tags": [],
        "url": "/2018/06/21/Apigee.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Core Java",
        "excerpt":"Annotation retention policy  What is Retention policy in java annotations?   A retention policy determines at what point annotation should be discarded. Java defined 3 types of retention policies through java.lang.annotation.RetentionPolicy enumeration. It has SOURCE, CLASS and RUNTIME. Annotation with retention policy SOURCE will be retained only with source code, and discarded during compile time. Annotation with retention policy CLASS will be retained till compiling the code, and discarded during runtime. Annotation with retention policy RUNTIME will be available to the JVM through runtime. The retention policy will be specified by using java built-in annotation @Retention, and we have to pass the retention policy type. The default retention policy type is CLASS.   overload and override  “selection among overloaded methods is static, while selection among overridden methods is dynamic. ”   “a method is overridden when a subclass contains a method declaration with the same signature as a method declaration in an ancestor.”   “// Broken! - What does this program print? public class CollectionClassifier {     public static String classify(Set&lt;?&gt; s) {         return \"Set\";     }      public static String classify(List&lt;?&gt; lst) {         return \"List\";     }      public static String classify(Collection&lt;?&gt; c) {         return \"Unknown Collection\";     }      public static void main(String[] args) {         Collection&lt;?&gt;[] collections = {             new HashSet&lt;String&gt;(),             new ArrayList&lt;BigInteger&gt;(),             new HashMap&lt;String, String&gt;().values()         };          for (Collection&lt;?&gt; c : collections)             System.out.println(classify(c));     } }  You might expect this program to print Set, followed by List and Unknown Collection, but it doesn’t. It prints Unknown Collection three times. Why does this happen? Because the classify method is overloaded, and the choice of which overloading to invoke is made at compile time.”   “the best way to fix the program is to replace all three overloadings of classify with a single method that does an explicit instanceof test:  public static String classify(Collection&lt;?&gt; c) {     return c instanceof Set  ? \"Set\" :            c instanceof List ? \"List\" : \"Unknown Collection\"; }  Because overriding is the norm and overloading is the exception, overriding sets people’s expectations for the behavior of method invocation. As demonstrated by the CollectionClassifier example, overloading can easily confound these expectations. It is bad practice to write code whose behavior is likely to confuse programmers. This is especially true for APIs.”   “Exactly what constitutes a confusing use of overloading is open to some debate. A safe, conservative policy is never to export two overloadings with the same number of parameters. If a method uses varargs, a conservative policy is not to overload it at all,”   “Functional programming is a term that means different things to different people.” “At the heart of functional programming is thinking about your problem domain in terms of immutable values and functions that translate between them.”   Target typing  “What is implicit in all these examples is that a lambda expression’s type is context dependent. It gets inferred by the compiler. This target typing isn’t entirely new, either.”   “This restriction is relaxed a bit in Java 8. It’s possible to refer to variables that aren’t final; however, they still have to be effectively final. Although you haven’t declared the variable(s) as final, you still cannot use them as nonfinal variable(s) if they are to be used in lambda expressions. If you do use them as nonfinal variables, then the compiler will show an error.”   Functional interface  “A functional interface is an interface with a single abstract method that is used as the type of a lambda expression.”   ","categories": [],
        "tags": ["Coding","Java"],
        "url": "/2018/06/26/Core-Java.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Solace MQ",
        "excerpt":"Solace PubSub+  It is a message broker that lets you establish event-driven interactions between applications and microservices across hybrid cloud environments using open APIs and protocols.   Deployment   Cloud PubSub+ Cloud is enterprise-grade messaging available as a fully managed service in your favorite public clouds.   What do we mean by “enterprise grade?”  Unrivaled Reliability  Intelligent routing protocols always identify best path and adapt around network issues to keep your applications humming, and automatic message buffering keeps bursts of data from affecting slow consumers or your system as a whole.   Built-in high availability and disaster recovery capabilities mean your system will bounce back in a flash, without ever losing a message, even in the event of major system or network failures.   Serious Security  PubSub+ supports authentication and authorization mechanisms ranging from username and password and one-time passwords to sophisticated access control lists and robust integration with existing security policies and systems such as LDAP, Radius, Kerberos.   To protect messages in transit, Solace supports transport-layer TLS encryption using a variety of cypher suites.   Messaging   In application development terms, messaging, which is also commonly known as message-oriented middleware or just middleware, refers to technology that lets computer systems share information without requiring direct connections or awareness of one another’s location.   Docker on Solace  To bounce solace server:  docker run -d -p 8080:8080 -p 55555:55555 --shm-size=2g --env 'username_admin_globalaccesslevel=admin' --env 'username_admin_password=admin' --name=solace solace-pubsub-standard:8.11.0.1029  # or bring up an existing one docker start solace   Solace CLI management access: Enter the following docker exec command:  docker exec -it solace /usr/sw/loads/currentload/bin/cli -A   ","categories": [],
        "tags": ["MQ","Java"],
        "url": "/2018/07/04/Solace-MQ.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "JXM",
        "excerpt":"Exporting your beans to JMX  The core class in Spring’s JMX framework is the MBeanExporter. This class is responsible for taking your Spring beans and registering them with a JMX MBeanServer.   when running inside a container that does not provide an MBeanServer. To address this you can create an MBeanServer instance declaratively by adding an instance of the org.springframework.jmx.support.MBeanServerFactoryBean class to your configuration. You can also ensure that a specific MBeanServer is used by setting the value of the MBeanExporter’s server property to the MBeanServer value returned by an MBeanServerFactoryBean  ","categories": [],
        "tags": [],
        "url": "/2018/07/10/JMX.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "QuickFixJ",
        "excerpt":"Settings   A settings file is set up with two types of heading, a [DEFAULT] and a [SESSION] heading. [SESSION] tells QuickFIX/J that a new Session is being defined. [DEFAULT] is a place that you can define settings which will be inherited by sessions that do not explicitly define them. If you do not provide a setting that QuickFIX/J needs, it will throw a ConfigError telling you what setting is missing or improperly formatted.   SSL cipher   An SSL cipher specification in cipher-spec is composed of 4 major attributes plus a few extra minor ones.   Key Exchange Algorithm: RSA or Diffie-Hellman variants.   Authentication Algorithm: RSA, Diffie-Hellman, DSS or none.   Cipher/Encryption Algorithm: DES, Triple-DES, RC4, RC2, IDEA or none.   MAC Digest Algorithm: MD5, SHA or SHA1.   InOut exchange  Although the FIX protocol is event-driven and asynchronous, there are specific pairs of messages that represent a request-reply message exchange. To use an InOut exchange pattern, there should be a single request message and single reply message to the request. Examples include an  OrderStatusRequest message and UserRequest.   #FIX Sequence Number Management If an application exception is thrown during synchronous exchange processing, this will cause QuickFIX/J to not increment incoming FIX message sequence numbers and will cause a resend of the counterparty message. This FIX protocol behavior is primarily intended to handle transport errors rather than application errors. There are risks associated with using this mechanism to handle application errors. The primary risk is that the message will repeatedly cause application errors each time it is re-received. A better solution is to persist the incoming message (database, JMS queue) immediately before processing it. This also allows the application to process messages asynchronously without losing messages when errors occur.   Although it is possible to send messages to a FIX session before it is logged on (the messages will be sent at logon time), it is usually a better practice to wait until the session is logged on. This eliminates the required sequence number resynchronization steps at logon. Waiting for session logon can be done by setting up a route that processes the SessionLogon event category and signals the application to start sending messages.   Source code  QuickFixJComponent.class   if (configuration != null) {                         settings = configuration.createSessionSettings();                     } else {                         settings = QuickfixjEngine.loadSettings(remaining);                     }   MessageStore  This interface Used by a Session to store and retrieve messages for resend purposes.      boolean set(int sequence, String message) throws IOException;   void get(int startSequence, int endSequence, Collection messages) throws IOException;   Implementations such as MemoryStore.java, it use one HashMap&lt;Integer, String&gt; to keep messages (string)   Parse body  private void parseBody(DataDictionary dd, boolean doValidation) throws InvalidMessage {         for(StringField field = this.extractField(dd, this); field != null; field = this.extractField(dd, this)) {             if (isTrailerField(field.getField())) {                 this.pushBack(field);                 return;             }   validate check sum  in message.class   private void validateCheckSum(String messageData) throws InvalidMessage {         try {             int checksum = this.trailer.getInt(10);             if (checksum != MessageUtils.checksum(messageData)) {                 throw new InvalidMessage(“Expected CheckSum=” + MessageUtils.checksum(messageData) + “, Received CheckSum=” + checksum + “ in “ + messageData);             } }   the first checksum is 131   in MessageUtils.checksum public static int checksum   int end = isEntireMessage ? data.lastIndexOf(“\\u000110=”) : -1;             int len = end &gt; -1 ? end + 1 : data.length();           for(int i = 0; i &lt; len; ++i) {             sum += data.charAt(i);         }          return sum &amp; 255; the checksum from above messageUtil is 87 ?? how to get and set this.trailer.10=131 ?  ","categories": [],
        "tags": [],
        "url": "/2018/07/10/QuickFixJ.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Apache Camel",
        "excerpt":"Camel’s message model  In Camel, there are two abstractions for modeling messages, both of which we’ll cover in this section.     org.apache.camel.Message—The fundamental entity containing the data being carried and routed in Camel   org.apache.camel.Exchange—The Camel abstraction for an exchange of mes- sages. This exchange of messages has an “in” message and as a reply, an “out” message   Message  Messages are the entities used by systems to communicate with each other when using messaging channels. Messages flow in one direction from a sender to a receiver,  Messages have a body (a payload), headers, and optional attachments,   Messages are uniquely identified with an identifier of type java.lang.String. The identifier’s uniqueness is enforced and guaranteed by the message creator, it’s protocol depen- dent, and it doesn’t have a guaranteed format.  For protocols that don’t define a unique message identification scheme, Camel uses its own UID generator. HEADERS AND ATTACHMENTS Headers are values associated with the message, such as sender identifiers, hints about content encoding, authentication infor- mation, and so on. Headers are name-value pairs; the name is a unique, case-insensitive string, and the value is of type java. lang.Object. This means that Camel imposes no constraints on the type of the headers. Headers are stored as a map within the message. A message can also have optional attachments, which are typically used for the web service and email components.   Exchange  An exchange in Camel is the message’s container during routing. An exchange also provides support for the various types of interactions between systems, also known as message exchange patterns (MEPs). MEPs are used to differentiate between one-way and request-response messaging styles. The Camel exchange holds a pattern property that can be either     InOnly—A one-way message (also known as an Event message). For example, JMS messaging is often one-way messaging.   InOut—A request-response message. For example, HTTP-based transports are often request reply, where a client requests to retrieve a web page, waiting for the reply from the server.   Start Camel application   Camel doesn’t start magically by itself. Often it’s the server (container) that Camel is running inside that invokes the start method on CamelContext, starting up Camel. This is also what you saw in chapter 1, where you used Camel inside a standalone Java application. A standalone Java application isn’t the only deployment choice—you can also run Camel inside a container such as Spring or OSGi. Regardless of which container you use, the same principle applies. The container must prepare and create an instance of CamelContext up front, before Camel can be started.   Spring container  Camel provides the CamelNamespaceHandler. When using Camel in the Spring XML file, you would define the  tag as follows:    The http://camel.apache.org/schema/spring namespace is the Camel custom namespace. To let Spring know about this custom namespace, it must be identified in the META-INF/spring.handlers, where you map the namespace to the class implementation: http\\://camel.apache.org/schema/spring=      org.apache.camel.spring.handler.CamelNamespaceHandler   The CamelNamespaceHandler is then responsible for parsing the XML and dele- gating to other factories for further pro- cessing. One of these factories is the Camel- ContextFactoryBean, which is responsible for creating the CamelContext that essen- tially is your Camel application. When Spring is finished initializing, it signals to third-party frameworks that they can start by broadcasting the Context- RefreshedEvent event.  # Startup At this point, CamelContext is ready to be started. What happens next is the same regardless of which container or deploy- ment option you’re using with Camel.   CamelContext is started by invoking its start method. The first step is to determines whether or not autostartup is enabled for Camel. If it’s disabled, the entire startup process is skipped. By default, Camel is set to autostart, which involves the following four steps.  1 Start internal services—Prepares and starts internal services used by Camel, such as the type-converter mechanism. 2 Compute starting order—Computes the order in which the routes should be started. By default, Camel will start up all the routes in the order they are defined in the Spring XML files or the RouteBuilder classes. We’ll cover how to configure the order of routes in section 13.1.3. 3 Prepare routes—Prepares the routes before they’re started. 4 Start routes—Starts the routes by starting the consumers, which essentially opens the gates to Camel and lets the messages start to flow in.   After step 4, Camel writes a message to the log indicating that it has been started and that the startup process is complete.  # Concept  ## ENDPOINT An endpoint is the Camel abstraction that models the end of a channel through which a system can send or receive messages.  In Camel, you configure endpoints using URIs, such as  ```bash file:data/inbox?delay=5000 ```  and you also refer to endpoints this way. At runtime, Camel will look up an endpoint based on the URI notation.   The scheme  denotes which Camel component handles that type of endpoint. In this case, the scheme of file selects the FileComponent. The FileComponent then works as a factory creat- ing the FileEndpoint based on the remaining parts of the URI.  The context path data/ inbox tells the FileComponent that the starting folder is data/inbox.  The option, delay=5000 indicates that files should be polled at a 5 second interval.  There’s more to an endpoint than meets the eye.    ## JMS  Queues are strictly point-to-point, where each message has only one consumer. Topics operate on a publish/subscribe scheme; a single message may be delivered to many consumers if they have subscribed to the topic.  JMS also provides a ConnectionFactory that clients (like Camel) can use to cre- ate a connection with a JMS provider. JMS providers are usually referred to as brokers because they manage the communication between a message producer and a mes- sage consumer.  ### HOW TO CONFIGURE CAMEL TO USE A JMS PROVIDER To connect Camel to a specific JMS provider, you need to configure Camel’s JMS com- ponent with an appropriate ConnectionFactory. Apache ActiveMQ is one of the most popular open source JMS providers, and it’s the primary JMS broker that the Camel team uses to test the JMS component.   ### JSM destinations There are two types of JMS destinations: queues and topics. The queue is a point-to-point channel, where each message has only one recipient. A topic delivers a copy of the message to all clients who have subscribed to receive it.   ### ActiveMQ o in the case of Apache ActiveMQ, you can create an ActiveMQConnectionFactory that points to the location of the running ActiveMQ broker: ```java ConnectionFactory connectionFactory =   new ActiveMQConnectionFactory(\"vm://localhost\"); ``` The vm://localhost URI means that you should connect to an embedded broker named “localhost” running inside the current JVM. The vm transport connector in ActiveMQ creates a broker on demand if one isn’t running already, so it’s very handy for quickly testing JMS applications; for production scenarios, it’s recommended that you connect to a broker that’s already running.   Next, when you create your CamelContext, you can add the JMS component as follows: CamelContext context = new DefaultCamelContext(); context.addComponent(\"jms\",     JmsComponent.jmsComponentAutoAcknowledge(connectionFactory)); The JMS component and the ActiveMQ-specific connection factory aren’t part of the camel-core module. In order to use these, you’ll need to add some dependencies to your Maven-based project. For the plain JMS component, all you have to add is this: ```xml    org.apache.camel   camel-jms   2.5.0  The connection factory comes directly from ActiveMQ, so you’ll need the following dependency:    org.apache.activemq   activemq-core   5.3.2  ```   ### USING URIS TO SPECIFY THE DESTINATION Once the JMS component is configured, you can start sending and receiving JMS mes- sages at your leisure. Because you’re using URIs, this is a real breeze to configure. Let’s say you want to send a JMS message to the queue named incomingOrders. The URI in this case would be ```java jms:queue:incomingOrders ``` This is pretty self-explanatory. The “jms” prefix indicates that you’re using the JMS component you configured before. By specifying “queue”, the JMS component knows to send to a queue named incomingOrders. You could even have omitted the queue qualifier, because the default behavior is to send to a queue rather than a topic.  NOTE Some endpoints can have an intimidating list of endpoint URI proper- ties. For instance, the JMS component has about 60 options, many of which are only used in specific JMS scenarios. Camel always tries to provide built-in defaults that fit most cases, and you can always find out what the default values are by browsing to the component’s page in the online Camel documentation.   Using Camel’s Java DSL, you can send a message to the incomingOrders queue by using the to keyword like this: ...to(\"jms:queue:incomingOrders\") This can be read as sending to the JMS queue named incomingOrders.  ## FINDING ROUTE BUILDERS Using the Spring CamelContext as a runtime and the Java DSL for route development is a great way of using Camel. In fact, it’s the most frequent usage of Camel. You saw before that you can explicitly tell the Spring CamelContext what route builders to load. You can do this by using the routerBuilder element: ```xml      ``` Being this explicit results in a clean and concise definition of what is being loaded into Camel. Sometimes, though, you may need to be a bit more dynamic. This is where the packageScan and contextScan elements come in: ```xml         camelinaction.routes     ``` This packageScan element will load all RouteBuilder classes found in the camelinac- tion.routes package, including all subpackages. You can even be a bit more picky about what route builders are included: ```xml         camelinaction.routes     **.*Test*     **.*     ``` In this case, you’re loading all route builders in the camelinaction.routes package, except for ones with “Test” in the class name. The matching syntax is similar to what is used in Apache Ant’s file pattern matchers.   # Endpoints  Camel supports the Message Endpoint pattern using the Endpoint interface. Endpoints are usually created by a Component and Endpoints are usually referred to in the DSL via their URIs.  From an Endpoint you can use the following methods  createProducer() will create a Producer for sending message exchanges to the endpoint createConsumer() implements the Event Driven Consumer pattern for consuming message exchanges from the endpoint via a Processor when creating a Consumer createPollingConsumer() implements the Polling Consumer pattern for consuming message exchanges from the endpoint via a PollingConsumer  # implementation When using the DSL to create Routes you typically refer to Message Endpoints by their URIs rather than directly using the Endpoint interface. Its then a responsibility of the CamelContext to create and activate the necessary Endpoint instances using the available Component implementations.   # connector Sub interface including Initiator. abstract class SessionConnector implements Connector   # session class Session.java  # FieldMaps Which leverage TreeMap&lt;Integer, Field&lt;?&gt;&gt; fields;  ## Field  ```java public class Field implements Serializable       public Field(int field, T object) {         this.tag = field;         this.object = object;     } ```    ","categories": [],
        "tags": ["Camel","Apache"],
        "url": "/2018/07/11/Camel.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "File Util in Apache Camel",
        "excerpt":"FileUtil.class   compactPath(String path)  To normalize path and join with provided separator   if path is null, return null if path.indexOf(47) == -1 &amp;&amp; path.indexOf(92) ==-1 (means /, ) return path   normalizePath   check whether it is windows String osName = System.getProperty(“os.name”).toLowerCase(Locale.ENGLISH);         return osName.contains(“windows”); for windows, replace / with \\, for linux, replace \\ with /   split path by separator (\\ or /) to get an array   traverse array and check whether current is “..”, if so , call stack.pop otherwise, stach.push(part)   Then iterate stack and then combine them by a StringBuilder via provided addtional parameter separator   ","categories": [],
        "tags": [],
        "url": "/2018/07/16/FileUtils_Camel.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "akka framework of scala",
        "excerpt":"philosophy  The actor model adopts the philosophy that everything is an actor. This is similar to the everything is an object philosophy used by some object-oriented programming languages.   Decoupling the sender from communications sent was a fundamental advance of the Actor model enabling asynchronous communication and control structures as patterns of passing messages.   Recipients of messages are identified by address, sometimes called “mailing address”. Thus an actor can only communicate with actors whose addresses it has.   Route  Routes effectively are simply highly specialised functions that take a RequestContext and eventually complete it, which could (and often should) happen asynchronously.   Directives create Routes.   The Route is the central concept of Akka HTTP’s Routing DSL. All the structures you build with the DSL, no matter whether they consists of a single line or span several hundred lines, are type turning a RequestContext into a Future[RouteResult].   type Route = RequestContext =&gt; Future[RouteResult]   Generally when a route receives a request (or rather a RequestContext for it) it can do one of these things:      Complete the request by returning the value of requestContext.complete(…)   Reject the request by returning the value of requestContext.reject(…) (see Rejections)   Fail the request by returning the value of requestContext.fail(…) or by just throwing an exception (see Exception Handling)   Do any kind of asynchronous processing and instantly return a Future[RouteResult] to be eventually completed later   The Routing Tree  Essentially, when you combine directives and custom routes via nesting and the ~ operator, you build a routing structure that forms a tree. When a request comes in it is injected into this tree at the root and flows down through all the branches in a depth-first manner until either some node completes it or it is fully rejected.   In RouteDirective.scala   /**    * Completes the request using the given arguments.    *    * @group route    */   def complete(m: ? ToResponseMarshallable): StandardRoute =     StandardRoute(_.complete(m))    RouteResult  RouteResult is a simple abstract data type (ADT) that models the possible non-error results of a Route. It is defined as such:  sealed trait RouteResult  object RouteResult {   final case class Complete(response: HttpResponse) extends RouteResult   final case class Rejected(rejections: immutable.Seq[Rejection]) extends RouteResult }   Routing DSL   In addition to the Core Server API Akka HTTP provides a very flexible ,Routing DSL, for elegantly defining RESTful web services.   Http().bindAndHandle(routes ~ abcRoute, host, port)   /**      * Returns a Route that chains two Routes. If the first Route rejects the request the second route is given a      * chance to act upon the request.      */     def ~(other: Route): Route = { ctx ?       import ctx.executionContext       route(ctx).fast.flatMap {         case x: RouteResult.Complete ? FastFuture.successful(x)         case RouteResult.Rejected(outerRejections) ?           other(ctx).fast.map {             case x: RouteResult.Complete               ? x             case RouteResult.Rejected(innerRejections) ? RouteResult.Rejected(outerRejections ++ innerRejections)           }       }     }    Path matching     Note The path matching DSL describes what paths to accept after URL decoding. This is why the path-separating slashes have special status and cannot simply be specified as part of a string! The string ¡°foo/bar¡± would match the raw URI path ¡°foo%2Fbar¡±, which is most likely not what you want!    Sink  A Sink is a set of stream processing steps that has one open input. Can be used as a Subscriber   supervision   What Supervision Means  As described in Actor Systems supervision describes a dependency relationship between actors: the supervisor delegates tasks to subordinates and therefore must respond to their failures. When a subordinate detects a failure (i.e. throws an exception), it suspends itself and all its subordinates and sends a message to its supervisor, signaling failure. Depending on the nature of the work to be supervised and the nature of the failure, the supervisor has a choice of the following four options:      Resume the subordinate, keeping its accumulated internal state   Restart the subordinate, clearing out its accumulated internal state   Stop the subordinate permanently   Escalate the failure, thereby failing itself   sealed class  object Supervision {   sealed trait Directive }  A sealed class may not be directly inherited, except if the inheriting template is defined in the same source file as the inherited class. However, subclasses of a sealed class can inherited anywhere.   implicit  A method can have an implicit parameter list, marked by the implicit keyword at the start of the parameter list. If the parameters in that parameter list are not passed as usual, Scala will look if it can get an implicit value of the correct type, and if it can, pass it automatically.   The places Scala will look for these parameters fall into two categories:   Scala will first look for implicit definitions and implicit parameters that can be accessed directly (without a prefix) at the point the method with the implicit parameter block is called. Then it looks for members marked implicit in all the companion objects associated with the implicit candidate type.   KillSwitch  A KillSwitch allows completion of Graphs from the outside by completing Graphs of FlowShape linked to the switch. Depending on whether the KillSwitch is a UniqueKillSwitch or a SharedKillSwitch one or multiple streams might be linked with the switch.   trait KillSwitch { //After calling KillSwitch.shutdown() the linked Graphs of FlowShape are completed normally.  def shutdown(): Unit  def abort(ex:Throwable): Unit }   Source  A Source is a set of stream processing steps that has one open output. It can comprise any number of internal sources and transformations that are wired together, or it can be an atomic source, e.g. from a collection or a file. Materialization turns a Source into a Reactive Streams Publisher (at least conceptually).   connect to Sink  /**    * Connect this [[akka.stream.scaladsl.Source]] to a [[akka.stream.scaladsl.Sink]],    * concatenating the processing steps of both.    */   def to[Mat2](sink: Graph[SinkShape[Out], Mat2]): RunnableGraph[Mat] = toMat(sink)(Keep.left)   Future  trait Future[+T]  extends Awaitable[T] A Future represents a value which may or may not currently be available, but will be available at some point, or an exception if that value could not be made available. Asynchronous computations that yield futures are created with the Future.apply call and are computed using a supplied ExecutionContext, which can be backed by a Thread pool.   import ExecutionContext.Implicits.global    val s = “Hello”    val f: Future[String] = Future {      s + “ future!”    }    f foreach {      msg =&gt; println(msg)    }   Future introduction  Futures provide a way to reason about performing many operations in parallel¨C in an efficient and non-blocking way. A Future is a placeholder object for a value that may not yet exist. Generally, the value of the Future is supplied concurrently and can subsequently be used. Composing concurrent tasks in this way tends to result in faster, asynchronous, non-blocking parallel code.   By default, futures and promises are non-blocking, making use of callbacks instead of typical blocking operations. To simplify the use of callbacks both syntactically and conceptually, Scala provides combinators such as flatMap, foreach, and filter used to compose futures in a non-blocking way. Blocking is still possible - for cases where it is absolutely necessary, futures can be blocked on (although this is discouraged).   Scala scope protection:  private[C] means that access is private “up to” C, where C is the corresponding package, class or singleton object.  private[http] def build = {   // ... }   The modi?er can be quali?ed with an identi?er C (e.g. private[C]) that must denote a class or package enclosing the de?nition. Members labeled with such a modi?er are accessible respectively only from code inside the package C or only from code inside the class C and its companion module (¡ì5.4). Such members are also inherited only from templates inside C.   Scala flexible import  you can import several classes the Scala way:  import java.io.{File, IOException, FileNotFoundException}  Use the following syntax to import everything from the java.io package:  import java.io._  The _ character in this example is similar to the * wildcard character in Java.   If the _ character feels unusual at first, it helps to know that it¡¯s used consistently throughout the Scala language as a wildcard character, and that consistency is very nice.   Directives  A ¡°Directive¡± is a small building block used for creating arbitrarily complex route structures. Akka HTTP already pre-defines a large number of directives and you can easily construct your own:   Regular Expression  val pattern = \"Scala\".r       val str = \"Scala is Scalable and cool\"              println(pattern findFirstIn str)    We create a String and call the r( ) method on it. Scala implicitly converts the String to a RichString and invokes that method to get an instance of Regex. To find a first match of the regular expression, simply call the findFirstIn() method. If instead of finding only the first occurrence we would like to find all occurrences of the matching word, we can use the findAllIn( ) method and in case there are multiple Scala words available in the target string, this will return a collection of all matching words.   PathMatcher      Segment: PathMatcher1[String] Matches if the unmatched path starts with a path segment (i.e. not a slash). If so the path segment is extracted as a String instance.   Option  In short, if you have a value of type A that may be absent, Scala uses an instance of Option[A] as its container. An Intance of Option is either an instance of case class Some when it is present or case object None when it is not. Since both Some and None are children of Option, your function signature should declare that the returned value is an Option of some type, e.g. Option[A]   Sample  It is very easy to create an Option in Scala, i.e. you can use a present/absent value directly.  val optionalInt: Option[Int] = Some(1) // or // val optionalInt: Option[Int] = None   To validate user login  def auth(user:String, pwd:String): AuthResult = \t(user, pwd) match { \tcase (u, _) if Option(u).exists(_.trim.isEmpty) =&gt; ErrorLogin \tcase (_, p) if Option(p).exists(_.trim.isEmpty) =&gt; ErrorPwd \tcase (u, p) =&gt; doAuth(u,p)  }  isDefined  you add a checker for the None value using the isDefined method and specify logic to handle each scenario accordingly.   def addTwoWithDefault(a: Option[Int]): Int = {   if(a.isDefined) a.get + 2 else 2 }   getOrElse   In many cases, you have a fallback or default value for your absent values, e.g. zero in the above example. With Option, you can easily provide a default value via the getOrElse method.   def addTwoWithDefault(a: Option[Int]): Int = a.getOrElse(0) + 2   flatten  Assume that we have a List of Option[Int].  val l: List[Option[Int]] = List(Some(3), Some(1), None, Some(5), Some(8), None)   A common scenario is that we need to filter out the absent values and return a List of Int. A straightfoward approach is to combine filter with .isDefined.   l.filter(_.isDefined).map(_.get)  // res1: List[Int] = List(3, 1, 5, 8)  However, Scala actually provides an elegent built-in function to achieve the same goal, which is often more preferred.   l.flatten // res1: List[Int] = List(3, 1, 5, 8)   underscore  In Scala, pattern matching is somewhat similar to java switch statement. But it is more powerful.    def matchTest(x: Int): String = x match {     case 1 =&gt; \"one\"     case 2 =&gt; \"two\"     case _ =&gt; \"anything other than one and two\"   }    _ acts like a wildcard. It will match anything. Scala allows nested patterns, so we can nest the _ also.Lets see another example that uses _ in nested pattern.   expr match {   case List(1,_,_) =&gt; \" a list with three element and the first element is 1\"   case List(_*)  =&gt; \" a list with zero or more elements \"   case Map[_,_] =&gt; \" matches a map with any key type and any value type \"   case _ =&gt;   }   Anonymous Functions  Scala represents anonymous functions with a elegant syntax. The _ acts as a placeholder for parameters in the anonymous function. The _ should be used only once, But we can use two or more underscores to refer different parameters.   List(1,2,3,4,5).foreach(print(_)) List(1,2,3,4,5).foreach( a =&gt; print(a)) // Here the _ refers to the parameter. The first one is a short form of the second one. Lets look at another example which take two parameters.  val sum = List(1,2,3,4,5).reduceLeft(_+_) val sum = List(1,2,3,4,5).reduceLeft((a, b) =&gt; a + b)   Functions  Scala is a functional language. So we can treat function as a normal variable. If you try to assign a function to a new variable, the function will be invoked and the result will be assigned to the variable. This confusion occurs due to the optional braces for method invocation. We should use _ after the function name to assign it to another variable.   List(1,2,3,4,5).foreach(print(_)) class Test {   def fun = {     // some code   }   val funLike = fun _ } List(1,2,3,4,5).foreach(print(_))   Either, Left, Right   Using Either, Left, and Right Prior to Scala 2.10, an approach similar to Try was available with the Either, Left, and Right classes. With these classes, Either is analogous to Try, Right is similar to Success, and Left is similar to Failure.   The following method demonstrates how to implement the Either approach:   def divideXByY(x: Int, y: Int): Either[String, Int] = {     if (y == 0) Left(\"Dude, can't divide by 0\")     else Right(x / y) }  As shown, your method should be declared to return an Either, and the method body should return a Right on success and a Left on failure. The Right type is the type your method returns when it runs successfully (an Int in this case), and the Left type is typically a String, because that¡¯s how the error message is returned.   As with Option and Try, a method returning an Either can be called in a variety of ways, including getOrElse or a match expression:   val x = divideXByY(1, 1).right.getOrElse(0)   // returns 1 val x = divideXByY(1, 0).right.getOrElse(0)   // returns 0  // prints \"Answer: Dude, can't divide by 0\" divideXByY(1, 0) match {     case Left(s) =&gt; println(\"Answer: \" + s)     case Right(i) =&gt; println(\"Answer: \" + i) }  You can also access the error message by testing the result with isLeft, and then accessing the left value, but this isn¡¯t really the Scala way:   scala&gt; val x = divideXByY(1, 0) x: Either[String,Int] = Left(Dude, can't divide by 0)  scala&gt; x.isLeft res0: Boolean = true  scala&gt; x.left res1: scala.util.Either.LeftProjection[String,Int] =       LeftProjection(Left(Dude, can't divide by 0))  Although the Either classes offered a potential solution prior to Scala 2.10, I now use the Try classes in all of my code instead of Either.   classOf  Retrieve the runtime representation of a class type. classOf[T] is equivalent to the class literal T.class in Java.   Operators  // Keywords &lt;-  // Used on for-comprehensions, to separate pattern from generator =&gt;  // Used for function types, function literals and import renaming   Akka framework  Akka is a toolkit and runtime for building highly concurrent, distributed, and fault-tolerant event-driven applications on the JVM.   Actors are the unit of execution in Akka. The Actor model is an abstraction that makes it easier to write correct concurrent, parallel and distributed systems.   This will get your feet wet, and hopefully inspire you to dive deeper into the wonderful sea of Akka!   The Akka team refers to their creation as a toolkit rather than a framework. Frameworks tend to be a mechanism for providing a discrete element of a stack (e.g. the ui, or the web services layer). Akka provides a set of tools to render any part of the stack, and to provide the interconnects between them.   He two ways (shared mutable state/message passing (Akka)) to solve the problem of selling tickets.   Actors do not share state, can only communicate through immutable messages and do not talk to each other directly but through actor references, similar to the addresses we talked about. This approach satisfies the three things we wanted to change. So why is this simpler than the shared mutable state approach?     We don’t need to manage locks. We don’t have to think about how to protect the shared data. Inside an actor we’re safe.   We are more protected from deadlocks caused by out of order access by multiple threads, that cause the system to wait forever, or other problems like race conditions and thread starvation. Use of Akka precludes most of these problems, relieving us of the burden.   Performance tuning a shared mutable state solution is hard work and error prone and verification through tests is nearly impossible.   Benefits of using the Actor Model   The following characteristics of Akka allow you to solve difficult concurrency and scalability challenges in an intuitive way:      Event-driven model — Actors perform work in response to messages. Communication between Actors is asynchronous, allowing Actors to send messages and continue their own work without blocking to wait for a reply.   Strong isolation principles — Unlike regular objects in Java, an Actor does not have a public API in terms of methods that you can invoke. Instead, its public API is defined through messages that the actor handles. This prevents any sharing of state between Actors; the only way to observe another actor’s state is by sending it a message asking for it.   Location transparency — The system constructs Actors from a factory and returns references to the instances. Because location doesn’t matter, Actor instances can start, stop, move, and restart to scale up and down as well as recover from unexpected failures.   Lightweight — Each instance consumes only a few hundred bytes, which realistically allows millions of concurrent Actors to exist in a single application.   Message  two special channels. The first is the Dead Letter channel, which contain message that couldn’t be delivered. This is sometimes also called a dead message queue. This channel can help when debugging, why some messages aren’t processed or to monitor where there are problems.   EventStream  the benefit of decoupling the receivers and the sender and the dynamic nature of the publish-subscribe channel, but because the EventStream is available for all actors is also a nice solution for messages which can be send from all over the system and needs to be collected at one or more Actors. A good example is logging. Logging can be done throughout the system and needs to be collected at one point and be written to a log file. Internally the ActorLogging is using the EventStream to collect the log lines from all over the system.   Dead Letter Message  Akka is using the EventStream to implement the dead letter queue. This way only the actors which are interested in the failed messages are receiving them. When a message is queued in a mailbox of an actor that Terminates or is send after the Termination, the message is send to the EventStream of the ActorSystem. The message is wrapped into a DeadLetter object. This Object contains the original message, the sender of the message and the intended receiver. This way the Dead letter queue is integrated in the EventStream. To get these dead letter messages you only need to subscribe your actor to the EventStream with the DeadLetter class as the Classifier.   Messages send to a Terminated Actor can’t be processed anymore and the ActorRef of this actor should not be used anymore. When there are messages send to a terminated Actor, these message will be send to the DeadLetter queue.   Another use of the DeadLetter queue is when the processing fails. This is a Actor specific decision. An actor can decide that a received message couldn’t be processed and that it doesn’t know what to do with it. In this situation the messages can be send to the dead letter queue.   Design recommendations  When defining Actors and their messages, keep these recommendations in mind:      Since messages are the Actor’s public API, it is a good practice to define messages with good names and rich semantic and domain specific meaning, even if they just wrap your data type. This will make it easier to use, understand and debug actor-based systems.   Messages should be immutable, since they are shared between different threads.   It is a good practice to put an actor’s associated messages as static classes in the class of the Actor. This makes it easier to understand what type of messages the actor expects and handles.   It is also a common pattern to use a static props method in the class of the Actor that describes how to construct the Actor.   Props  The static props method creates and returns a Props instance. Props is a configuration class to specify options for the creation of actors, think of it as an immutable and thus freely shareable recipe for creating an actor that can include associated deployment information. This example simply passes the parameters that the Actor requires when being constructed. We will see the props method in action later in this tutorial.   configuration   When using the default, the library will try to find the configuration file. Since the library supports a number of different configuration formats, it looks for different files, in the following order: application.properties This file should contain the configuration properties in the java property file format. application.json This file should contain the configuration properties in the json style   application.conf This file should contain the configuration properties in the HOCON format. This is a format based on json but easier to read.. It is possible to use all the different files at the same time. For the example below, in listing 7.2 we use the last file: MyAppl {     version = 10     description = “My application”     database {         connect=”jdbc:mysql://localhost/mydata”         user=”me”         } } Nesting is done by simply grouping with {}s   substitution  hostname=”localhost” hostname=${?HOST_NAME} MyAppl {     version = 10     description = “My     application”     database {         connect=”jdbc:mysql://${hostname}/mydata” user=”me” } }   define the variable first, if system environment do exist, override it, otherwise use default ? means get a variable from system envrionment   Default/fallback properies  Default properties are configured in the file reference.conf and placed in the root of the jar file; the idea is that every library contains its own defaults. The configuration library will find all the reference.conf files and integrate these settings into the configuration fall-back structure.   Order of properties  System properties-&gt;application.conf-&gt;applicaiton.json-&gt;application.properties-&gt;reference.conf   The power of location transparency   In Akka you can’t create an instance of an Actor using the new keyword. Instead, you create Actor instances using a factory. The factory does not return an actor instance, but a reference, akka.actor.ActorRef, that points to the actor instance. This level of indirection adds a lot of power and flexibility in a distributed system.   In Akka location doesn’t matter. Location transparency means that the ActorRef can, while retaining the same semantics, represent an instance of the running actor in-process or on a remote machine. If needed, the runtime can optimize the system by changing an Actor’s location or the entire application topology while it is running. This enables the “let it crash” model of failure management in which the system can heal itself by crashing faulty Actors and restarting healthy ones.   The Akka ActorSystem   The akka.actor.ActorSystem factory is, to some extent, similar to Spring’s BeanFactory. It acts as a container for Actors and manages their life-cycles. The actorOf factory method creates Actors and takes two parameters, a configuration object called Props and a name.   Asynchronous communication  Actors are reactive and message driven. An Actor doesn’t do anything until it receives a message. Actors communicate using asynchronous messages. This ensures that the sender does not stick around waiting for their message to be processed by the recipient. Instead, the sender puts the message in the recipient’s mailbox and is free to do other work. The Actor’s mailbox is essentially a message queue with ordering semantics. The order of multiple messages sent from the same Actor is preserved, but can be interleaved with messages sent by another Actor.   You might be wondering what the Actor is doing when it is not processing messages, i.e. doing actual work? It is in a suspended state in which it does not consume any resources apart from memory. Again, showing the lightweight, efficient nature of Actors.   Sending messages to an Actor  To put a message into an Actor’s mailbox, use the tell method on the ActorRef. For example, the main class of Hello World sends messages to the Greeter Actor like this:   howdyGreeter.tell(new WhoToGreet(\"Akka\"), ActorRef.noSender()); howdyGreeter.tell(new Greet(), ActorRef.noSender());   The test class is using akka.test.javadsl.TestKit, which is a module for integration testing of actors and actor systems. This class only uses a fraction of the functionality provided by TestKit.   Akka under the hood  So are there no concurrency primitives like locks used at all in Akka? Well, of course there are, it’s just that you don’t have to deal with them directly . Everything still eventually runs on threads and low level concurrency primitives. Akka uses the java.util.concurrent library to coordinate message processing and takes great care to minimize the number of locks used to an absolute bare minimum. It uses lock free and wait free algorithms where possible, for example compare-and-swap (CAS) techniques, which are beyond the scope of this book. And because nothing can be shared between actors, the shared locks that you would normally have between objects are not present at all.   There are other benefits that stem from the message passing approach that Akka uses, which we will discuss in the next sections. We have touched on them briefly already:     Even in this first, simple example, the message passing approach is clearly more fault tolerant, averting catastrophic failure if one component (no matter how key) fails.   The shared mutable state is always in one place in the example (in one JVM if it is kept entirely in memory). If you need to scale beyond this constraint, you will have to (re)distribute the data somehow. Since the message passing style uses addresses, looking ahead, you can see that if local and remote addresses were interchangeable, scaling out would be possible without code changes of any kind.   This scenario is one example of a fault tolerance strategy that Akka provides, which is called the Restart strategy. Other strategies that can be used are Resume, Stop and Escalate.   Scale up and Scale out  In our Ticketing example, scaling up would mean getting more TicketingAgents running on our one server, scaling out would be bringing up TicketingAgents on a number of machines.   Locking   locks result in contention, which will mean the number of threads doing work at any one time is often less than the total number, as some will have to wait on each other to finish. Sharing as little as possible means locking as little as possible, which is the goal of the message passing approach.   Every thread has a stack to store runtime data. The size of the stack differs per operating system, for instance on the linux x64 platform it is normally 256kB. The stack size is one of the factors that limits the number of threads that run at the same time on a server. Around 4096 threads can fit in 1GB of memory on the linux x64 platform.   Dispatcher   Actors run on an abstraction which is called a dispatcher. The dispatcher takes care of which threading model is used and processes the mailboxes.   Actors are lightweight because they run on top of dispatchers, the actors are not necessarily directly proportional to the number of threads. Akka Actors take a lot less space than threads, around 2.7 million actors can fit in 1GB of memory. A big difference compared to 4096 threads, which means that you can create different types of actors more freely than you would when using threads directly. There are different types of dispatchers to choose from which can be tuned to specific needs.   We identified that we had to make the following changes to get to a message passing style:     No mutable shared data structure.   Immutable message passing.   Asynchronous message sending.   Akka implements actors and which components compare to the concepts we’ve talked about so far: actors, addresses and mailboxes.   Actor Path  So how do you get an actor reference to an actor in the hierarchy? This is where ActorPaths come in. You could compare the hierarchy of actors to a URL path structure. Every actor has a name. This name needs to be unique per level in the hierarchy, two sibling actors cannot have the same name (if you do not provide a name Akka generates one for you, but it is a good idea to name all your actors). All actor references can be located directly by an actor path, absolute or relative, and it has to follow the URI generic syntax. An actor path is built just like a URI, starting with a scheme followed by a scheme-specific part,   Core Actor Operations  Another way to look at an actor is to describe the operations that it supports. An Akka actor has four core operations :     CREATE: An actor can create another actor. In Akka, actors are part of an actor system, which defines the root of the actor hierarchy and creates top-level actors. Every actor can create child actors. The topology of the actors is dynamic, it depends on which actors create other actors and which addresses are used to communicate with them.   SEND: An actor can send a message to another actor. Messages are sent asynchronously, using an address to send to an Actor associated with a given Mailbox.   BECOME: The behavior of an actor can be dynamically changed. Messages are received one at a time and an actor can designate that it wants to handle next messages in a different way, basically swapping its behavior, which we will look at in later chapters.   SUPERVISE: An actor supervises and monitors its children in the actor hierarchy and manages the failures that happen. As we will see in chapter 3, this provides a clean separation between message processing and error handling.   Akka concurrency  Message passing enables an easier road to real concurrency  With that concurrent approach, we will be able to scale up and out We can scale both the request and the processing elements of our application Messages also unlock greater fault tolerance capabilities Supervision provides a means of modeling for both concurrency and fault tolerance Akka infuses our code with these powers in a lightweight, unobtrusive manner   Build Akka  using TypeSafe’s Simple Build Tool (SBT) to create a single jar file that can be used to run the app   If you have not worked with the SBT DSL before it is important to note that you need to put en empty line between lines in the file (this is the price we pay for not telling Scala where each expression ends).   tell vs ask  Messages are sent to an Actor through one of the following methods.   ! means “fire-and-forget”, e.g. send a message asynchronously and return immediately. Also known as tell.   ? sends a message asynchronously and returns a Future representing a possible reply. Also known as ask.   So below line is equivalent to tell  class ReceiveActor extends Actor {    def receive = {     case \"Hello\" =&gt; sender ! \"And Hello to you!\" // same as sender.tell(\"And Hello to you!\")   } }  Sample of actor  package com.goticks import akka.actor.{PoisonPill, Actor} class TicketSeller extends Actor {   import TicketProtocol._   var tickets = Vector[Ticket]()   def receive = {     case GetEvents =&gt; sender ! tickets.size     case Tickets(newTickets) =&gt;       tickets = tickets ++ newTickets     case BuyTicket =&gt;       if (tickets.isEmpty) {         sender ! SoldOut         self ! PoisonPill }       tickets.headOption.foreach { ticket =&gt;         tickets = tickets.tail         sender ! ticket } } }  case Event(name, nrOfTickets) =&gt;   if(context.child(name).isEmpty) { If TicketSellers have not been    val ticketSeller = context.actorOf(Props[TicketSeller], name)   val tickets = Tickets((1 to nrOfTickets).map{     nr=&gt; Ticket(name, nr)).toList }   ticketSeller ! tickets } sender ! EventCreated    The BoxOffice creates TicketSellers for each event. Notice that it uses it’s context instead of the actor system to create the actor; Actors created with the context of another Actor are its children and subject to the parent Actor’s supervision   Test  Right now it always fails since it is not implemented yet, as is expected in Red-Green-Refactor style, where you first make sure the test fails (Red), then implement the code to make it pass (Green), after which you might refactor the code to make it nicer.   source code   UntypedActor  This class is the Java cousin to the akka.actor.Actor Scala interface. Subclass this abstract class to create a MDB-style untyped actor.   An actor has a well-defined (non-cyclic) life-cycle.   RUNNING (created and started actor) - can receive messages SHUTDOWN (when 'stop' or 'exit' is invoked) - can't do anything   The Actor’s own akka.actor.ActorRef is available as getSelf(), the current message’s sender as getSender() and the akka.actor.UntypedActorContext as getContext(). The only abstract method is onReceive() which is invoked for each processed message unless dynamically overridden using getContext().become().   Annotations     @Deprecated @deprecated  Deprecated   (Since version 2.5.0) Use AbstractActor instead of UntypedActor.   loggingAdaptor  trait LoggingAdapter extends AnyRef   Logging wrapper to make nicer and optimize: provide template versions which evaluate .toString only if the log level is actually enabled. Typically used by obtaining an implementation from the Logging object:  ","categories": [],
        "tags": [],
        "url": "/2018/07/23/akka-scala.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Scala",
        "excerpt":"Scala String   Scala offers the magic of implicit conver‐ sions, String instances also have access to all the methods of the StringOps class, so you can do many other things with them, such as treating a String instance as a sequence of characters. As a result, you can iterate over every character in the string using the foreach method:  scala&gt; \"hello\".foreach(println) h e l l o //You can treat a String as a sequence of characters in a for loop: scala&gt; for (c &lt;- \"hello\") println(c)  scala&gt; val result = \"hello world\".filter(_ != 'l') result: String = heo word  ","categories": [],
        "tags": [],
        "url": "/2018/07/28/Scala.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Cucumber",
        "excerpt":"Acceptance testing vs unit test  It’s sometimes said that unit tests ensure you build the thing right, whereas acceptance tests ensure you build the right thing.   Cucumber.  The acceptance testing tool.   Source of Truth  For many teams, the Cucumber feature files become the definitive source of truth as to what the system does.   Scenarios  Scenarios are written before production code. They start their life as an executable specification. As the production code emerges, Scenarios take on a role as living documentation and automated tests.   Each Cucumber test is called a scenario, and each scenario contains steps that tell Cucumber what to do.   Gherkin  The keywords Feature, Scenario, Given, When, and Then are the structure, and everything else is documentation.  The structure is called Gherkin.   Cucumber features are all about communicating with business users in their language, and it’s important that we don’t force them to sound like robots.   Structure  We start with features, which contain our scenarios and steps. The steps of our scenarios call step definitions that provide the link between the Gherkin fea- tures and the application being built.   This principle, deliberately doing the minimum useful work the tests will let us get away with, might seem lazy, but in fact it’s a discipline. It ensures that we make our tests thorough: if the test doesn’t drive us to write the right thing, then we need a better test.   Step Definitions  Step definitions are the glue that binds your Cucumber tests to the application you’re testing.   A scenario that’s been executed can end up in any of the following states:     Failed   Pending   Undefined   Skipped   Passed These states are designed to help indicate the progress that you make as you develop your tests.   Pending Steps  When Cucumber discovers a step definition that’s halfway through being implemented, it marks the step as pending (yellow). Again, the scenario will be stopped, and the rest of the steps will be skipped or marked as undefined.   public class Steps { @Given(\"^I have deposited \\\\$(\\\\d+) in my account$\") public void iHaveDeposited$InMyAccount(int amount) throws Throwable {     // Write code here that turns the phrase above into concrete actions throw new PendingException(); } }  Step definition recap   The pending status is a bit like those under construction signs you used to see all over the Internet in the 1990s. You can use it as a temporary signpost to your teammates that you’re in the middle of working on something.   Because regular expressions can contain wildcards, this means you have the flexibility to make the Gherkin steps nice and readable, while keeping your Java step definition code clean and free of duplication. • Step definitions provide a mapping from the Gherkin scenarios’ plain- language descriptions of user actions into Java code, which simulates those actions. • Step definitions are registered with Cucumber by using @Given, @When, @Then, or one of the aliases for your spoken language. • Step definitions use regular expressions to declare the steps that they can handle. Because regular expressions can contain wildcards, one step definition can handle several different steps. • A step definition communicates its result to Cucumber by raising, or not raising, an exception.   recap      Readability should be your number-one goal when writing Gherkin fea- tures. Always try to sit together with a stakeholder when you write your scenarios, or at the very least pass them over for feedback once you’ve written them. Keep fine-tuning the language in your scenarios to make them more readable.   Use a Background to factor out repeated steps from a feature and to help tell a story.   Repetitive scenarios can be collapsed into a Scenario Outline.   Steps can be extended with multiline strings or data tables.   You can organize features into subfolders, like chapters in a book.   Tags allow you to mark up scenarios and features so you select particular sets to run or report on.   Sample   compile and run via CLI  javac -cp \"jars/*\" step_definitions/CheckoutSteps.java java -cp \"jars/*:.\" cucumber.api.cli.Main -p pretty --snippets camelcase \\                           -g step_definitions features  Line 1 compiles the CheckoutSteps class that we’ve just created. Then line 2 invokes Cucumber. There are two slight additions to Cucumber’s invocation:     We’ve added the current directory “.” to the classpath.   We’ve added the -g step_definitions command-line argument to tell Cucumber where to look for the step definitions that it will need to “glue” the steps in the feature file to the checkout application (which we haven’t written yet).   feature file  Feature: Is it Friday yet?   Everybody wants to know when it's Friday    Scenario: Sunday isn't Friday     Given today is Sunday     When I ask whether it's Friday yet     Then I should be told \"Nope\"   The first line of this file starts with the keyword Feature: followed by a name. It’s a good idea to use a name similar to the file name.   The second line is a brief description of the feature. Cucumber does not execute this line, it’s just documentation.   The fourth line, Scenario: Sunday is not Friday is a Scenario, which is a concrete example illustrating how the software should behave.   The last three lines starting with Given, When and Then are the steps of our scenario. This is what Cucumber will execute.   Notice how we go from Scenario to Scenario Outline when we start using Examples.  Feature: Is it Friday yet?   Everybody wants to know when it's Friday    Scenario Outline: Today is or is not Friday     Given today is &lt;day&gt;     When I ask whether it's Friday yet     Then I should be told &lt;answer&gt;    Examples:     | day | answer |     | \"Friday\" | \"TGIF\" |     | \"Sunday\" | \"Nope\" |     | \"anything else!\" | \"Nope\" |   Scenario Outline: Withdraw fixed amount Given I have  in my account When I choose to withdraw the fixed amount of  Then I should receive  cash And the balance of my account should be  Examples:       | Balance | Withdrawal | Received | Remaining | | $500 | $500 | $500 |$50 |$50 |$450 | | $100 | $100 | $400 | | $200 | $200 | $300 | We indicate placeholders within the scenario outline using angle brackets (&lt;..&gt;) where we want real values to be substituted. The scenario outline itself is useless without an Examples table, which lists rows of values to be substituted for each placeholder.   Doc Strings  Doc strings allow you to specify a larger piece of text than you could fit on a single line. For example, if you need to describe the precise content of an email message, you could do it like this: Scenario: Ban Unscrupulous Users When I behave unscrupulously Then I should receive an email containing: “””     Dear Sir,     Your account privileges have been revoked due to your unscrupulous behavior. Sincerely, The Management “”” And my account should be locked Just like a data table, the entire string between the “”” triple quotes is attached to the step above it. The indentation of the opening “”” is not important, although common practice is to indent two spaces from the enclosing step, as we’ve shown. The indentation inside the triple quotes, however, is signifi- cant: imagine the left margin running down from the start of the first “””. If you want to include indentation within your string, you need to indent it within this margin.  TGIF  Thanks God It’s Friday   Data table  Given these Users: | name | date of birth |  | Michael Jackson | August 29, 1958 |  | Elvis | January 8, 1935 |  | John Lennon | October 9, 1940 |  That’s much clearer. The table starts on the line immediately following the step, and its cells are separated using the pipe character: |. You can line up the pipes using whitespace to make the table look tidy, although Cucumber doesn’t mind whether you do; it will strip out the values in each cell, ignoring the surrounding whitespace.   public class BoardSteps { @Given(“^a board like this:$”) public void aBoardLikeThis(DataTable arg1) throws Throwable {         // Write code here that turns the phrase above into concrete actions         // For automatic transformation, change DataTable to one of         // List, List&lt;List&gt;, List&lt;Map&lt;K,V&gt;&gt; or Map&lt;K,V&gt;.         // E,K,V must be a scalar (String, Integer, Date, enum etc) throw new PendingException(); }   BDD  The two main practices in the BDD approach are discovery workshops, which bridge the communication gap between business and IT, and executable specifications.   Background   Background: Given I have been issued a new card And I insert the card, entering the correct PIN And I choose \"Change PIN\" from the menu Scenario: Change PIN successfully When I change the PIN to 9876 Then the system should remember my PIN is now 9876 Scenario: Try to change PIN to the same as before When I try to change the PIN to the original PIN number Then I should see a warning message And the system should not have changed my PIN  Our refactoring hasn’t changed the behavior of the tests at all: at runtime, the steps in the background are executed at the beginning of each scenario, just as they were before. What we have done is made each individual scenario much easier to read.   Using a Background element isn’t always necessary, but it’s often useful to improve the readability of your features by removing repetitive steps from individual scenarios.   A good ‘background’     Make your Background section vivid. Use colorful names and try to tell a story, because your readers can keep track of stories much better than they can keep track of dull names like User A, User B, Site 1, and so on. If it’s worth mentioning at all, make it really stand out.   Keep your scenarios short, and don’t have too many. If the Background is more than three or four steps long, think about using higher-level steps or splitting the feature file in two. You can use a background as a good indicator of when a feature is getting too long: if the new scenarios you want to add don’t fit with the existing background, consider splitting the feature.   Avoid putting technical details such as clearing queues, starting back- end services, or opening browsers in a background.   Backgrounds are useful for taking Given (and sometimes When) steps that are repeated in each scenario and moving them to a single place. This helps keep your scenarios clear and concise.   Discovery Workshops   Discovery workshops (or Specification workshops) are short and frequent meetings where business and IT meet to gain a common understanding of how the software should behave.   Relationship with TDD  The main difference is that Cucumber operates on a higher abstraction level, closer to the domain and farther away from classes and methods. BDD builds on TDD, while preserving a strong link between the business requirements and the technical solution.   Outside in  This technique is called Outside-in because programmers typically start with the functionality that is closest to the user (the user interface, which is on the outside of the system) and gradually work towards the guts of the system (business logic, persistence, messaging and so on) as they discover more of what needs to be implemented.      Your cucumber features should drive your implementation, not reflect it.    This means Cucumber features should be written before the code implementing the feature.   Notice that we’re just sketching out the interface to the class, rather than adding any implementation to it. This way of working is fundamental to out- side-in development. We try not to think about how the Account is going to work yet but concentrate on what it should be able to do.   Keeping specifications, regression tests and documentation in a single place reduces the overhead of keeping multiple documents in sync - the Cucumber scenarios work as a shared source of truth for business and IT.   While many people focus on the value added by the automated “tests” you get out of BDD, the real value is actually the shared understanding we get at the beginning.   Cucumber is not a tool for testing software. It is a tool for testing people’s understanding of how software (yet to be written) should behave.   The biggest advantage of BDD approach for software development might be that they describe a set of functions that a user expects from a system in a very concrete and direct manner. The sum of these behaviors essentially document a contract with the user/client. If any of the tests fail, this contract is not upheld.   Process  the most important stage of BDD. Three amigos (business persons, developers, testers) get together and identify the expected behavior of our product by discussing examples. We can use feature mapping approach to effectively analyse and elaborate the product behavior.   always make sure that your scenarios are not tightly coupled with your tests. Your BDD scenarios should change only when the requirement changes, not when the the implementation changes (i.e. your BDD scenarios must drive the implementation, not the other way around).   Executable Specification   An Executable Specification is a Definition of Done that you can run as a test. In Behavior Driven Development (BDD), we refer to acceptance criteria as “executable specifications.” Executable Specifications are meant to be clear, unambiguous, written in business terms, and easy to automate. Each acceptance criteria is a concrete example of how a user interacts with the system to achieve some business goal.   The most well-known format for BDD acceptance criteria uses the “Given-When-Then” structure:  Given &lt;some precondition&gt; When &lt;something happens&gt; Then &lt;we expect some outcome&gt;  This format is a great way to make sure that we are thinking in terms of the outcomes we want to achieve. After all, the outcomes of an application are where the value lies.   These scenarios are also easy to automate with BDD tools like Cucumber and Specflow.   No silver bullet  “The hardest single part of building a software system is deciding precisely what to build.” We’ve all worked on projects where, because of a misunderstanding, code that we’d worked hard on for several days or more had to be thrown away. Better communication between developers and stakeholders is essential to help avoid this kind of wasted time. One technique that really helps facilitate this communication is the use of concrete examples to illustrate what we want the software to do.   Concrete Examples  By using real-world examples to describe the desired behavior of the system we want to build, we stay grounded in language and terminology that makes sense to our stakeholders: we’re speaking their language.   To illustrate this, let’s imagine you’re building a credit card payment system. One of the requirements is to make sure users can’t enter bad data. Here’s one way of expressing that: Customers should be prevented from entering invalid credit card details. This is an example of what Agile teams often call acceptance criteria or condi- tions of satisfaction.1 We use the word acceptance because they tell us what the system must be able to do in order for our stakeholders to find it acceptable.   The previous requirements statement is useful, but it leaves far too much room for ambiguity and misunderstanding. It lacks precision. What exactly makes a set of details invalid? How exactly should the user be prevented from entering them? We’ve seen too many projects get dragged into the tar pit2 by these kind of worthy but vague statements. Let’s try illustrating this requirement with a concrete example: If a customer enters a credit card number that isn’t exactly 16 digits long, when they try to submit the form, it should be redisplayed with an error message advising them of the correct number of digits.   Can you see how much more specific this second statement is? As a developer implementing this feature, we know almost everything we need to be able to sit down and start working on the code. As a stakeholder, we have a much clearer idea of what the developer is going to build. In fact, a stakeholder reading this might point out that there are certain types of cards that are valid with fewer than 16 digits and give us another example. This is the real power of examples: they stimulate our imagination, enabling us to explore and discover edge cases we might otherwise not have found until much later.   By giving an example to illustrate our requirement, we’ve turned an acceptance criterion into an acceptance test. Now we have something unambiguous that we can use to test the behavior of the system, either manually or by using an automated test script.   Gherkins                  Gherkin use main keywords: Feature, Scenario, Given, When, Then, And, But, Background, Scenario Outline, Examplesand some extra syntax “”” (Doc strings),       (Data tables), @(Tags), # (Comments).           dry run  $ java -cp \".:jars/*\" cucumber.api.cli.Main -g step_definitions --dry-run features   The –dry-run switch tells Cucumber to parse the file without executing it. It will tell you if your Gherkin isn’t valid.   Replacing Given/When/Then with Bullets  Some people find Given, When, Then, And, and But a little verbose. There is an additional keyword you can use to start a step: * (an asterisk). We could have written the previous scenario like this: Scenario: Attempt withdrawal using stolen card * I have $100 in my account     my card is invalid   I request $50   my card should not be returned   I should be told to contact the bank To Cucumber, this is exactly the same scenario. Do you find this version easier to read? Maybe. Did some of the meaning get lost? Maybe. It’s up to you and your team how you want to word things. The only thing that matters is that everybody understands what’s communicated.   CucumberOptions  the @CucumberOptions. One can define the location of features, glue files (step definitions), and formatter plugins inside this Cucumber options.   @CucumberOptions(         features = \"src/test/resources/features\",         glue = {\"stepdefs\"},         tags = {\"~@Ignore\"},         format = {                 \"pretty\",                 \"html:target/cucumber-reports/cucumber-pretty\",                 \"json:target/cucumber-reports/CucumberTestReport.json\",                 \"rerun:target/cucumber-reports/rerun.txt\"         }) public class TestRunner {   Step definitions  Cucumber doesn’t know how to execute your scenarios out-of-the-box. It needs Step Definitions to translate plain text Gherkin steps into actionsthat will interact with the system. When Cucumber executes a Step in a Scenario, it will look for a matching Step Definition to execute.   After  one can implement initial configurations of the project in TestNG’s BeforeClass method. In cucumber’s Before hook, one can implement code to open web browser which is a prerequisite for all scenarios. In Background of each feature, one can implement steps to navigate to web site and/or login to account. In Cucumber’s After hook, one can take a snapshot of failure and close the browser.   tags  Grouping Features, Scenarios, and Step Definitions using Tags Tags is a great way made for Cucumber power users to organize their features and scenarios. In above example, by changing tags = {“~@Ignore”} line totags = {“@UpdateProfile”}, one can choose run only the features and scenarios tagged with @UpdateProfile tag. A Scenario or feature can have as many tags as you like. Just separate them with spaces: @important @maintenance @db @auth   If subfolders are the chapters in your book of features, then tags are the sticky notes you’ve put on pages you want to be able to find easily. You tag a scenario by putting a word prefixed with the @ character on the line before the Scenario keyword, like this: @widgets Scenario: Generate report Given I am logged in   There are three main reasons for tagging scenarios:     Documentation: You want to use a tag to attach a label to certain scenarios, for example to label them with an ID from a project management tool.   Filtering: Cucumber allows you to use tags as a filter to pick out specific scenarios to run or report on. You can even have Cucumber fail your test run if a certain tag appears too many times.   Hooks: Run a block of code whenever a scenario with a particular tag is about to start or has just finished.   config tag   Tags are a great way to organise your features and scenarios. Consider this example:   @billing Feature: Verify billing   @important   Scenario: Missing product description     Given hello   Scenario: Several products     Given hello A feature or scenario or can have as many tags as you like. Just separate them with spaces:   @billing @bicker @annoy Feature: Verify billing Tags can be placed above the following Gherkin elements:   Feature Scenario Scenario Outline Examples It is not possible to place tags above Background or steps (Given, When, Then, And and But).   Cucumber for java 8 lambda  Using Lambda Expressions for Step Definitions Java Step Definitions are written in regular classes which don’t need to extend or implement anything. They can be written either using lambda expressions or method annotations. In the above, we used the method annotations. To use lambda expressions, use cucumber-java8 module instead of cucumber-java module in your pom.xml file.   When you use the cucumber-java8 module, you can write the Step Definitions using lambdas:   package cucumber;  import cucumber.api.java8.En;   public class StepDefinitions implements En {     public StepDefinitions() {         Given(\"I have (\\\\d+) cukes in my belly\", (Integer cukes) -&gt; {             System.out.format(\"Cukes: %n\\n\", cukes);         });     } }   package steps;   import cucumber.api.java8.En;   public class MyStepdefs implements En {       public MyStepdefs() {        Given(\"I login as (.*)$\",(String name)-&gt; System.out.println(name));     } }    Gherkin  Cucumber tests are expressed using a syntax called Gherkin. Gherkin files are plain text and have a .feature extension.   Steps and Step Definitions  Let’s start by clarifying the distinction between a step and a step definition. Each Gherkin scenario is made up of a series of steps, written in plain lan- guage. On its own, a step is just documentation; it needs a step definition to bring it to life. A step definition is a piece of code that says to Cucumber, “If you see a step that looks like this…, then here’s what I want you to do….” When Cucumber tries to execute each step, it looks for a matching step defi- nition to execute. So, how does Cucumber match a step definition to a step?   Creating a Step Definition  If Cucumber sees a step definition with this regular expression, it will execute it when it comes to the first step of our scenario. So, how do we create a step definition?   Step definitions live in ordinary files. To create a step definition in Java, you use a special Cucumber annotation, such as @Given, like this: @Given(“I have \\$100 in my Account”) public void iHave$100InMyAccount() throws Throwable {     // TODO: code that puts $100 into User’s Account goes here }   Given, When, Then Are the Same  It doesn’t actually matter which of the three methods you use to register a step definition, because Cucumber ignores the keyword when matching a step. Under the hood, all of the annotations are aliases for StepDefAnnotation.   The best way we’ve found to avoid this kind of problem is to pay careful attention to the precise wording in your steps. You could change both steps to be less ambiguous: Given I have deposited $100 in my Account Then the balance of my Account should be $100 By rewording the steps like this, you’ve made them better at communicating exactly what they will do when executed. Learning to spot and remove this kind of ambiguity is something that takes practice. Paying attention to the distinction in wording between two steps like this can also give you hints about concepts that may not be expressed in your code but need to be. It might seem pedantic, but we’ve found that teams who pay this much careful attention to detail write much better software, faster.   Alternation  We can specify a wildcard in a regular expression using a few different approaches. One of the simplest is alternation, where we express different options separated by a pipe character |, like this:  @Given(\"I have deposited \\\\$(100|250) in my Account\") public void iHaveDeposited$InMyAccount(int amount) {   // TODO: code goes here }  This step definition will now match a step with either of the two values 100 or 250 in it, and the number will be captured and passed to the method as an argument. Alternation can be useful if there are a fixed set of values that you want to accept in your step definition, but normally you’ll want something a little looser.   The Dot  The dot is a metacharacter, meaning it has magical powers in a regular expression. Literally, a dot means match any single character. So, we can try this instead:  @Given(\"I have deposited \\\\$(...) in my Account\") public void iHaveDeposited$InMyAccount(int amount) {   // TODO: code goes here }  That will now match a step with any three-figure dollar sum and send the matched amount into the method.   What If I Actually Want to Match a Dot?   Any of the metacharacters like the dot can be escaped by preceding them with a backslash. So, if you wanted to specifically match, say 3.14, you could use “3\\.14”. You might have noticed that there’s a backslash in front of the dollar amount in the step definition we’re using. That’s because $ itself is a metacharacter (it’s an anchor, which we’ll explain later), so we need to escape to make it match a normal dollar sign.   Star modifier  The star modifier means any number of times. So, with .* we’re capturing any character, any number of times. Now we’re getting somewhere—this will allow us to capture all those different amounts. But there’s still a problem. The star modifier is a bit of a blunt instrument. Because we’re using it with the dot that matches any character, it will gobble up any text at all up until the phrase in my Account. This is why, in regex terminology, the star modifier is known as a greedy operator. For example, it would happily match this step: Given I have deposited $1 and a cucumber in my Account The amount captured by our regular expression in this case would be 1 and a cucumber. We need to be more specific about the characters we want to match and just capture numbers. Instead of a dot, we can use something else.   Character Classes  Character classes allow you to tell the regular expression engine to match one of a range of characters. You just place all of the characters you would accept inside square brackets:   @Given(\"I have deposited \\\\$([01234567890]*) in my Account\") public void iHaveDeposited$InMyAccount(int amount) {   // TODO: code goes here } For a continuous range of characters like we have, you can use a hyphen: @Given(\"I have deposited \\\\$([0-9]*) in my Account\") public void iHaveDeposited$InMyAccount(int amount) {   // TODO: code goes here }   Shorthand Character Classes  For common patterns of characters like [0-9], there are a few shorthand char- acter classes that you can use instead. You may find this just makes your regular expressions more cryptic, but there are only a few to learn. For a digit, you can use \\d as a shorthand for [0-9]:  @Given(\"I have deposited \\\\$(\\\\d*) in my Account\") public void iHaveDeposited$InMyAccount(int amount) {   // TODO: code goes here }  Here are the most useful shorthand character classes: \\d stands for digit, or [0-9]. \\w stands for word character, specifically [A-Za-z0-9_]. Notice that underscores and digits are included but not hyphens. \\s stands for whitespace character, specifically [ \\t\\r\\n]. That means a space, a tab, or a line break. \\b stands for word boundary, which is a lot like \\s but actually means the opposite of \\w. Anything that is not a word character is a word boundary. You can also negate shorthand character classes by capitalizing them, so for example, \\D means any character except a digit. Back to matching our amount. It looks like we’re done, but there’s one last problem to fix. Can you see what it is?   question mark  Like the star and the plus, the question mark modifies the character that precedes it, specifying how many times it can be repeated. The question mark modifier means zero or one times; in other words, it makes the preceding character optional. In step definitions, it’s particularly useful for plurals: @Given(“I have (\\d+) cucumbers? in my basket”) public void iHaveCucumbersInMyBasket(int number) {   // TODO: code goes here }   noncapturing group  @When(“I (?:visit|go to) the homepage”) public void iVisitTheHomepage() {   // TODO: code goes here } Notice that we’ve had to prefix the list of alternates with another bit of regular expression magic. The ?: at the start of the group marks it as noncapturing, meaning Cucumber won’t pass it as an argument to our block.   Anchors  The undefined steps start with a ^ and end with a $. These two metacharacters are called anchors, because they’re used to tie down each end of the regular expression to the beginning and end of the string that they match on.   Generally, it’s best to keep your regular expressions as tight as you can so that there’s less chance of two step definitions clashing with each other.   Guides on how to write scenarios  Try to avoid being guided by existing step definitions when you write your scenarios and just write down exactly what you want to happen, in plain English. In fact, try to avoid programmers or testers writing scenarios on their own. Instead, get nontechnical stakeholders or analysts to write the first draft of each scenario from a purely business-focused perspective or ideally in a pair with a programmer to help them share their mental model. With a well- engineered support layer, you can confidently and quickly write new step definitions to match the way the scenario has been expressed.   Imperative Steps  In computer programming, there are two contrasting styles for expressing the instructions you give to a computer to make it do something for you. These styles are called imperative programming and declarative programming.   Imperative programming means using a sequence of commands for the com- puter to perform in a particular order. Java is an example of an imperative language: you write a program as a series of statements that Java runs one at a time, in order. A declarative program tells the computer what it should do without prescribing precisely how to do it. CSS is an example of a declar- ative language: you tell the computer what you want the various elements on a web page to look like, and you leave it to take care of the rest.   Use a Declarative Style Instead  Let’s raise the level of abstraction in this scenario and rewrite it using a more declarative style: Scenario: Redirect user to originally requested page after logging in Given I am an unauthenticated User When I attempt to view some restricted content Then I am shown a login form When I authenticate with valid credentials Then I should be shown the restricted content The beauty of this style is that it is not coupled to any specific implementation of the user interface. This same scenario could apply to a thick-client or mobile application. The words it uses aren’t technical and are instead written in a language (unauthenticated, restricted, credentials) that any stakeholder interested in security should be able to clearly understand. It’s by expressing every scenario at this level of abstraction that you discover your team’s ubiquitous language.   DAMP   However, when you are using examples to drive your code, there is another principle in play that I believe trumps the DRY principle: the examples should tell a good story. They are the docu- mentation narrative that will guide future programmers (including you when you come back to change this code in three months time and you’ve forgotten what it does). In this case, clarity of intent is found in the quality of the narrative, not necessarily in minimizing duplication.   Some people refer to this as the DAMP principle: Descriptive and Meaningful Phrases. When you’re writing examples, readability is paramount, and DAMP trumps DRY.   We consider fixture data to be an antipattern. We much prefer using Test Data Builders, on page 104, where the relevant data is created within the test itself, rather than being buried away in a big tangled set of fixture data.   We find that teams that have a single humongous build also tend to have an architecture that could best be described as a big ball of mud. Because all of the behavior in the system is implemented in one place, all the tests have to live in one place, too, and have to all be run together as one big lump. This is a classic ailment of long-lived applications, which have grown organically without obvious interfaces between their subsystems.   Defect Prevention  Toyota’s counterintuitive but hugely successful policy of stopping the line works because it’s part of a wider process, known as defect prevention, that focuses on continuously improving the manufacturing system. Without this wider process, stop the line itself would have very little effect. There are four steps to this process:     Detect the abnormality.   Stop what you’re doing.   Fix or correct the immediate problem.   Investigate the root cause and install a countermeasure. This fourth step is crucial because it seizes the opportunity offered by the problem at hand to understand something more fundamental about your process. It also means that fixing things becomes a habit, rather than some- thing you put off to do someday later when you’re not in such a hurry.   Cucumber might just seem like a testing tool, but at its heart it’s really a collaboration tool. If you make a genuine effort to write features that work as documentation for the nontechnical stakeholders on your team, you’ll find you are forced to talk with them about details that you might never have otherwise made the time to talk about. Those conversations reveal insights about their understanding of the problem, insights that will help you build a much better solution than you would have otherwise. This is Cucumber’s big secret: the tests and documentation are just a happy side effect; the real value lies in the knowledge you discover during those conversations.   Reference     https://medium.com/agile-vision/cucumber-bdd-part-2-creating-a-sample-java-project-with-cucumber-testng-and-maven-127a1053c180   https://codoid.com/cucumber-lambda-expressions/  ","categories": [],
        "tags": [],
        "url": "/2018/08/08/Cucumber.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Chronicle",
        "excerpt":"Overview   Chronicle Software is about simplifying fast data.  It is a suite of libraries to make it easier to write, monitor and tune data processing systems where performance and scalability are concerned.   Writing to a Queue  In Chronicle Queue we refer to the act of writing your data to the Chronicle queue, as storing an excerpt. This data could be made up from any data type, including text, numbers, or serialised blobs. Ultimately, all your data, regardless of what it is, is stored as a series of bytes.   Just before storing your excerpt, Chronicle Queue reserves an 8-byte header. Chronicle Queue writes the length of your data into this header. This way, when Chronicle Queue comes to read your excerpt, it knows how long each blob of data is. We refer to this 8-byte header, along with your excerpt, as a document. So strictly speaking Chronicle Queue can be used to read and write documents.      Within this 8-byte header we also reserve a few bits for a number of internal operations, such as locking, to make Chronicle Queue thread-safe across both processors and threads. The important thing to note is that because of this, you can’t strictly convert the 8 bytes to an integer to find the length of your data blob.    To write data to a Chronicle-Queue, you must first create an Appender   try (ChronicleQueue queue = SingleChronicleQueueBuilder.binary(path + \"/trades\").build()) {    final ExcerptAppender appender = queue.acquireAppender(); }  Chronicle Queue uses the following low-level interface to write the data:  try (final DocumentContext dc = appender.writingDocument()) {       dc.wire().write().text(“your text data“); }  So, Chronicle Queue uses an Appender to write to the queue and a Tailer to read from the queue. Unlike other java queuing solutions, messages are not lost when they are read with a Tailer.   Each Chronicle Queue excerpt has a unique index.   try (final DocumentContext dc = appender.writingDocument()) {     dc.wire().write().text(“your text data“);     System.out.println(\"your data was store to index=\"+ dc.index()); }   The high-level methods below such as writeText() are convenience methods on calling appender.writingDocument(), but both approaches essentially do the same thing. The actual code of writeText(CharSequence text) looks like this:   /**  * @param text to write a message  */ void writeText(CharSequence text) {     try (DocumentContext dc = writingDocument()) {         dc.wire().bytes().append8bit(text);     } }    This is the highest-level API which hides the fact you are writing to messaging at all. The benefit is that you can swap calls to the interface with a real component, or an interface to a different protocol.  // using the method writer interface. RiskMonitor riskMonitor = appender.methodWriter(RiskMonitor.class); final LocalDateTime now = LocalDateTime.now(Clock.systemUTC()); riskMonitor.trade(new TradeDetails(now, \"GBPUSD\", 1.3095, 10e6, Side.Buy, \"peter\"));   You can write a “self-describing message”. Such messages can support schema changes. They are also easier to understand when debugging or diagnosing problems.  // writing a self describing message appender.writeDocument(w -&gt; w.write(\"trade\").marshallable(         m -&gt; m.write(\"timestamp\").dateTime(now)                 .write(\"symbol\").text(\"EURUSD\")                 .write(\"price\").float64(1.1101)                 .write(\"quantity\").float64(15e6)                 .write(\"side\").object(Side.class, Side.Sell)                 .write(\"trader\").text(\"peter\")));   You can write “raw data” which is self-describing. The types will always be correct; position is the only indication as to the meaning of those values.  // writing just data appender.writeDocument(w -&gt; w         .getValueOut().int32(0x123456)         .getValueOut().int64(0x999000999000L)         .getValueOut().text(\"Hello World\"));  You can write “raw data” which is not self-describing. Your reader must know what this data means, and the types that were used.  // writing raw data appender.writeBytes(b -&gt; b         .writeByte((byte) 0x12)         .writeInt(0x345678)         .writeLong(0x999000999000L)         .writeUtf8(\"Hello World\"));  This is the lowest level way to write data. You get an address to raw memory and you can write what you want.  // Unsafe low level appender.writeBytes(b -&gt; {     long address = b.address(b.writePosition());     Unsafe unsafe = UnsafeMemory.UNSAFE;     unsafe.putByte(address, (byte) 0x12);     address += 1;     unsafe.putInt(address, 0x345678);     address += 4;     unsafe.putLong(address, 0x999000999000L);     address += 8;     byte[] bytes = \"Hello World\".getBytes(StandardCharsets.ISO_8859_1);     unsafe.copyMemory(bytes, Unsafe.ARRAY_BYTE_BASE_OFFSET, null, address, bytes.length);     b.writeSkip(1 + 4 + 8 + bytes.length); });  You can print the contents of the queue. You can see the first two, and last two messages store the same data.   // dump the content of the queue System.out.println(queue.dump());  position: 262568, header: 0  — !!data #binary trade: {   timestamp: 2016-07-17T15:18:41.141,   symbol: GBPUSD,   price: 1.3095,   quantity: 10000000.0,   side: Buy,   trader: peter }  position: 262684, header: 1  — !!data #binary trade: {   timestamp: 2016-07-17T15:18:41.141,   symbol: EURUSD,   price: 1.1101,   quantity: 15000000.0,   side: Sell,   trader: peter }  position: 262800, header: 2  — !!data #binary !int 1193046 168843764404224 Hello World  position: 262830, header: 3  — !!data #binary 000402b0       12 78 56 34 00 00  90 99 00 90 99 00 00 0B   ·xV4·· ········ 000402c0 48 65 6C 6C 6F 20 57 6F  72 6C 64                Hello Wo rld  position: 262859, header: 4  — !!data #binary 000402c0                                               12                 · 000402d0 78 56 34 00 00 90 99 00  90 99 00 00 0B 48 65 6C xV4····· ·····Hel 000402e0 6C 6F 20 57 6F 72 6C 64   Finding the index at the end of a Chronicle Queue   Chronicle Queue appenders are thread-local. In fact when you ask for:  final ExcerptAppender appender = queue.acquireAppender();  the acquireAppender() uses a thread-local pool to give you an appender which will be reused to reduce object creation.   As such, the method call to:  long index = appender.lastIndexAppended();  will only give you the last index appended by this appender; not the last index appended by any appender.   If you wish to find the index of the last record written, then you have to call:  long index = queue.createTailer().toEnd().index();   Dumping a Chronicle Queue, cq4 file as text to the Command Line   Chronicle Queue stores its data in binary format, with a file extension of cq4:   \\��@π�header∂�SCQStoreÇE���»wireType∂�WireTypeÊBINARYÕwritePositionèèèèß��������ƒroll∂�SCQSRollÇ*���∆length¶ÄÓ6�∆format ÎyyyyMMdd-HH≈epoch¶ÄÓ6�»indexing∂\fSCQSIndexingÇN��� indexCount•��ÃindexSpacing�Àindex2Indexé����ß��������…lastIndexé� ���ß��������ﬂlastAcknowledgedIndexReplicatedé������ßˇˇˇˇˇˇˇˇ»recovery∂�TimedStoreRecoveryÇ����…timeStampèèèß���������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������� This can often be a bit difficult to read, so it is better to dump the cq4 files as text. This can also help you fix your production issues, as it gives you the visibility as to what has been stored in the queue, and in what order.   You have to use the chronicle-queue.jar, from any version 4.5.3 or later, and set up the dependent files in the class path.   $ java -cp chronicle-queue-4.5.5.jar net.openhft.chronicle.queue.DumpQueueMain 19700101-02.cq4   this will dump the 19700101-02.cq4 file out as text, as shown below:   — !!meta-data #binary header: !SCQStore {   wireType: !WireType BINARY,   writePosition: 0,   roll: !SCQSRoll {     length: !int 3600000,     format: yyyyMMdd-HH,     epoch: !int 3600000   },   indexing: !SCQSIndexing {     indexCount: !short 4096,     indexSpacing: 4,     index2Index: 0,     lastIndex: 0   },   lastAcknowledgedIndexReplicated: -1,   recovery: !TimedStoreRecovery {     timeStamp: 0   } }   … 4198044 bytes remaining   Reading from a Queue using a Tailer   Reading the queue follows the same pattern as writing, except there is a possibility there is not a message when you attempt to read it.   Start Reading  try (ChronicleQueue queue = SingleChronicleQueueBuilder.binary(path + \"/trades\").build()) {    final ExcerptTailer tailer = queue.createTailer(); }   You can turn each message into a method call based on the content of the message.  // reading using method calls RiskMonitor monitor = System.out::println; MethodReader reader = tailer.methodReader(monitor); // read one message assertTrue(reader.readOne());  You can decode the message yourself.  assertTrue(tailer.readDocument(w -&gt; w.read(\"trade\").marshallable(         m -&gt; {             LocalDateTime timestamp = m.read(\"timestamp\").dateTime();             String symbol = m.read(\"symbol\").text();             double price = m.read(\"price\").float64();             double quantity = m.read(\"quantity\").float64();             Side side = m.read(\"side\").object(Side.class);             String trader = m.read(\"trader\").text();             // do something with values.         })));   You can read self-describing data values. This will check the types are correct, and convert as required.   assertTrue(tailer.readDocument(w -&gt; {     ValueIn in = w.getValueIn();     int num = in.int32();     long num2 = in.int64();     String text = in.text();     // do something with values }));   You can read raw data as primitives and strings.   assertTrue(tailer.readBytes(in -&gt; {     int code = in.readByte();     int num = in.readInt();     long num2 = in.readLong();     String text = in.readUtf8();     assertEquals(\"Hello World\", text);     // do something with values }));   or, you can get the underlying memory address and access the native memory.   assertTrue(tailer.readBytes(b -&gt; {     long address = b.address(b.readPosition());     Unsafe unsafe = UnsafeMemory.UNSAFE;     int code = unsafe.getByte(address);     address++;     int num = unsafe.getInt(address);     address += 4;     long num2 = unsafe.getLong(address);     address += 8;     int length = unsafe.getByte(address);     address++;     byte[] bytes = new byte[length];     unsafe.copyMemory(null, address, bytes, Unsafe.ARRAY_BYTE_BASE_OFFSET, bytes.length);     String text = new String(bytes, StandardCharsets.UTF_8);     assertEquals(\"Hello World\", text);     // do something with values }));  Tailers and File Handlers Clean up   Chronicle queue tailers may create file handlers, the file handlers are cleaned up whenever the associated chronicle queue is close() or whenever the Jvm runs a Garbage Collection.   ExcerptTailer.toEnd()   In some applications, it may be necessary to start reading from the end of the queue (e.g. in a restart scenario). For this use-case, ExcerptTailer provides the toEnd() method.   If it is necessary to read backwards through the queue from the end, then the tailer can be set to read backwards:  ExcerptTailer tailer = queue.createTailer(); tailer.direction(TailerDirection.BACKWARD).toEnd();   When reading backwards, then the toEnd() method will move the tailer to the last record in the queue. If the queue is not empty, then there will be a DocumentContext available for reading:  // this will be true if there is at least one message in the queue boolean messageAvailable = tailer.toEnd().direction(TailerDirection.BACKWARD).         readingDocument().isPresent();   Low GC  Ultra low GC means less than one minor collection per day.   the principles of Zero-copy eliminating unnecessary garbage collection and increased speed. Runtime code generation that reduces code size for efficient CPU cache usage and increased speed. Smart ordering for optimal parsing and you guessed it increased speed. All these combine to allow Chronicle FIX to achieve excellent performance results.   Low garbage rate   Minimising garbage is key to avoiding GC pauses. To use your L1 and L2 cache efficiently, you need to keep your garbage rates very low.  If you are not using these cache efficiently your application can be 2-5x slower.   The garbage from Chronicle is low enough that you can process one million events without jstat detecting you have created any garbage.  jstat only displays multiples of 4 KB, and only when a new TLAB is allocated.  Chronicle does create garbage, but it is extremely low. i.e. a few objects per million events processes.   Once you make the GC pauses manageable, or non-existent, you start to see other sources of delay in your system.   Take away the boulders and you start to see the rocks.  Take away the rocks and you start to see the pebbles.   Chronicle has minimal interaction with the Operating System.   System calls are slow, and if you can avoid call the OS, you can save significant amounts of latency.   For example, if you send a message over TCP on loopback, this can add a 10 micro-seconds latency between writing and reading the data.  You can write to a chronicle, which is a plain write to memory, and read from chronicle, which is also a read from memory with a latency of 0.2 micro-seconds. (And as I mentioned before, you get persistence as well)   No need to worry about running out of heap.   A common problem with unbounded queues and this uses an open ended amount of heap.   Chronicle solves this by not using the heap to store data, but instead using memory mapped files.  This improve memory utilisation by making the data more compact but also means a 1 GB JVM can stream 1 TB of data over a day without worrying about the heap or how much main memory you have.  In this case, an unbounded queue becomes easier to manage.   how it works  Chronicle uses a memory mapped file to continuously journal messages, chronicles file-based storage will slowly grow in size as more data is written to the queue, the size of the queue can exceed your available memory, you are only constrained by the amount of disk space you have on your server. Chronicle writes data directly into off-heap memory which is shared between java processes on the same server.   Chronicle is very fast, it is able to write and read a message in just two microseconds with no garbage. Typically at the end of each day, you archive the queue and start the next day with a fresh empty queue.   Chronicle Queue is a distributed unbounded persisted queue.   Chronicle Queue:   supports asynchronous RMI and Publish/Subscribe interfaces with microsecond latencies.   passes messages between JVMs in under a microsecond (in optimised examples)   passes messages between JVMs on different machines via replication in under 10 microseconds (in optimised examples)   provides stable, soft, real time latencies into the millions of messages per second for a single thread to one queue; with total ordering of every event.   Queue introduction   Chronicle Queue is a Java project focused on building a persisted low-latency messaging framework for high performance and critical applications.   Chronicle diagram 005 At first glance Chronicle Queue can be seen as simply another queue implementation. However, it has major design choices that should be emphasised.   Using non-heap storage options (RandomAccessFile), Chronicle Queue provides a processing environment where applications do not suffer from Garbage Collection (GC). When implementing high-performance and memory-intensive applications (you heard the fancy term “bigdata”?) in Java, one of the biggest problems is garbage collection.   Garbage collection may slow down your critical operations non-deterministically at any time. In order to avoid non-determinism, and escape from garbage collection delays, off-heap memory solutions are ideal. The main idea is to manage your memory manually so it does not suffer from garbage collection. Chronicle Queue behaves like a management interface over off-heap memory so you can build your own solutions over it.   Chronicle Queue uses RandomAccessFiles while managing memory and this choice brings lots of possibilities. RandomAccessFiles permit non-sequential, or random, access to a file’s contents. To access a file randomly, you open the file, seek a particular location, and read from or write to that file. RandomAccessFiles can be seen as “large” C-type byte arrays that you can access at any random index “directly” using pointers. File portions can be used as ByteBuffers if the portion is mapped into memory.   This memory mapped file is also used for exceptionally fast interprocess communication (IPC) without affecting your system performance. There is no garbage collection as everything is done off-heap.   Message type     TCP: Stream-oriented   UDP, SCTP: message-oriented .   On heap vs off heap memory usage  Overview   I was recently asked about the benefits and wisdom of using off heap memory in Java.  The answers may be of interest to others facing the same choices.   Off heap memory is nothing special.  The thread stacks, application code, NIO buffers are all off heap.  In fact in C and C++, you only have unmanaged memory as it does not have a managed heap by default.  The use of managed memory or “heap” in Java is a special feature of the language. Note: Java is not the only language to do this. new Object() vs Object pool vs Off Heap memory.   new Object()   Before Java 5.0, using object pools was very popular.  Creating objects was still very expensive.   However, from Java 5.0, object allocation and garbage cleanup was made much cheaper, and developers found they got a performance speed up and a simplification of their code by removing object pools and just creating new objects whenever needed.  Before Java 5.0, almost any object pool, even an object pool which used objects provided an improvement, from Java 5.0 pooling only expensive objects obviously made sense e.g. threads, socket and database connections.   Object pools   In the low latency space it was still apparent that recycling mutable objects improved performance by reduced pressure on your CPU caches.  These objects have to have simple life cycles and have a simple structure, but you could see significant improvements in performance and jitter by using them.  Another area where it made sense to use object pools is when loading large amounts of data with many duplicate objects. With a significant reduction in memory usage and a reduction in the number of objects the GC had to manage, you saw a reduction in GC times and an increase in throughput. These object pools were designed to be more light weight than say using a synchronized HashMap, and so they still helped.   Take this StringInterner class as an example. You pass it a recycled mutable StringBuilder of the text you want as a String and it will provide a String which matches.  Passing a String would be inefficient as you would have already created the object.  The StringBuilder can be recycled. Note: this structure has an interesting property that requires no additional thread safety features, like volatile or synchronized, other than is provided by the minimum Java guarantees. i.e. you can see the final fields in a String correctly and only read consistent references.   public class StringInterner {     private final String[] interner;     private final int mask;     public StringInterner(int capacity) {         int n = Maths.nextPower2(capacity, 128);         interner = new String[n];         mask = n - 1;     }   private static boolean isEqual(@Nullable CharSequence s, @NotNull CharSequence cs) {     if (s == null) return false;     if (s.length() != cs.length()) return false;     for (int i = 0; i &lt; cs.length(); i++)         if (s.charAt(i) != cs.charAt(i))             return false;     return true; }  @NotNull public String intern(@NotNull CharSequence cs) {     long hash = 0;     for (int i = 0; i &lt; cs.length(); i++)         hash = 57 * hash + cs.charAt(i);     int h = (int) Maths.hash(hash) &amp; mask;     String s = interner[h];     if (isEqual(s, cs))         return s;     String s2 = cs.toString();     return interner[h] = s2; } } Off heap memory usage   Using off heap memory and using object pools both help reduce GC pauses, this is their only similarity.  Object pools are good for short lived mutable objects, expensive to create objects and long live immutable objects where there is a lot of duplication.  Medium lived mutable objects, or complex objects are more likely to be better left to the GC to handle.  However, medium to long lived mutable objects suffer in a number of ways which off heap memory solves.   Off heap memory provides;   Scalability to large memory sizes e.g. over 1 TB and larger than main memory. Notional impact on GC pause times. Sharing between processes, reducing duplication between JVMs, and making it easier to split JVMs. Persistence for faster restarts or replying of production data in test. The use of off heap memory gives you more options in terms of how you design your system.  The most important improvement is not performance, but determinism.   Off heap and testing   One of the biggest challenges in high performance computing is reproducing obscure bugs and being able to prove you have fixed them.  By storing all your input events and data off heap in a persisted way you can turn your critical systems into a series of complex state machines. (Or in simple cases, just one state machine) In this way you get reproducible behaviour and performance between test and production.   A number of investment banks use this technique to replay a system reliably to any event in the day and work out exactly why that event was processed the way it was.  More importantly, once you have a fix you can show that you have fixed the issue which occurred in production, instead of finding an issue and hoping this was the issue.   Along with deterministic behaviour comes deterministic performance.  In test environments, you can replay the events with realistic timings and show the latency distribution you expect to get in production.  Some system jitter can’t be reproduce esp if the hardware is not the same, but you can get pretty close when you take a statistical view.  To avoid taking a day to replay a day of data you can add a threshold. e.g. if the time between events is more than 10 ms you might only wait 10 ms.  This can allow you to replay a day of events with realistic timing in under an hour and see whether your changes have improved your latency distribution or not.   By going more low level don’t you lose some of “compile once, run anywhere”?   To some degree this is true, but it is far less than you might think.  When you are working closer the processor and so you are more dependant on how the processor, or OS behaves.  Fortunately, most systems use AMD/Intel processors and even ARM processors are becoming more compatible in terms of the low level guarantees they provide.  There is also differences in the OSes, and these techniques tend to work better on Linux than Windows.  However, if you develop on MacOSX or Windows and use Linux for production, you shouldn’t have any issues.  This is what we do at Higher Frequency Trading.   What new problems are we creating by using off heap?   Nothing comes for free, and this is the case with off heap.  The biggest issue with off heap is your data structures become less natural.  You either need a simple data structure which can be mapped directly to off heap, or you have a complex data structure which serializes and deserializes to put it off heap.  Obvious using serialization has its own headaches and performance hit.  Using serialization thus much slower than on heap objects.   In the financial world, most high ticking data structure are flat and simple, full of primitives which maps nicely off heap with little overhead.   How does Chronicle Queue work   Terminology      Messages are grouped by topics. A topic can contain any number of sub-topics which are logically stored together under the queue/topic.   An appender is the source of messages.   A tailer is a receiver of messages.   Chronicle Queue is broker-less by default. You can use Chronicle Engine to act as a broker for remote access.      Note We deliberately avoid the term consumer as messages are not consumed/destroyed by reading.    At a high level:           appenders write to the end of a queue. There is no way to insert, or delete excerpts.            tailers read the next available message each time they are called.       By using Chronicle Engine, a Java or C# client can publish to a queue to act as a remote appender, and you subscribe to a queue to act as a remote tailer.   Topics and Queue files   Each topic is a directory of queues. There is a file for each roll cycle. If you have a topic called mytopic, the layout could look like this:   mytopic/     20160710.cq4     20160711.cq4     20160712.cq4     20160713.cq4 To copy all the data for a single day (or cycle), you can copy the file for that day on to your development machine for replay testing.   Appenders and tailers are cheap as they don’t even require a TCP connection; they are just a few Java objects.   File Retention   You can add a StoreFileListener to notify you when a file is added, or no longer used. This can be used to delete files after a period of time. However, by default, files are retained forever. Our largest users have over 100 TB of data stored in queues.   Every Tailer sees every message.   An abstraction can be added to filter messages, or assign messages to just one message processor. However, in general you only need one main tailer for a topic, with possibly, some supporting tailers for monitoring etc.   As Chronicle Queue doesn’t partition its topics, you get total ordering of all messages within that topic. Across topics, there is no guarantee of ordering; if you want to replay deterministically from a system which consumes from multiple topics, we suggest replaying from that system’s output.   Guarantees   Chronicle Queue provides the following guarantees;   for each appender, messages are written in the order the appender wrote them. Messages by different appenders are interleaved,   for each tailer, it will see every message for a topic in the same order as every other tailer,   when replicated, every replica has a copy of every message.   Use Cases   Chronicle Queue is most often used for producer-centric systems where you need to retain a lot of data for days or years.   What is a producer-centric system?   Most messaging systems are consumer-centric. Flow control is implemented to avoid the consumer ever getting overloaded; even momentarily. A common example is a server supporting multiple GUI users. Those users might be on different machines (OS and hardware), different qualities of network (latency and bandwidth), doing a variety of other things at different times. For this reason it makes sense for the client consumer to tell the producer when to back off, delaying any data until the consumer is ready to take more data.   Chronicle Queue is a producer-centric solution and does everything possible to never push back on the producer, or tell it to slow down. This makes it a powerful tool, providing a big buffer between your system, and an upstream producer over which you have little, or no, control.   For market data in particular, real time means in a few microseconds; it doesn’t mean intra-day (during the day).   Chronicle Queue is fast and efficient, and has been used to increase the speed that data is passed between threads. In addition, it also keeps a record of every message passed allowing you to significantly reduce the amount of logging that you need to do.   Latency Sensitive Micro-services   Chronicle Queue supports low latency IPC (Inter Process Communication) between JVMs on the same machine in the order of magnitude of 1 microsecond; as well as between machines with a typical latency of 10 microseconds for modest throughputs of a few hundred thousands. Chronicle Queue supports throughputs of millions of events per second, with stable microsecond latencies.   Log Replacement   As Chronicle Queue can be used to build state machines. All the information about the state of those components can be reproduced externally, without direct access to the components, or to their state. This significantly reduces the need for additional logging.   However, any logging you do need can be recorded in great detail. This makes enabling DEBUG logging in production practical. This is because the cost of logging is very low; less than 10 microseconds. Logs can be replicated centrally for log consolidation.   Chronicle Queue is being used to store 100+ TB of data, which can be replayed from any point in time.   Source code   MappedFile  package net.openhft.chronicle.bytes;  import net.openhft.chronicle.core.Jvm; import net.openhft.chronicle.core.OS; import net.openhft.chronicle.core.ReferenceCounted; import net.openhft.chronicle.core.ReferenceCounter; import net.openhft.chronicle.core.io.IORuntimeException; import org.jetbrains.annotations.NotNull; import org.jetbrains.annotations.Nullable;  import java.io.File; import java.io.FileNotFoundException; import java.io.IOException; import java.io.RandomAccessFile; import java.lang.ref.WeakReference; import java.nio.channels.FileChannel; import java.nio.channels.FileChannel.MapMode; import java.nio.channels.FileLock; import java.nio.file.Files; import java.util.ArrayList; import java.util.List; import java.util.concurrent.atomic.AtomicBoolean;  import static net.openhft.chronicle.core.io.Closeable.closeQuietly;  /**  * A memory mapped files which can be randomly accessed in chunks. It has overlapping regions to  * avoid wasting bytes at the end of chunks.  */ public class MappedFile implements ReferenceCounted {     private static final long DEFAULT_CAPACITY = 128L &lt;&lt; 40;     // A single JVM cannot lock a file more than once.     private static final Object GLOBAL_FILE_LOCK = new Object();     @NotNull     private final RandomAccessFile raf;     private final FileChannel fileChannel;   public interface BytesStore extends RandomDataInput, RandomDataOutput, ReferencedCount, CharSequence   public interface Memory {     default long heapUsed() {         Runtime runtime = Runtime.getRuntime();         return runtime.totalMemory() - runtime.freeMemory();     }   @Override @ForceInline public void writeByte(long address, byte b) {     UNSAFE.putByte(address, b); }   /**     Marker annotation for some methods and constructors in the JSR 292 implementation.                To utilise this annotation se Chronicle Enterprise Warmup module.  */ @Target({ElementType.METHOD, ElementType.CONSTRUCTOR}) @Retention(RetentionPolicy.RUNTIME) public @interface ForceInline { }   Reference     https://github.com/OpenHFT/Chronicle-Queue   http://vanillajava.blogspot.com/2015/08/what-does-chronicle-software-do.html  ","categories": [],
        "tags": [],
        "url": "/2018/08/09/Chronicle.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Presto DB",
        "excerpt":"WHAT IS PRESTO?   Presto is an open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes.   Presto was designed and written from the ground up for interactive analytics and approaches the speed of commercial data warehouses while scaling to the size of organizations like Facebook.   Introduction   Presto is a distributed system that runs on a cluster of machines. A full installation includes a coordinator and multiple workers. Queries are submitted from a client such as the Presto CLI to the coordinator. The coordinator parses, analyzes and plans the query execution, then distributes the processing to the workers.   Presto does not use MapReduce and thus only requires HDFS.   Separation of Storage and Compute  Architected for separation of storage and compute, Presto can scale up and down based on your analytics demand to access this data. There’s no need to move your data and provisioning compute to the exact need results in significant cost savings.   Config Properties   The config properties file, etc/config.properties, contains the configuration for the Presto server. Every Presto server can function as both a coordinator and a worker, but dedicating a single machine to only perform coordination work provides the best performance on larger clusters.  Catalog Properties  Presto accesses data via connectors, which are mounted in catalogs. The connector provides all of the schemas and tables inside of the catalog. For example, the Hive connector maps each Hive database to a schema, so if the Hive connector is mounted as the hive catalog, and Hive contains a table clicks in database web, that table would be accessed in Presto as hive.web.clicks.   Catalogs are registered by creating a catalog properties file in the etc/catalog directory. For example, create etc/catalog/jmx.properties with the following contents to mount the jmx connector as the jmx catalog:   ","categories": [],
        "tags": [],
        "url": "/2018/08/15/PrestoDB.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Zoo-keeper",
        "excerpt":"ZK Motto     the motto “ZooKeeper: Because Coordinating Distributed Systems is a Zoo.”    Features of Zookeeper      Synchronization − Mutual exclusion and co-operation between server processes.   Ordered Messages - The strict ordering means that sophisticated synchronization primitives can be implemented at the client.   Reliability - The reliability aspects keep it from being a single point of failure.   Atomicity − Data transfer either succeeds or fails completely, but no transaction is partial.   High performant - The performance aspects of Zookeeper means it can be used in large, distributed systems.   Distributed.   High avaliablity.   Fault-tolerant.   Loose coupling.   Partial failure.   High throughput and low latency - data is stored data in memory and on disk as well.   Replicated.   Automatic failover: When a Zookeeper dies, the session is automatically migrated over to another Zookeeper.   Different type of data  When designing an application with ZooKeeper, one ideally separates application data from control or coordination data. For example, the users of a web-mail service are interested in their mailbox content, but not on which server is handling the requests of a particular mailbox. The mailbox content is application data, whereas the mapping of the mailbox to a specific mail server is part of the coordination data (or metadata). A ZooKeeper ensemble manages the latter.   The multiple processes consequently need to implement mutual exclusion. We can actually think of the task of acquiring mastership as the one of acquiring a lock: the process that acquires the mastership lock exercises the role of master.   Coordination does not always take the form of synchronization primitives like leader election or locks. Configuration metadata is often used as a way for a process to convey what others should be doing. For example, in a master-worker system, workers need to know the tasks that have been assigned to them, and this information must be available even if the master crashes.   How world work without ZooKeeper  It is certainly possible to build distributed systems without using ZooKeeper. ZooKeep‐ er, however, offers developers the possibility of focusing more on application logic rather than on arcane distributed systems concepts. Programming distributed systems without ZooKeeper is possible, but more difficult.   What does ZooKeeper does not do   The ensemble of ZooKeeper servers manages critical application data related to coor‐ dination. ZooKeeper is not for bulk storage. For bulk storage of application data, there are a number of options available, such as databases and distributed file systems. When designing an application with ZooKeeper, one ideally separates application data from control or coordination data.   ZooKeeper, however, does not implement the tasks for you. It does not elect a master or track live processes for the application out of the box. Instead, it provides the tools for implementing such tasks. The developer decides what coordination tasks to implement.   Processes in a distributed system have two broad options for communication: they can exchange messages directly through a network, or read and write to some shared storage. ZooKeeper uses the shared storage model to let applications implement coordination and synchronization primitives. But shared storage itself requires network communi‐ cation between the processes and the storage. It is important to stress the role of network communication because it is an important source of complications in the design of a distributed system.   This scenario leads to a problem commonly called split-brain: two or more parts of the system make progress independ‐ ently, leading to inconsistent behavior. As part of coming up with a way to cope with master failures, it is critical that we avoid split-brain scenarios.   Tasks  The following requirements for our master-worker architecture: Master election It is critical for progress to have a master available to assign tasks to workers. Crash detection The master must be able to detect when workers crash or disconnect. Group membership management The master must be able to figure out which workers are available to execute tasks. Metadata management The master and the workers must be able to store assignments and execution sta‐ tuses in a reliable manner.   CAP   known as CAP, which stands for Consistency, Availability, and Partition-tolerance, says that when designing a distributed system we may want all three of those properties, but that no system can handle all three.2 Zoo‐ Keeper has been designed with mostly consistency and availability in mind, although it also provides read-only capability in the presence of network partitions.   ZooKeeper Basics  Several primitives used for coordination are commonly shared across many applica‐ tions. Consequently, one way of designing a service used for coordination is to come up with a list of primitives, expose calls to create instances of each primitive, and ma‐ nipulate these instances directly. For example, we could say that distributed locks con‐ stitute an important primitive and expose calls to create, acquire, and release locks. Such a design, however, suffers from a couple of important shortcomings. First, we need to either come up with an exhaustive list of primitives used beforehand, or keep ex‐ tending the API to introduce new primitives. Second, it does not give flexibility to the application using the service to implement primitives in the way that is most suitable for it. We consequently have taken a different path with ZooKeeper. ZooKeeper does not ex‐ pose primitives directly. Instead, it exposes a file system-like API comprised of a small set of calls that enables applications to implement their own primitives. We typically use recipes to denote these implementations of primitives. Recipes include ZooKeeper operations that manipulate small data nodes, called znodes, that are organized hier‐ archically as a tree, just like in a file system.   znodes   a few other znodes that could be useful in a master- worker configuration:     The /workers znode is the parent znode to all znodes representing a worker avail‐ able in the system. Figure 2-1 shows that one worker (foo.com:2181) is available. If a worker becomes unavailable, its znode should be removed from /workers.   The /tasks znode is the parent of all tasks created and waiting for workers to execute them. Clients of the master-worker application add new znodes as children of /tasks to represent new tasks and wait for znodes representing the status of the task.   The /assign znode is the parent of all znodes representing an assignment of a task to a worker. When a master assigns a task to a worker, it adds a child znode to /assign.     API      API Overview Znodes may or may not contain data. If a znode contains any data, the data is stored as a byte array. The exact format of the byte array is specific to each application, and ZooKeeper does not directly provide support to parse it. Serialization packages such as Protocol Buffers, Thrift, Avro, and MessagePack may be handy for dealing with the format of the data stored in znodes, but sometimes string encodings such as UTF-8 or ASCII suffice.       The ZooKeeper API exposes the following operations:  create /path data Creates a znode named with /path and containing data delete /path Deletes the znode /path exists /path Checks whether /path exists setData /path data Sets the data of znode /path to data getData /path Returns the data in /path getChildren /path Returns the list of children under /path One important note is that ZooKeeper does not allow partial writes or reads of the znode data. When setting the data of a znode or reading it, the content of the znode is replaced or read entirely.   ZooKeeper clients connect to a ZooKeeper service and establish a session through which they make API calls.   If a worker becomes unavailable, its session expires and its znode in /workers disappears automatically.   An ephemeral znode can be deleted in two situations:     When the session of the client creator ends, either by expiration or because it ex‐ plicitly closed.   When a client, not necessarily the creator, deletes it.   Sequential znodes  A znode can also be set to be sequential. A sequential znode is assigned a unique, mo‐ notonically increasing integer. This sequence number is appended to the path used to create the znode. For example, if a client creates a sequential znode with the path /tasks/ task-, ZooKeeper assigns a sequence number, say 1, and appends it to the path. The path of the znode becomes /tasks/task-1. Sequential znodes provide an easy way to create znodes with unique names. They also provide a way to easily see the creation order of znodes.   watch  This is a common problem with polling. To replace the client polling, we have opted for a mechanism based on notifications: clients register with ZooKeeper to receive notifi‐ cations of changes to znodes. Registering to receive a notification for a given znode consists of setting a watch. A watch is a one-shot operation, which means that it triggers one notification. To receive multiple notifications over time, the client must set a new watch upon receiving each notification.   Versions  Each znode has a version number associated with it that is incremented every time its data changes. A couple of operations in the API can be executed conditionally: setDa ta and delete. Both calls take a version as an input parameter, and the operation suc‐ ceeds only if the version passed by the client matches the current version on the server. The use of versions is important when multiple ZooKeeper clients might be trying to perform operations over the same znode. For example, suppose that a client c1 writes a znode /config containing some configuration. If another client c2 concurrently updates the znode, the version c1 has is stale and the setData of c1 must not succeed. Using versions avoids such situations. In this case, the version that c1 uses when writing back doesn’t match and the operation fails.   ZooKeeper Architecture  Now that we have discussed at a high level the operations that ZooKeeper exposes to applications, we need to understand more of how the service actually works. Applica‐ tions make calls to ZooKeeper through a client library. The client library is responsible for the interaction with ZooKeeper servers.   ZooKeeper servers run in two modes: standalone and quorum. Standalone mode is pretty much what the term says: there is a single server, and ZooKeeper state is not replicated. In quorum mode, a group of ZooKeeper servers, which we call a ZooKeeper ensemble, replicates the state, and together they serve client requests. From this point on, we use the term “ZooKeeper ensemble” to denote an installation of servers. This installation could contain a single server and operate in standalone mode or contain a group of servers and operate in quorum mode.   ZooKeeper Quorums  In quorum mode, ZooKeeper replicates its data tree across all servers in the ensemble. But if a client had to wait for every server to store its data before continuing, the delays might be unacceptable. In public administration, a quorum is the minimum number of legislators required to be present for a vote. In ZooKeeper, it is the minimum number of servers that have to be running and available in order for ZooKeeper to work. This number is also the minimum number of servers that have to store a client’s data before telling the client it is safely stored. For instance, we might have five ZooKeeper servers in total, but a quorum of three. So long as any three servers have stored the data, the client can continue, and the other two servers will eventually catch up and store the data.   It is important to choose an adequate size for the quorum. Quorums must guarantee that, regardless of delays and crashes in the system, any update request the service pos‐ itively acknowledges will persist until another request supersedes it.   The bottom line is that we should always shoot for an odd number of servers.   sessions   All operations a client submits to ZooKeeper are associated to a session. When a session ends for any reason, the ephemeral nodes created during that session disappear.   the session may be moved to a different server if the client has not heard from its current server for some time. Moving a session to a different server is handled transparently by the ZooKeeper client library.   Sessions offer order guarantees, which means that requests in a session are executed in FIFO (first in, first out) order. Typically, a client has only a single session open, so its requests are all executed in FIFO order. If a client has multiple concurrent sessions, FIFO ordering is not necessarily preserved across the sessions.   Commands  [zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 5] stat /master cZxid = 0x4 ctime = Mon Aug 20 21:10:23 AEST 2018 mZxid = 0x4 mtime = Mon Aug 20 21:10:23 AEST 2018 pZxid = 0x4 cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x10003c70e250001 dataLength = 11 numChildren = 0   [zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 8] create /workers \"\" Created /workers [zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 9] create /tasks \"\" Created /tasks [zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 10] create /assign \"\" Created /assign [zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 11] ls / [assign, master, tasks, workers, zookeeper]   link master and workers   In a real application, these znodes need to be created either by a primary process before it starts assigning tasks or by some bootstrap procedure. Regardless of how they are created, once they exist, the master needs to watch for changes in the children of /workers and /tasks:     [zk: localhost:2181(CONNECTED) 4] ls /workers true     []     [zk: localhost:2181(CONNECTED) 5] ls /tasks true     []     [zk: localhost:2181(CONNECTED) 6] Note that we have used the optional true parameter with ls, as we did before with stat on the master. The true parameter, in this case, creates a watch for changes to the set of children of the corresponding znode.   [zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 14] create -e /workers/todd-worker1 \"\" Created /workers/todd-worker1  WATCHER::  WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/workers [zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 15]  Recall that the master has set a watch for changes to the children of /workers. Once the worker creates a znode under /workers, the master observes the following notification:     WATCHER::     WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/workers   Tasks workflows     Clients add tasks to the system. Here we assume that the client asks the master-worker system to run a command cmd. To add a task to the system, a client executes the following:   [zk: localhost:2181(CONNECTED) 0] create -s /tasks/task- “cmd”   Created /tasks/task-0000000000   The client now has to wait until the task is executed.   The worker that executes the task creates a status znode for the task once the task completes.   The client determines that the task has been executed when it sees that a status znode for the task has been created;   the client consequently must watch for the creation of the status znode:   [zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 18] create -s /tasks/task- \"cmd\" Created /tasks/task-0000000000 [zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 19] ls /tasks [task-0000000000] [zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 20] ls -w /tasks/task-0000000000  [] [zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 21] ls -w /workers [todd-worker1] [zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 22] create /assign/todd-worker1/task-0000000000 \"\" Ephemerals cannot have children: /assign/todd-worker1/task-0000000000 [zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 23] delete /assign/todd-worker1 [zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 24] create /assign/todd-worker1 Created /assign/todd-worker1 [zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 25] create /assign/todd-worker1/task-0000000000 \"\" Created /assign/todd-worker1/task-0000000000 [zk: 127.0.0.1:2181,127.0.0.1:2182(CONNECTED) 26] ls /assign/todd-worker1 [task-0000000000]    # Once the worker finishes executing the task, it adds a status znode to /tasks:     [zk: localhost:2181(CONNECTED) 4] create /tasks/task-0000000000/status \"done\"     Created /tasks/task-0000000000/status     [zk: localhost:2181(CONNECTED) 5] # and the client receives a notification and checks the result: WATCHER::     WatchedEvent state:SyncConnected type:NodeChildrenChanged     path:/tasks/task-0000000000     [zk: localhost:2181(CONNECTED) 2] get /tasks/task-0000000000     \"cmd\"   ZooKeeper API   Setting the ZooKeeper CLASSPATH   ZOOBINDIR=”/bin\"     . \"$ZOOBINDIR\"/zkEnv.sh   handle  The ZooKeeper API is built around a ZooKeeper handle that is passed to every API call. This handle represents a session with ZooKeeper. A session that is established with one ZooKeeper server will migrate to another ZooKeeper server if its connection is broken. As long as the session is alive, the handle will remain valid, and the ZooKeeper client library will continually try to keep an active connection to a ZooKeeper server to keep the session alive. If the handle is closed, the ZooKeeper client library will tell the ZooKeeper servers to kill the session. If ZooKeeper decides that a client has died, it will invalidate the session. If a client later tries to reconnect to a Zoo‐ Keeper server using the handle that corresponds to the invalidated session, the Zoo‐ Keeper server informs the client library that the session is no longer valid and the handle returns errors for all operations.   The constructor that creates a ZooKeeper handle usually looks like: ZooKeeper( String connectString, int sessionTimeout, Watcher watcher)  Implementing a Watcher  To receive notifications from ZooKeeper, we need to implement watchers. Let’s look a bit more closely at the Watcher interface. It has the following declaration: public interface Watcher { void process(WatchedEvent event); }   Sample ZooKeeper handle  import org.apache.zookeeper.ZooKeeper;  import org.apache.zookeeper.Watcher;  public class Master implements Watcher {    ZooKeeper zk;         String hostPort;  Master(String hostPort)  { this.hostPort = hostPort; } void startZK() { zk = new ZooKeeper(hostPort, 15000, this); } public void process(WatchedEvent e) { System.out.println(e); } public static void main(String args[]) throws Exception { Master m = new Master(args[0]);             m.startZK();             // wait for a bit             Thread.sleep(60000);         } }   nce we have connected to ZooKeeper, there will be a background thread that will maintain the ZooKeeper session. This thread is a daemon thread, which means that the program may exit even if the thread is still active. Here we sleep for a bit so that we can see some events come in before the program exits. We can compile this simple example using the following: $ javac -cp $CLASSPATH Master.java Once we have compiled Master.java, we run it and see the following: $ java -cp $CLASSPATH Master 127.0.0.1:2181   disconnect   When developers see the Disconnected event, some think they need to create a new ZooKeeper handle to reconnect to the service. Do not do that! See what happens when you start the server, start the Master, and then stop and start the server while the Master is still running. You should see the SyncConnected event followed by the Disconnec ted event and then another SyncConnected event. The ZooKeeper client library takes care of reconnecting to the service for you. Unfortunately, network outages and server failures happen. Usually, ZooKeeper can deal with these failures.  ","categories": [],
        "tags": [],
        "url": "/2018/08/19/ZooKeeper.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Sudo in a Nutshell",
        "excerpt":"Sudo in a Nutshell   Sudo (su “do”) allows a system administrator to give certain users (or groups of users) the ability to run some (or all) commands as root while logging all commands and arguments. Sudo operates on a per-command basis, it is not a replacement for the shell. Its features include:   The ability to restrict what commands a user may run on a per-host basis. Sudo does copious logging of each command, providing a clear audit trail of who did what. When used in tandem with syslogd, the system log daemon, sudo can log all commands to a central host (as well as on the local host). At CU, all admins use sudo in lieu of a root shell to take advantage of this logging. Sudo uses timestamp files to implement a “ticketing” system. When a user invokes sudo and enters their password, they are granted a ticket for 5 minutes (this timeout is configurable at compile-time). Each subsequent sudo command updates the ticket for another 5 minutes. This avoids the problem of leaving a root shell where others can physically get to your keyboard. There is also an easy way for a user to remove their ticket file, useful for placing in a .logout file. Sudo’s configuration file, the sudoers file, is setup in such a way that the same sudoers file may be used on many machines. This allows for central administration while keeping the flexibility to define a user’s privileges on a per-host basis. Please see the samples sudoers file below for a real-world example.   sudo.conf  The sudo.conf file is used to configure the sudo front end. It specifies the security policy and I/O logging plugins, debug flags as well as plugin-agnostic path names and settings.   sudo supports a plugin architecture for security policies and input/output logging. Third parties can develop and distribute their own policy and I/O logging plugins to work seamlessly with the sudo front end. Plugins are dynamically loaded based on the contents of sudo.conf. A Plugin line consists of the Plugin keyword, followed by the symbol_name and the path to the dynamic shared object that contains the plugin. The symbol_name is the name of the struct policy_plugin or struct io_plugin symbol contained in the plugin. The path may be fully qualified or relative. If not fully qualified, it is relative to the directory specified by the plugin_dir Path setting, which defaults to /usr/local/libexec/sudo. In other words: Plugin sudoers_policy sudoers.so is equivalent to: Plugin sudoers_policy /usr/local/libexec/sudo/sudoers.so   Configurations          sudoers_file=pathname The sudoers_file argument can be used to override the default path to the sudoers file.            sudoers_uid=uid The sudoers_uid argument can be used to override the default owner of the sudoers file. It should be specified as a numeric user ID.       email notification  If a user who is not listed in the policy tries to run a command via sudo, mail is sent to the proper authorities. The address used for such mail is configurable via the mailto Defaults entry (described later) and defaults to root.   Note that no mail will be sent if an unauthorized user tries to run sudo with the -l or -v option unless there is an authentication error and either the mail_always or mail_badpass flags are enabled. This allows users to determine for themselves whether or not they are allowed to use sudo. All attempts to run sudo (successful or not) will be logged, regardless of whether or not mail is sent.   sudoers uses per-user time stamp files for credential caching. Once a user has been authenticated, a record is written containing the user ID that was used to authenticate, the terminal session ID, the start time of the session leader (or parent process) and a time stamp (using a monotonic clock if one is available). The user may then use sudo without a password for a short period of time (5 minutes unless overridden by the timestamp_timeout option). By default, sudoers uses a separate record for each terminal, which means that a user’s login sessions are authenticated separately. The timestamp_type option can be used to select the type of time stamp record sudoers will use.   File format  The sudoers file is composed of two types of entries: aliases (basically variables) and user specifications (which specify who may run what).   When multiple entries match for a user, they are applied in order. Where there are multiple matches, the last match is used (which is not necessarily the most specific match). The sudoers file grammar will be described below in Extended Backus-Naur Form (EBNF). Don’t despair if you are unfamiliar with EBNF; it is fairly simple, and the definitions below are annotated.   environment   By default, the env_reset option is enabled. This causes commands to be executed with a new, minimal environment.   Lists have two additional assignment operators, += and -=. These operators are used to add to and delete from a list respectively. It is not an error to use the -= operator to remove an element that does not exist in a list.   35 Defaults    env_reset  36 Defaults    env_keep += \"BLOCKSIZE\"  37 Defaults    env_keep += \"COLORFGBG COLORTERM\"  38 Defaults    env_keep += \"__CF_USER_TEXT_ENCODING\"  39 Defaults    env_keep += \"CHARSET LANG LANGUAGE LC_ALL LC_COLLATE LC_CTYPE\"  40 Defaults    env_keep += \"LC_MESSAGES LC_MONETARY LC_NUMERIC LC_TIME\"  41 Defaults    env_keep += \"LINES COLUMNS\"  42 Defaults    env_keep += \"LSCOLORS\"  43 Defaults    env_keep += \"SSH_AUTH_SOCK\"  44 Defaults    env_keep += \"TZ\"  45 Defaults    env_keep += \"DISPLAY XAUTHORIZATION XAUTHORITY\"  46 Defaults    env_keep += \"EDITOR VISUAL\"  47 Defaults    env_keep += \"HOME MAIL\"  48   49 Defaults    lecture_file = \"/etc/sudo_lecture\"   lecture  lecture  This option controls when a short lecture will be printed along with the password prompt. It has the following possible values:     always Always lecture the user.   never Never lecture the user.   once Only lecture the user the first time they run sudo. If no value is specified, a value of once is implied. Negating the option results in a value of never being used. The default value is once.     lecture_file      Path to a file containing an alternate sudo lecture that will be used in place of the standard lecture if the named file exists. By default, sudo uses a built-in lecture.       Everything is file   A fundamental and very powerful, consistent abstraction provided in UNIX and compatible operating systems is the file abstraction. Many OS services and device interfaces are implemented to provide a file or file system metaphor to applications.   manage user group  # Alternatively, gpasswd may be used. Though the username can only be added (or removed) from one group at a time: gpasswd --add username group  # Add users to a group with the gpasswd command: gpasswd -a user group  #To remove users from a group: gpasswd -d user group    gpasswd - administer the /etc/group file   EXAMPLES 1. Add user (tracy) to the group (hrd) $ gpasswd -a tracy hrd 2. Add multiper users to the group (developer) $ gpasswd -a pavan,john developer 3. Remove user (rakesh) from group (sqa) $ gpasswd -d rakesh sqa 4. Remove multiple users from group (managers) $ gpasswd -d shane,ron,ram managers 5. Set user (joy) and group administrator for (managers) $ gpasswd -A joy managers   to show user details  id todzhang  Display group membership with the groups command:  $ groups user   To change the user’s login shell:  # usermod -s /bin/bash username   reference     https://www.sudo.ws/man/sudo.conf.man.html  ","categories": [],
        "tags": [],
        "url": "/2018/08/21/sudo.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Protobuf",
        "excerpt":"What are protocol buffers?   Protocol buffers are a flexible, efficient, automated mechanism for serializing structured data – think XML, but smaller, faster, and simpler.   You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages. You can even update your data structure without breaking deployed programs that are compiled against the “old” format.   How do they work?   You specify how you want the information you’re serializing to be structured by defining protocol buffer message types in .proto files. Each protocol buffer message is a small logical record of information, containing a series of name-value pairs.   message Person {   required string name = 1;   required int32 id = 2;   optional string email = 3;    enum PhoneType {     MOBILE = 0;     HOME = 1;     WORK = 2;   }    message PhoneNumber {     required string number = 1;     optional PhoneType type = 2 [default = HOME];   }    repeated PhoneNumber phone = 4; }   As you can see, the message format is simple – each message type has one or more uniquely numbered fields, and each field has a name and a value type, where value types can be numbers (integer or floating-point), booleans, strings, raw bytes, or even (as in the example above) other protocol buffer message types, allowing you to structure your data hierarchically. You can specify optional fields, required fields, and repeated fields.   Why not just use XML?   Protocol buffers have many advantages over XML for serializing structured data. Protocol buffers:     are simpler   are 3 to 10 times smaller   are 20 to 100 times faster   are less ambiguous   generate data access classes that are easier to use programmatically   Assigning Field Numbers   As you can see, each field in the message definition has a unique number. These numbers are used to identify your fields in the message binary format, and should not be changed once your message type is in use. Note that field numbers in the range 1 through 15 take one byte to encode, including the field number and the field’s type (you can find out more about this in Protocol Buffer Encoding). Field numbers in the range 16 through 2047 take two bytes. So you should reserve the field numbers 1 through 15 for very frequently occurring message elements. Remember to leave some room for frequently occurring elements that might be added in the future.   Field Rules   You specify that message fields are one of the following:      required: a well-formed message must have exactly one of this field.   optional: a well-formed message can have zero or one of this field (but not more than one).   repeated: this field can be repeated any number of times (including zero) in a well-formed message. The order of the repeated values will be preserved.      Some engineers at Google have come to the conclusion that using required does more harm than good; they prefer to use only optional and repeated. However, this view is not universal.    Combining Messages leads to bloat While multiple message types (such as message, enum, and service) can be defined in a single .proto file, it can also lead to dependency bloat when large numbers of messages with varying dependencies are defined in a single file. It’s recommended to include as few message types per .proto file as possible.   Reserved fields  to make sure this doesn’t happen is to specify that the field numbers (and/or names, which can also cause issues for JSON serialization) of your deleted fields are reserved. The protocol buffer compiler will complain if any future users try to use these field identifiers.  message Foo {   reserved 2, 15, 9 to 11;   reserved \"foo\", \"bar\"; }  Note that you can’t mix field names and field numbers in the same reserved statement.   Optional fields and default  optional int32 result_per_page = 3 [default = 10];   Importing Definitions   You can use definitions from other .proto files by importing them. To import another .proto’s definitions, you add an import statement to the top of your file:  import \"myproject/other_protos.proto\";   To generate class  ../protoc-3/bin/protoc --java_out=./  ticket.proto   Builders vs. Messages   The message classes generated by the protocol buffer compiler are all immutable. Once a message object is constructed, it cannot be modified, just like a Java String. To construct a message, you must first construct a builder, set any fields you want to set to your chosen values, then call the builder’s build() method.   You may have noticed that each method of the builder which modifies the message returns another builder. The returned object is actually the same builder on which you called the method. It is returned for convenience so that you can string several setters together on a single line of code.   Here’s an example of how you would create an instance of Person:  Person john =   Person.newBuilder()     .setId(1234)     .setName(\"John Doe\")     .setEmail(\"jdoe@example.com\")     .addPhones(       Person.PhoneNumber.newBuilder()         .setNumber(\"555-4321\")         .setType(Person.PhoneType.HOME))     .build();   Parsing and Serialization   Finally, each protocol buffer class has methods for writing and reading messages of your chosen type using the protocol buffer binary format. These include:      byte[] toByteArray();: serializes the message and returns a byte array containing its raw bytes.   static Person parseFrom(byte[] data);: parses a message from the given byte array.   void writeTo(OutputStream output);: serializes the message and writes it to an OutputStream.   pstatic Person parseFrom(InputStream input);: reads and parses a message from an InputStream.   Write a message  // Write the new address book back to disk.     FileOutputStream output = new FileOutputStream(args[0]);     addressBook.build().writeTo(output);   Reading A Message  Use message’s parseFrom method on stream:       AddressBook.parseFrom(new FileInputStream(args[0]));    import com.example.tutorial.AddressBookProtos.AddressBook; import com.example.tutorial.AddressBookProtos.Person; import java.io.FileInputStream; import java.io.IOException; import java.io.PrintStream;  class ListPeople {   // Iterates though all people in the AddressBook and prints info about them.   static void Print(AddressBook addressBook) {     for (Person person: addressBook.getPeopleList()) {       System.out.println(\"Person ID: \" + person.getId());       System.out.println(\"  Name: \" + person.getName());       if (person.hasEmail()) {         System.out.println(\"  E-mail address: \" + person.getEmail());       }        for (Person.PhoneNumber phoneNumber : person.getPhonesList()) {         switch (phoneNumber.getType()) {           case MOBILE:             System.out.print(\"  Mobile phone #: \");             break;           case HOME:             System.out.print(\"  Home phone #: \");             break;           case WORK:             System.out.print(\"  Work phone #: \");             break;         }         System.out.println(phoneNumber.getNumber());       }     }   }    // Main function:  Reads the entire address book from a file and prints all   //   the information inside.   public static void main(String[] args) throws Exception {     if (args.length != 1) {       System.err.println(\"Usage:  ListPeople ADDRESS_BOOK_FILE\");       System.exit(-1);     }      // Read the existing address book.     AddressBook addressBook =       AddressBook.parseFrom(new FileInputStream(args[0]));      Print(addressBook);   } }   References     https://developers.google.com/protocol-buffers/docs/overview  ","categories": [],
        "tags": [],
        "url": "/2018/08/23/protobuf.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Mockito",
        "excerpt":"Feature   There only 2 things you can do with Mockito mocks - verify or stub. Stubbing goes before execution and verification afterwards.   References     https://github.com/mockito/mockito/wiki/Mockito-vs-EasyMock  ","categories": [],
        "tags": [],
        "url": "/2018/09/03/distruptor.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Distruptor",
        "excerpt":"multithreading   Concurrent execution of code ia bout two things: mutal exclusion and visibility of change.      Mutual exclusion is about managing contented updtes to some resources.   Visibiliyt of change is about controlling when such changes are made visible to other threads.   mutal exclusion  It is possible to avoid the need for mutal exclusion if you can eliminate the need for contented updates. If your algorithm can guarantee that any given resource is modified by only one thread then utal exclusion is unnecessary.   Read and write operations require that all changes are made visible to other threads.   The most costly operation in any concurrent environment is a contended write access.   locks  Lock provide mutual exclusion and ensure that the visibility of change occurs in an ordered manner. Locks are incredibly expensive because they require arbitration when contended. This arbitration is achieved by a context switch to the OS kernerl which will suspend threads waiting on a lock until it’s released. During such a context switch , as well as releasing control to the OS which may decided to do other house-keeping tasks which it has control, execution context can lose previously cached data nad instrucionts. This can have a serious performance impact on modern CPU.  References     https://developers.google.com/protocol-buffers/docs/overview  ","categories": [],
        "tags": [],
        "url": "/2018/09/06/mockito.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "YAML",
        "excerpt":"Key points  All YAML files (regardless of their association with Ansible or not) can optionally begin with — and end with …. This is part of the YAML format and indicates the start and end of a document.   In a way, YAML is to JSON what Markdown is to HTML. #   References     https://developers.google.com/protocol-buffers/docs/overview  ","categories": [],
        "tags": [],
        "url": "/2018/09/16/yaml-config.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Guice",
        "excerpt":"A new type of Juice  Put simply, Guice alleviates the need for factories and the use of new in your Java code. Think of Guice’s @Inject as the new new. You will still need to write factories in some cases, but your code will not depend directly on them. Your code will be easier to change, unit test and reuse in other contexts.   Guice embraces Java’s type safe nature, especially when it comes to features introduced in Java 5 such as generics and annotations. You might think of Guice as filling in missing features for core Java. Ideally, the language itself would provide most of the same features, but until such a language comes along, we have Guice.   Guice helps you design better APIs, and the Guice API itself sets a good example. Guice is not a kitchen sink. We justify each feature with at least three use cases. When in doubt, we leave it out. We build general functionality which enables you to extend Guice rather than adding every feature to the core framework.   Guice vs Spring   Spring and Google Guice are two powerful dependency injection frameworks in use today. Both frameworks fully embrace the concepts of dependency injection, but each has its own way of implementing them. Although Spring provides many benefits, it was created in a pre-Java-5 world. The Guice framework takes DI to the next level.   The advent of Java 5 brought significant changes to the language like generics and annotations: features that enhance the power of Java static typing. Guice is a DI framework that was built from the ground up with the intent to take full advantage of these new features and that has focused on one primary goal: to do dependency injection well.   Guice aims to make development and debugging easier and faster, not harder and slower. In that vein, Guice steers clear of surprises and magic. You should be able to understand code with or without tools, though tools can make things even easier. When errors do occur, Guice goes the extra mile to generate helpful messages.   core differences between the two, and see why I prefer to use Guice.      Living in XML Hell   Eliminating reliance on String identifiers   Preferring Constructor Injection   Although Spring and Guice both support constructor and setter injection, each framework has a preference. Spring has long favored setter injection. Back in the early days of Spring, the authors believed the lack of argument names and default arguments in constructor injection reduced clarity. In addition, constructor injection makes it difficult to have optional dependencies, requires dependencies to be configured in a specific order, and forces subclasses to deal with superclass dependencies. Using setter injection eliminates these problems, and so Spring favors that approach.   The Guice authors saw difficulties with setter injection. One problem is immutability: it is impossible to make immutable a class that uses setter injection. Constructor injection, on the other hand, makes the creation of immutable classes easy, an important consideration in writing multi-threaded applications. In addition, optional dependencies, while perhaps convenient, can introduce confusion about how a class is configured at runtime. Configuring a class through setter injection can often lead to missed required dependencies. Though Spring does provide a @Required annotation to solve this problem, using constructor injection eliminates it by default.   Constructor injection also makes a class’s dependencies immediately clear at a glance. If you’re writing or modifying a unit test, it’s easy to read what the system-under-test needs. Lastly, because Guice uses types to wire classes together, constructor argument order isn’t an issue. You can feel free to reorder them how you want without needing to modify configuration code at all.   The potential drawbacks posed by setter injection outweigh the benefits in many common scenarios, and so Guice established a best practice of favoring constructor injection instead. Its API is well-suited to that approach. But if you should choose to switch from one form of injection to the other, Guice makes that easy too. Changing from setter to constructor injection or vice versa is simply a matter of modifying the class in question. Unlike in Spring, you need never touch a configuration file.      Nullifying NullPointerExceptions   Null is easily one of the most non-communicative return values possible from a method call. Guice hates nulls as much as I do. By default, Guice refuses to inject a null into any object, and if an accidental null shows up during wiring, it fails-fast with a ProvisionException. Guice does allow for the exception case by permitting fields to be annotated with @Nullable, but this is discouraged.      Intruding into the domain   Guice aims to eliminate all of this boilerplate without sacrificing maintainability. With Guice, you implement modules. Guice passes a binder to your module, and your module uses the binder to map interfaces to implementations. The following module tells Guice to map Service to ServiceImpl in singleton scope:         public class MyModule implements Module {          public void configure(Binder binder) {           binder.bind(Service.class)            .to(ServiceImpl.class)            .in(Scopes.SINGLETON); } }   A module tells Guice what we want to inject. Now, how do we tell Guice where we want it injected? With Guice, you annotate constructors, methods and fields with @Inject.         public class Client {          private final Service service; @Inject          public Client(Service service) {            this.service = service; }          public void go() {            service.go(); } }   Guice vs. Dependency Injection By Hand  As you can see, Guice saves you from having to write factory classes. You don’t have to write explicit code wiring clients to their dependencies. If you forget to provide a dependency, Guice fails at startup. Guice handles circular dependencies automatically. Guice enables you to specify scopes declaratively. For example, you don’t have to write the same code to store an object in the HttpSession over and over. In the real world, you often don’t know an implementation class until runtime. You need meta factories or service locators for your factories. Guice addresses these problems with minimal effort. When injecting dependencies by hand, you can easily slip back into old habits and introduce direct dependencies, especially if you’re new to the concept of dependency injection. Using Guice turns the tables and makes doing the right thing easier. Guice helps keep you on track.   Guice annotations  When possible, Guice enables you to use annotations in lieu of explicit bindings and eliminate even more boilerplate code. Back to our example, if you need an interface to simplify unit testing but you don’t care about compile time dependencies, you can point to a default implementation directly from your interface.         @ImplementedBy(ServiceImpl.class)        public interface Service {          void go(); }   If a client needs a Service and Guice can’t find an explicit binding, Guice will inject an instance of ServiceImpl.   By default, Guice injects a new instance every time. If you want to specify a different scope, you can annotate the implementation class, too.  @Singleton        public class ServiceImpl implements Service {          public void go() { ... } }   Architectural Overview  We can break Guice’s architecture down into two distinct stages: startup and runtime. You build an Injector during startup and use it to inject objects at runtime.   Startup  You configure Guice by implementing Module. You pass Guice a module, Guice passes your module a Binder, and your module uses the binder to configure bindings. A binding most commonly consists of a mapping between an interface and a concrete implementation. For example:         public class MyModule implements Module {          public void configure(Binder binder) {           // Bind Foo to FooImpl. Guice will create a new            // instance of FooImpl for every injection.            binder.bind(Foo.class).to(FooImpl.class);            // Bind Bar to an instance of Bar.            Bar bar = new Bar();            binder.bind(Bar.class).toInstance(bar); }   Injecting Providers  With normal dependency injection, each type gets exactly one instance of each of its dependent types. The RealBillingService gets one CreditCardProcessor and one TransactionLog. Sometimes you want more than one instance of your dependent types. When this flexibility is necessary, Guice binds a provider. Providers produce a value when the get() method is invoked:  public interface Provider&lt;T&gt; {   T get(); }   @Provides Methods  When you need code to create an object, use an @Provides method. The method must be defined within a module, and it must have an @Provides annotation. The method’s return type is the bound type. Whenever the injector needs an instance of that type, it will invoke the method.  public class BillingModule extends AbstractModule {   @Override   protected void configure() {     ...   }    @Provides   TransactionLog provideTransactionLog() {     DatabaseTransactionLog transactionLog = new DatabaseTransactionLog();     transactionLog.setJdbcUrl(\"jdbc:mysql://localhost/pizza\");     transactionLog.setThreadPoolSize(30);     return transactionLog;   } }  If the @Provides method has a binding annotation like @PayPal or @Named(“Checkout”), Guice binds the annotated type. Dependencies can be passed in as parameters to the method. The injector will exercise the bindings for each of these before invoking the method.    @Provides @PayPal   CreditCardProcessor providePayPalCreditCardProcessor(       @Named(\"PayPal API key\") String apiKey) {     PayPalCreditCardProcessor processor = new PayPalCreditCardProcessor();     processor.setApiKey(apiKey);     return processor;   }   References  ","categories": [],
        "tags": [],
        "url": "/2018/09/18/Google-Guice.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Citrix receiver",
        "excerpt":"“Cannot connect to remote desktop” with Citrix Receiver   cd /opt/Citrix/ICAClient/keystore/ then: rm -rf cacerts and finally: ln -s /etc/ssl/certs cacerts   Cann’t show full sreen in linux citrix receiver  There is workaround, i.e. Press Alt and drag RDP window, then maximum it.  ","categories": [],
        "tags": [],
        "url": "/2018/09/26/Citrix.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Citrix receiver",
        "excerpt":"Simple Binary Encoding (SBE)   SBE is an OSI layer 6 presentation for encoding and decoding binary application messages for low-latency financial applications.   ","categories": [],
        "tags": [],
        "url": "/2018/10/13/SBE.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Seconds",
        "excerpt":"nano seconds   ns: 1/1,000,000,000 second, i.e. 10(-9) seconds 1 ns = 1/1,000 micro second 1 ns = 1/1,000,000 milis second   used in telecommunications   micro seconds  Its symbol is μs. 微秒 1 μs = 1000 ns 1 μs = 1/1,000 milli Seconds   8.01 μs： light took the time to travel 1 mile in vaccum   The average human eye blink takes 350,000 microseconds (just over 1/3 of one second). The average human finger snap takes 150,000 microseconds (just over 1/7 of one second). A camera flash illuminates for 1000 microseconds.   milli second  ms 毫秒 1 ms = 1/1,000 second 1 ms = 1,000 μs = 1,000,000 ns   3 ms: fly flgp its wing 5 ms: bee flap wing 300-400 ms: human eye to blink   ","categories": [],
        "tags": [],
        "url": "/2018/11/01/seconds.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Inter Processes Communication",
        "excerpt":"IPC   interprocess communication (IPC)   Posted by: Margaret Rouse WhatIs.com   Interprocess communication (IPC) is a set of programming interfaces that allow a programmer to coordinate activities among different program processes that can run concurrently in an operating system. This allows a program to handle many user requests at the same time. Since even a single user request may result in multiple processes running in the operating system on the user’s behalf, the processes need to communicate with each other. The IPC interfaces make this possible. Each IPC method has its own advantages and limitations so it is not unusual for a single program to use all of the IPC methods.   Inter process communication (IPC) is a mechanism which allows processes to communicate each other and synchronize their actions. The communication between these processes can be seen as a method of co-operation between them. Processes can communicate with each other using these two ways: Shared Memory Message passing  ","categories": [],
        "tags": [],
        "url": "/2019/01/03/IPC.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Foreign Exchange",
        "excerpt":"currency pairs     Direct ccy: means USD is part of currency pair   Cross ccy: means ccy wihtout USD, so except NDF, the deal will be split to legs, both with USD. e.g. EUR/GBP will split to EURUSD and USDGBP   non-convention ccy pair: that’s depends on where you sit on. e.g. for Aussie traders, they would trade AUDNZD, but for kiwi traders, they would trade for NZDAUD, then AUDNZD would be non-convention  ","categories": [],
        "tags": [],
        "url": "/2019/01/17/FX-ForeignExchange.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Mifid",
        "excerpt":"FX Spot is not covered by the regulation, as it is not considered to be a financial instrument by ESMA, the European Union (EU) regulator. As FX is considered “illiquid” it does not have pre-trade reporting requirements.   Recordkeeping – MiFID II requires firms to keep extensive records of all transactions, communications, services and activities for 10 years, in order for them to be able to provide transparency into the trade life cycle. This is to support trade reconstruction if required.   Overall there are 3 points in the text which we think will be of particular interest to foreign exchange brokers.   The EC has determined that FX Forward contracts remain outside the scope of MiFID II if they satisfy all of the following conditions: The contract for deliverable FX is physically settled At least one of the parties to the contract is a non-financial counterparty The purpose of the contract is to facilitate payment for identifiable goods, services or direct investment The contract is not traded on a trading venue    FX Forwards will qualify for the means of payment exclusion if they meet the following criteria:           The counterparty is a corporate entity (a non-financial counterparty (‘NFC’) as defined under EMIR);            The FX forwards are traded for the purpose of facilitating payment for identifiable goods or services (for example, entering into an FX forward in order to pay an upcoming invoice in a foreign currency, or in preparation of an upcoming purchase in a foreign currency, as opposed to trading FX forwards for speculative purposes); and            The FX forwards are traded bilaterally, as opposed to on a regulated trading venue (note that Agile Markets is not a regulated trading venue and does not affect eligibility);       The Financial Conduct Authority has provided some examples of scenarios that would fit within the exclusion.  Please find the examples provided on the link here.  ","categories": [],
        "tags": ["MTF","Mifid"],
        "url": "/2019/01/17/Mifid.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Kafka",
        "excerpt":"Kafka   Kafka is fast. A single node can handle hundreds of read/writes from thousands of clients in real time. Kafka is also distributed and scalable. It creates and takes down nodes in an elastic manner, without incurring any downtime. Data streams are split into partitions and spread over different brokers for capability and redundancy.   History of Kafka  The result was a publish/subscribe messaging system that had an interface typical of messaging systems but a storage layer more like a log-aggregation system. Combined with the adoption of Apache Avro for message serialization, Kafka was effective for handling both metrics and user-activity tracking at a scale of billions of messages per day.   Kafka features           Language Agnostic Producers and consumers use binary protocol to talk to a Kafka cluster.            Durability Kafka does not track which messages were read by each consumer. Kafka keeps all messages for a finite amount of time, and it is consumers’ responsibility to track their location per topic, i.e. offsets.       Terminology:   Topic: a feed of messages or packages Partition: group of topics split for scalability and redundancy Producer: process that introduces messages into the queue Consumer: process that subscribes to various topics and processes from a feed of published messages Broker: a node that is part of the Kafka cluster   Topics and Partitions  Messages in Kafka are categorized into topics. The closest analogies for a topic are a database table or a folder in a filesystem. Topics are additionally broken down into a number of partitions. Going back to the “commit log” description, a partition is a sin‐ gle log. Messages are written to it in an append-only fashion, and are read in order from beginning to end. Note that as a topic typically has multiple partitions, there is no guarantee of message time-ordering across the entire topic, just within a single partition.   Partitions are also the way that Kafka provides redundancy and scalability. Each partition can be hosted on a different server, which means that a single topic can be scaled horizontally across multiple servers to provide performance far beyond the ability of a single server.   Producers and consumers  Kafka clients are users of the system, and there are two basic types: producers and consumers. There are also advanced client APIs—Kafka Connect API for data inte‐ gration and Kafka Streams for stream processing. The advanced clients use producers and consumers as building blocks and provide higher-level functionality on top.   producers  Producers create new messages. In other publish/subscribe systems, these may be called publishers or writers. In general, a message will be produced to a specific topic. By default, the producer does not care what partition a specific message is written to and will balance messages over all partitions of a topic evenly. In some cases, the pro‐ ducer will direct messages to specific partitions. This is typically done using the mes‐ sage key and a partitioner that will generate a hash of the key and map it to a specific partition. This assures that all messages produced with a given key will get written to the same partition. The producer could also use a custom partitioner that follows other business rules for mapping messages to partitions.   Consumers  Consumers read messages. In other publish/subscribe systems, these clients may be called subscribers or readers. The consumer subscribes to one or more topics and reads the messages in the order in which they were produced. The consumer keeps track of which messages it has already consumed by keeping track of the offset of messages. The offset is another bit of metadata—an integer value that continually increases—that Kafka adds to each message as it is produced. Each message in a given partition has a unique offset. By storing the offset of the last consumed message for each partition, either in Zookeeper or in Kafka itself, a consumer can stop and restart without losing its place.   Consumers work as part of a consumer group, which is one or more consumers that work together to consume a topic. The group assures that each partition is only con‐ sumed by one member. there are three consumers in a single group consuming a topic. Two of the consumers are working from one partition each, while the third consumer is working from two partitions. The mapping of a consumer to a partition is often called ownership of the partition by the consumer.   Consumer group  Consumers may be grouped in a consumer group with multiple consumers. Each consumer in a consumer group will read messages from a unique subset of partitions in each topic they subscribe to. Each message is delivered to one consumer in the group, and all messages with the same key arrive at the same consumer.   Brokers and Clusters  A single Kafka server is called a broker. The broker receives messages from producers, assigns offsets to them, and commits the messages to storage on disk. It also services consumers, responding to fetch requests for partitions and responding with the mes‐ sages that have been committed to disk. Depending on the specific hardware and its performance characteristics, a single broker can easily handle thousands of partitions and millions of messages per second. Kafka brokers are designed to operate as part of a cluster. Within a cluster of brokers, one broker will also function as the cluster controller (elected automatically from the live members of the cluster). The controller is responsible for administrative operations, including assigning partitions to brokers and monitoring for broker failures. A partition is owned by a single broker in the cluster, and that broker is called the leader of the partition. A partition may be assigned to multiple brokers, which will result in the partition being replicated This provides redundancy of messages in the partition, such that another broker can take over leadership if there is a broker failure. However, all consumers and producers operating on that partition must connect to the leader.   retentions  A key feature of Apache Kafka is that of retention, which is the durable storage of messages for some period of time. Kafka brokers are configured with a default reten‐ tion setting for topics, either retaining messages for some period of time (e.g., 7 days) or until the topic reaches a certain size in bytes (e.g., 1 GB). Once these limits are reached, messages are expired and deleted so that the retention configuration is a minimum amount of data available at any time. Individual topics can also be config‐ ured with their own retention settings so that messages are stored for only as long as they are useful. For example, a tracking topic might be retained for several days, whereas application metrics might be retained for only a few hours. Topics can also be configured as log compacted, which means that Kafka will retain only the last mes‐ sage produced with a specific key. This can be useful for changelog-type data, where only the last update is interesting.   mirror maker  The Kafka project includes a tool called MirrorMaker, used for this purpose. At its core, MirrorMaker is simply a Kafka consumer and producer, linked together with a queue. Messages are consumed from one Kafka cluster and produced for another.   Why Kafka?  Multiple Producers  Kafka is able to seamlessly handle multiple producers, whether those clients are using many topics or the same topic.   Multiple Consumers  In addition to multiple producers, Kafka is designed for multiple consumers to read any single stream of messages without interfering with each other. This is in contrast to many queuing systems where once a message is consumed by one client, it is not available to any other. Multiple Kafka consumers can choose to operate as part of a group and share a stream, assuring that the entire group processes a given message only once.   ##Disk-Based Retention Not only can Kafka handle multiple consumers, but durable message retention means that consumers do not always need to work in real time. Messages are committed to disk, and will be stored with configurable retention rules.   Scalable  Kafka’s flexible scalability makes it easy to handle any amount of data. Users can start with a single broker as a proof of concept, expand to a small development cluster of three brokers, and move into production with a larger cluster of tens or even hun‐ dreds of brokers that grows over time as the data scales up.   High Performance  All of these features come together to make Apache Kafka a publish/subscribe mes‐ saging system with excellent performance under high load. Producers, consumers, and brokers can all be scaled out to handle very large message streams with ease. This can be done while still providing subsecond message latency from producing a mes‐ sage to availability to consumers.   Process of producing message  We start producing messages to Kafka by creating a ProducerRecord, which must include the topic we want to send the record to and a value. Optionally, we can also specify a key and/or a partition. Once we send the ProducerRecord, the first thing the producer will do is serialize the key and value objects to ByteArrays so they can be sent over the network. Next, the data is sent to a partitioner. If we specified a partition in the ProducerRecord, the partitioner doesn’t do anything and simply returns the partition we specified. If we didn’t, the partitioner will choose a partition for us, usually based on the ProducerRecord key. Once a partition is selected, the producer knows which topic and partition the record will go to. It then adds the record to a batch of records that will also be sent to the same topic and partition. A separate thread is responsible for sending those batches of records to the appropriate Kafka brokers. When the broker receives the messages, it sends back a response. If the messages were successfully written to Kafka, it will return a RecordMetadata object with the topic, partition, and the offset of the record within the partition. If the broker failed to write the messages, it will return an error. When the producer receives an error, it may retry sending the message a few more times before giving up and returning an error.   Constructing a Kafka Producer  The first step in writing messages to Kafka is to create a producer object with the properties you want to pass to the producer. A Kafka producer has three mandatory properties:  bootstrap.servers  List of host:port pairs of brokers that the producer will use to establish initial connection to the Kafka cluster. This list doesn’t need to include all brokers, since the producer will get more information after the initial connection. But it is rec‐ ommended to include at least two, so in case one broker goes down, the producer will still be able to connect to the cluster.  key.serializer  Name of a class that will be used to serialize the keys of the records we will pro‐ duce to Kafka. Kafka brokers expect byte arrays as keys and values of messages. However, the producer interface allows, using parameterized types, any Java object to be sent as a key and value. This makes for very readable code, but it also means that the producer has to know how to convert these objects to byte arrays. key.serializer should be set to a name of a class that implements the org.apache.kafka.common.serialization.Serializer interface. The producer will use this class to serialize the key object to a byte array. The Kafka client pack‐ age includes ByteArraySerializer (which doesn’t do much), StringSerializer, and IntegerSerializer, so if you use common types, there is no need to implement your own serializers. Setting key.serializer is required even if you intend to send only values.  value.serializer  Name of a class that will be used to serialize the values of the records we will pro‐ duce to Kafka. The same way you set key.serializer to a name of a class that will serialize the message key object to a byte array, you set value.serializer to a class that will serialize the message value object.   Sample code to generate producer record  private Properties kafkaProps = new Properties();     kafkaProps.put(\"bootstrap.servers\", \"broker1:9092,broker2:9092\");     kafkaProps.put(\"key.serializer\",       \"org.apache.kafka.common.serialization.StringSerializer\");     kafkaProps.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");     producer = new KafkaProducer&lt;String, String&gt;(kafkaProps);   Deliver message  Once we instantiate a producer, it is time to start sending messages. There are three primary methods of sending messages:  Fire-and-forget  We send a message to the server and don’t really care if it arrives succesfully or not. Most of the time, it will arrive successfully, since Kafka is highly available and the producer will retry sending messages automatically. However, some mes‐ sages will get lost using this method.  Synchronous send  We send a message, the send() method returns a Future object, and we use get() to wait on the future and see if the send() was successful or not.  Asynchronous send  We call the send() method with a callback function, which gets triggered when it receives a response from the Kafka broker.   Sample code  ProducerRecord&lt;String, String&gt; record =             new ProducerRecord&lt;&gt;(\"CustomerCountry\", \"Precision Products\", \"France\");     try {       producer.send(record);  //fire and forget       producer.send(record).get(); // synchronously, calling Future.get()     } catch (Exception e) {             e.printStackTrace(); }    We use the producer object send() method to send the ProducerRecord. As we’ve seen in the producer architecture diagram in Figure 3-1, the message will be placed in a buffer and will be sent to the broker in a separate thread. The send() method returns a Java Future object with RecordMetadata   Samle code of asynchronous  private class DemoProducerCallback implements Callback {             @Override         public void onCompletion(RecordMetadata recordMetadata, Exception e) {          if (e != null) {              e.printStackTrace();             } } }     ProducerRecord&lt;String, String&gt; record =             new ProducerRecord&lt;&gt;(\"CustomerCountry\", \"Biomedical Materials\", \"USA\");     producer.send(record, new DemoProducerCallback());   Rebalancing  Moving partition ownership from one consumer to another is called a rebalance. Rebalances are important because they provide the consumer group with high availa‐ bility and scalability (allowing us to easily and safely add and remove consumers), but in the normal course of events they are fairly undesirable. During a rebalance, con‐ sumers can’t consume messages, so a rebalance is basically a short window of unavail‐ ability of the entire consumer group. In addition, when partitions are moved from one consumer to another, the consumer loses its current state; if it was caching any data, it will need to refresh its caches—slowing down the application until the con‐ sumer sets up its state again. Throughout this chapter we will discuss how to safely handle rebalances and how to avoid unnecessary ones.   cosumber   Subscribing to Topics  Once we create a consumer, the next step is to subscribe to one or more topics. The subcribe() method takes a list of topics as a parameter, so it’s pretty simple to use:      consumer.subscribe(Collections.singletonList(\"customerCountries\"));  Here we simply create a list with a single element: the topic name customerCountries.   Sample consumer code   The Poll Loop At the heart of the consumer API is a simple loop for polling the server for more data. Once the consumer subscribes to topics, the poll loop handles all details of coordina‐ tion, partition rebalances, heartbeats, and data fetching, leaving the developer with a clean API that simply returns available data from the assigned partitions. The main body of a consumer will look as follows:      try {       while (true) {           ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);           for (ConsumerRecord&lt;String, String&gt; record : records)           {               log.debug(\"topic = %s, partition = %s, offset = %d,                  customer = %s, country = %s\\n\",                  record.topic(), record.partition(), record.offset(),                  record.key(), record.value());               int updatedCount = 1;               if (custCountryMap.countainsValue(record.value())) {                   updatedCount = custCountryMap.get(record.value()) + 1;               }               custCountryMap.put(record.value(), updatedCount)               JSONObject json = new JSONObject(custCountryMap);               System.out.println(json.toString(4))           } } } finally {       consumer.close();     }   Thread safety  Thread Safety You can’t have multiple consumers that belong to the same group in one thread and you can’t have multiple threads safely use the same consumer. One consumer per thread is the rule. To run mul‐ tiple consumers in the same group in one application, you will need to run each in its own thread. It is useful to wrap the con‐ sumer logic in its own object and then use Java’s ExecutorService to start multiple threads each with its own consumer.   Commit   As discussed before, one of Kafka’s unique characteristics is that it does not track acknowledgments from consumers the way many JMS queues do. Instead, it allows consumers to use Kafka to track their posi‐ tion (offset) in each partition. We call the action of updating the current position in the partition a commit.   How does a consumer commit an offset? It produces a message to Kafka, to a special __consumer_offsets topic, with the committed offset for each partition. As long as all your consumers are up, running, and churning away, this will have no impact. However, if a consumer crashes or a new consumer joins the consumer group, this will trigger a rebalance. After a rebalance, each consumer may be assigned a new set of partitions than the one it processed before. In order to know where to pick up the work, the consumer will read the latest committed offset of each partition and con‐ tinue from there.   Automatic Commit   With autocommit enabled, a call to poll will always commit the last offset returned by the previous poll. It doesn’t know which events were actually processed, so it is critical to always process all the events returned by poll() before calling poll() again. (Just like poll(), close() also commits offsets automatically.) This is usually not an issue, but pay attention when you handle exceptions or exit the poll loop prematurely.   Manual commit  It is important to remember that commitSync() will commit the latest offset returned by poll(), so make sure you call commitSync() after you are done processing all the records in the collection, or you risk missing messages as described previously. When rebalance is triggered, all the messages from the beginning of the most recent batch until the time of the rebalance will be processed twice. Here is how we would use commitSync to commit offsets after we finished processing the latest batch of messages:  while (true) {         ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);         for (ConsumerRecord&lt;String, String&gt; record : records)         {             System.out.printf(\"topic = %s, partition = %s, offset =               %d, customer = %s, country = %s\\n\",                  record.topic(), record.partition(),                  record.offset(), record.key(), record.value()); } try {           consumer.commitSync();         } catch (CommitFailedException e) {             log.error(\"commit failed\", e)         } }   Combining Synchronous and Asynchronous Commits  Normally, occasional failures to commit without retrying are not a huge problem because if the problem is temporary, the following commit will be successful. But if we know that this is the last commit before we close the consumer, or before a reba‐ lance, we want to make extra sure that the commit succeeds. Therefore, a common pattern is to combine commitAsync() with commitSync() just before shutdown. Here is how it works (we will discuss how to commit just before rebalance when we get to the section about rebalance listeners):      try {         while (true) {             ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);             for (ConsumerRecord&lt;String, String&gt; record : records) {                 System.out.printf(\"topic = %s, partition = %s, offset = %d,                 customer = %s, country = %s\\n\",                 record.topic(), record.partition(),                 record.offset(), record.key(), record.value()); }             consumer.commitAsync();         }     } catch (Exception e) {         log.error(\"Unexpected error\", e);     } finally {         try {             consumer.commitSync();         } finally {             consumer.close();         } }   Exit   When you decide to exit the poll loop, you will need another thread to call con sumer.wakeup(). If you are running the consumer loop in the main thread, this can be done from ShutdownHook. Note that consumer.wakeup() is the only consumer method that is safe to call from a different thread. Calling wakeup will cause poll() to exit with WakeupException, or if consumer.wakeup() was called while the thread was not waiting on poll, the exception will be thrown on the next iteration when poll() is called.   The Controller  The controller is one of the Kafka brokers that, in addition to the usual broker func‐ tionality, is responsible for electing partition leaders (we’ll discuss partition leaders and what they do in the next section). The first broker that starts in the cluster becomes the controller by creating an ephemeral node in ZooKeeper called /control ler. When other brokers start, they also try to create this node, but receive a “node already exists” exception, which causes them to “realize” that the controller node already exists and that the cluster already has a controller.   Replication  Replication is at the heart of Kafka’s architecture. The very first sentence in Kafka’s documentation describes it as “a distributed, partitioned, replicated commit log ser‐ vice.” Replication is critical because it is the way Kafka guarantees availability and durability when individual nodes inevitably fail.   Memeroy   Kafka should run entirely on RAM. JVM heap size shouldn’t be bigger than your available RAM. That is to avoid swapping.   Swap usage  Watch for swap usage, as it will degrade performance on Kafka and lead to operations timing out (set vm.swappiness = 0).    When used swap is &gt; 128MB.   Kafka Monitoring Tools   Any monitoring tools with JMX support should be able to monitor a Kafka cluster. Here are 3 monitoring tools we liked:   First one is check_kafka.pl from Hari Sekhon. It performs a complete end to end test, i.e. it inserts a message in Kafka as a producer and then extracts it as a consumer. This makes our life easier when measuring service times.   Another useful tool is KafkaOffsetMonitor for monitoring Kafka consumers and their position (offset) in the queue. It aids our understanding of how our queue grows and which consumers groups are lagging behind.   Last but not least, the LinkedIn folks have developed what we think is the smartest tool out there: Burrow. It analyzes consumer offsets and lags over a window of time and determines the consumer status. You can retrieve this status over an HTTP endpoint and then plug it into your favourite monitoring tool (Server Density for example).   Oh, and we would be amiss if we didn’t mention Yahoo’s Kafka-Manager. While it does include some basic monitoring, it is more of a management tool. If you are just looking for a Kafka management tool, check out AirBnb’s kafkat.   commands  Start zookeeper   bin/zookeeper-server-start.sh config/zookeeper.properties   bin/kafka-server-start.sh config/server.properties   ~/dev/git/kafka-demo/kafka_2.11-2.0.0/bin/kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic todtest  bin/kafka-topics.sh –list –zookeeper localhost:2181 bin/kafka-console-producer.sh –broker-list localhost:9092 –topic todtest bin/kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic todtest –from-beginning   bin/kafka-topics.sh –describe –zookeeper localhost:2181 –topic test   list topics  ./kafka-topics.sh --list --zookeeper localhost:2181   describe topics  ./kafka-topics.sh –describe –zookeeper localhost:2181   using connector  bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties   mvn archetype:generate       -DarchetypeGroupId=org.apache.kafka       -DarchetypeArtifactId=streams-quickstart-java       -DarchetypeVersion=2.0.0       -DgroupId=io       -DartifactId=todzhang       -Dversion=0.1       -Dpackage=todzhangapp   Security   The keystore stores each machine’s own identity. The truststore stores all the certificates that the machine should trust. Importing a certificate into one’s truststore also means trusting all certificates that are signed by that certificate. As the analogy above, trusting the government (CA) also means trusting all passports (certificates) that it has issued. This attribute is called the chain of trust, and it is particularly useful when deploying SSL on a large Kafka cluster. You can sign all certificates in the cluster with a single CA, and have all machines share the same truststore that trusts the CA. That way all machines can authenticate all other machines.   To deploy SSL, the general steps are:     Generate the keys and certificates   Create your own Certificate Authority (CA)   Sign the certificate   Generate the key and the certificate for each Kafka broker in the cluster. Generate the key into a keystore called kafka.server.keystore so that you can export and sign it later with CA. The keystore file contains the private key of the certificate; therefore, it needs to be kept safely.   With user prompts  keytool -keystore kafka.server.keystore.jks -alias localhost -genkey   Without user prompts, pass command line arguments  keytool -keystore kafka.server.keystore.jks -alias localhost -validity {validity} -genkey -storepass {keystore-pass} -keypass {key-pass} -dname {distinguished-name} -ext SAN=DNS:{hostname} Ensure that the common name (CN) exactly matches the fully qualified domain name (FQDN) of the server. The client compares the CN with the DNS domain name to ensure that it is indeed connecting to the desired server, not a malicious one. The hostname of the server can also be specified in the Subject Alternative Name (SAN). Since the distinguished name is used as the server principal when SSL is used as the inter-broker security protocol, it is useful to have hostname as a SAN rather than the CN.   Create your own Certificate Authority (CA)  Generate a CA that is simply a public-private key pair and certificate, and it is intended to sign other certificates.  openssl req -new -x509 -keyout ca-key -out ca-cert -days {validity}  Add the generated CA to the clients’ truststore so that the clients can trust this CA:  keytool -keystore kafka.client.truststore.jks -alias CARoot -import -file ca-cert  Add the generated CA to the brokers’ truststore so that the brokers can trust this CA.  keytool -keystore kafka.server.truststore.jks -alias CARoot -import -file ca-cert   Sign the certificate  To sign all certificates in the keystore with the CA that you generated:   Export the certificate from the keystore:   keytool -keystore kafka.server.keystore.jks -alias localhost -certreq -file cert-file Sign it with the CA:   openssl x509 -req -CA ca-cert -CAkey ca-key -in cert-file -out cert-signed -days {validity} -CAcreateserial -passin pass:{ca-password} Import both the certificate of the CA and the signed certificate into the broker keystore:   keytool -keystore kafka.server.keystore.jks -alias CARoot -import -file ca-cert keytool -keystore kafka.server.keystore.jks -alias localhost -import -file cert-signed   SASL  Simple Authentication and Security Layer (SASL) is a framework for authentication and data security in Internet protocols. It decouples authentication mechanisms from application protocols, in theory allowing any authentication mechanism supported by SASL to be used in any application protocol that uses SASL. Authentication mechanisms can also support proxy authorization, a facility allowing one user to assume the identity of another. They can also provide a data security layer offering data integrity and data confidentiality services. DIGEST-MD5 provides an example of mechanisms which can provide a data-security layer. Application protocols that support SASL typically also support Transport Layer Security (TLS) to complement the services offered by SASL.   References     https://blog.serverdensity.com/how-to-monitor-kafka/  ","categories": [],
        "tags": ["Kafka"],
        "url": "/2019/07/07/Kafka.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Terraform",
        "excerpt":"Why Terraform   Software isn’t done when the code is working on your computer. It’s not done when the tests pass. And it’s not done when someone gives you a “ship it” on a code review. Software isn’t done until you deliver it to the user.   ","categories": [],
        "tags": ["aws","clouds"],
        "url": "/2019/07/26/Terraform.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "AWS certification",
        "excerpt":"Footprint   As of 2019, AWS has distinct operations in 22 geographical “regions”:[7] 7 in North America, 1 in South America, 6 in Europe, 1 in the Middle-East, 1 in Africa and 8 in Asia Pacific.   Concepting   Cloud computing is the on-demand delivery of IT resources and applications via the Internet with pay-as-you-go pricing. Whether you run applications that share photos to millions of mobile users or deliver services that support the critical operations of your business, the cloud provides rapid access to flexible and low-cost IT resources.   In its simplest form, cloud computing provides an easy way to access servers, storage, databases, and a broad set of application services over the Internet.   Benefits of AWS   There are six advantages for AWS clouding      Global in minutes   Variable vs capital expense   Economies of scale   Stop guessing capacity   Focus on business differentaiors   Increate speed and agility   Cost saving  One of the key benefits of cloud computing is the opportunity to replace up-front capital infrastructure expenses with low variable costs that scale with your business. With the cloud, businesses no longer need to plan for and procure servers and other IT infrastructure weeks or months in advance. Instead, they can instantly spin up hundres or thousands of servers in minutes and deliver results faster.   With pay-per-use billing, AWS clouding services become an operational expense instead of a capital expense.   Metadata   Metadata, known as tags, that you can create and assign to your Amazon EC2 resources   AZ (Available Zones)     Each availability zone is a physical data center in the region, but separate from the other ones (so that they’re isolated from disasters)   AWS Consoles are region scoped (except IAM, S3 &amp; Route53)   EC2  Here you need to create an AMI, but because AMI are bounded in the regions they are created, they need to be copied across regions for disaster recovery purposes   EC2 instance stopping  If you stopped an EBS-backed EC2 instance, the volume is preserved but the data in any attached Instance store volumes will be erased. Keep in mind that an EC2 instance has an underlying physical host computer. If the instance is stopped, AWS usually moves the instance to a new host computer. Your instance may stay on the same host computer if there are no problems with the host computer. In addition, its Elastic IP address is disassociated from the instance if it is an EC2-Classic instance. Otherwise, if it is an EC2-VPC instance, the Elastic IP address remains associated.   Placement group  Placements groups are the answer here, where “cluster” guarantees high network performance (correct answer), whereas “spread” would guarantee independent failures between instances.   When you launch a new EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. Depending on the type of workload, you can create a placement group using one of the following placement strategies:   Cluster – packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications.   Partition – spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka.   Spread – strictly places a small group of instances across distinct underlying hardware to reduce correlated failures.   There is no charge for creating a placement group.   Placement Groups is primarily used to determine how your instances are placed on the underlying hardware while Enhanced Networking, on the other hand, is for providing high-performance networking capabilities using single root I/O virtualization (SR-IOV) on supported EC2 instance types.   Security Group  When you create a security group, it has no inbound rules. Therefore, no inbound traffic originating from another host to your instance is allowed until you add inbound rules to the security group. By default, a security group includes an outbound rule that allows all outbound traffic. You can remove the rule and add outbound rules that allow specific outbound traffic only. If your security group has no outbound rules, no outbound traffic originating from your instance is allowed.   Options 1 and 4 are both incorrect because any changes to the Security Groups or Network Access Control Lists are applied immediately and not after 60 minutes or after the instance reboot.   Option 2 is incorrect because the scenario says that VPC is using a default configuration. Since by default, Network ACL allows all inbound and outbound IPv4 traffic, then there is no point of explicitly allowing the port in the Network ACL. Security Groups, on the other hand, does not allow incoming traffic by default, unlike Network ACL.   Custom port   To allow the custom port, you have to change the Inbound Rules in your Security Group to allow traffic coming from the mobile devices. Security Groups usually control the list of ports that are allowed to be used by your EC2 instances and the NACLs control which network or list of IP addresses can connect to your whole VPC.   Cluster   Cluster Placement Groups   A cluster placement group is a logical grouping of instances within a single Availability Zone. A placement group can span peered VPCs in the same Region. The chief benefit of a cluster placement group, in addition to a 10 Gbps flow limit, is the non-blocking, non-oversubscribed, fully bi-sectional nature of the connectivity. In other words, all nodes within the placement group can talk to all other nodes within the placement group at the full line rate of 10 Gbps flows and 100 Gbps aggregate without any slowing due to over-subscription.   ASG   ASG Lauch configuration   Launch configurations are immutable meaning they cannot be updated. You have to create a new launch configuration, attach it to the ASG and then terminate old instances / launch new instances   ASG termination  AZs will be balanced first, then the instance with the oldest launch configuration within that AZ will be terminated. For a reference to the default termination policy logic, have a look at this link: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html   AMI   the EC2 instances you are currently using depends on a pre-built AMI. This AMI is not accessible to another region hence,  you have to copy it to the us-west-2 region to properly establish your disaster recovery instance.   You can copy an Amazon Machine Image (AMI) within or across an AWS region using the AWS Management Console, the AWS command line tools or SDKs, or the Amazon EC2 API, all of which support the CopyImage action. You can copy both Amazon EBS-backed AMIs and instance store-backed AMIs. You can copy encrypted AMIs and AMIs with encrypted snapshots   AWS Device Farm  AWS Device Farm is an app testing service that lets you test and interact with your Android, iOS, and web apps on many devices at once, or reproduce issues on a device in real time.   IAM  Your whole AWS security is there:  • Users • Groups  • Roles   Policies are written in JSON (JavaScript Object Notation)   IAM has a global view   Permissions let you specify access to AWS resources. Permissions are granted to IAM entities (users, groups, and roles) and by default these entities start with no permissions. In other words, IAM entities can do nothing in AWS until you grant them your desired permissions. To give entities permissions, you can attach a policy that specifies the type of access, the actions that can be performed, and the resources on which the actions can be performed. In addition, you can specify any conditions that must be set for access to be allowed or denied.   To enforce IAM      Enable Multi-Factor Authentication   Assign an IAM role to the Amazon EC2 instance   Always remember that you should associate IAM roles to EC2 instances and not an IAM user, for the purpose of accessing other AWS services. IAM roles are designed so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use. Instead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles.   IAM policies  A permissions policy describes who has access to what. Policies attached to an IAM identity are identity-based policies (IAM policies) and policies attached to a resource are resource-based policies. Amazon RDS supports only identity-based policies (IAM policies).   DB authenticatioin via IAM   MySQL and PostgreSQL both support IAM database authentication.   To protect the confidential data of your customers, you have to ensure that your RDS database can only be accessed using the profile credentials specific to your EC2 instances via an authentication token.   You can authenticate to your DB instance using AWS Identity and Access Management (IAM) database authentication. IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don’t need to use a password when you connect to a DB instance. Instead, you use an authentication token.   An authentication token is a unique string of characters that Amazon RDS generates on request. Authentication tokens are generated using AWS Signature Version 4. Each token has a lifetime of 15 minutes. You don’t need to store user credentials in the database, because authentication is managed externally using IAM. You can also still use standard database authentication.   IAM Federation  • Big enterprises usually integrate their own repository of users with IAM  • This way, one can login into AWS using their company credentials • Identity Federation uses the SAML standard (Active Directory)   • One IAM User per PHYSICAL PERSON • One IAM Role per Application   STS  Temporary Security Credentials You can use the AWS Security Token Service (AWS STS) to create and provide trusted users with temporary security credentials that can control access to your AWS resources. Temporary security credentials work almost identically to the long-term access key credentials that your IAM users can use, with the following differences:   Temporary security credentials are short-term, as the name implies. They can be configured to last for anywhere from a few minutes to several hours. After the credentials expire, AWS no longer recognizes them or allows any kind of access from API requests made with them.   Temporary security credentials are not stored with the user but are generated dynamically and provided to the user when requested. When (or even before) the temporary security credentials expire, the user can request new credentials, as long as the user requesting them still has permissions to do so.   Amazon Cognito  This service is primarily used for user authentication and not for providing access to your AWS resources. A JSON Web Token (JWT) is meant to be used for user authentication and session management.   AWS SSO  This service uses STS, it does not issue short-lived credentials by itself. AWS Single Sign-On (SSO) is a cloud SSO service that makes it easy to centrally manage SSO access to multiple AWS accounts and business applications.   Storage  Performance  Instance Store will have the highest disk performance but comes with the storage being wiped if the instance is terminated, which is acceptable in this case. EBS volumes would provide good performance as far as disk goes, but not as good as Instance Store. EBS data survives instance termination or reboots. EFS is a network drive, and finally S3 cannot be mounted as a local disk (natively).   Need to define two terms: • RPO: Recovery Point Objective • RTO: Recovery Time Objective   S3  Generating S3 pre-signed URLs would bypass CloudFront, therefore we should use CloudFront signed URL. To generate that URL we must code, and Lambda is the perfect tool for running that code on the fly.   As the file is greater than 5GB in size, you must use Multi Part upload to upload that file to S3.   S3 ETag  Every S3 object has an associated Entity tag or ETag which can be used for file and object comparison.      The ETag may or may not be an MD5 digest of the object data    If an object is created by either the Multipart Upload or Part Copy operation, the ETag is not an MD5 digest, regardless of the method of encryption.   For multipart uploads the ETag is the MD5 hexdigest of each part’s MD5 digest concatenated together, followed by the number of parts separated by a dash.   E.g. for a two part object the ETag may look something like this:   d41d8cd98f00b204e9800998ecf8427e-2   S3 Glacier  Expedited retrievals allow you to quickly access your data when occasional urgent requests for a subset of archives are required. For all but the largest archives (250 MB+), data accessed using Expedited retrievals are typically made available within 1–5 minutes. Provisioned Capacity ensures that retrieval capacity for Expedited retrievals is available when you need it.   To make an Expedited, Standard, or Bulk retrieval, set the Tier parameter in the Initiate Job (POST jobs) REST API request to the option you want, or the equivalent in the AWS CLI or AWS SDKs. If you have purchased provisioned capacity, then all expedited retrievals are automatically served through your provisioned capacity.   Provisioned capacity ensures that your retrieval capacity for expedited retrievals is available when you need it. Each unit of capacity provides that at least three expedited retrievals can be performed every five minutes and provides up to 150 MB/s of retrieval throughput. You should purchase provisioned retrieval capacity if your workload requires highly reliable and predictable access to a subset of your data in minutes. Without provisioned capacity Expedited retrievals are accepted, except for rare situations of unusually high demand. However, if you require access to Expedited retrievals under all circumstances, you must purchase provisioned retrieval capacity.   Amazon Glacier Select  It is not an archive retrieval option and is primarily used to perform filtering operations using simple Structured Query Language (SQL) statements directly on your data archive in Glacier.   Bulk retrievals  It typically complete within 5–12 hours hence, this does not satisfy the requirement of retrieving the data within 15 minutes. The provisioned capacity option is also not compatible with Bulk retrievals.   ranged archive retrievals  using ranged archive retrievals is not enough to meet the requirement of retrieving the whole archive in the given timeframe. In addition, it does not provide additional retrieval capacity which is what the provisioned capacity option can offer.   S3 Select  It is an Amazon S3 feature that makes it easy to retrieve specific data from the contents of an object using simple SQL expressions without having to retrieve the entire object.  Similiarly, Amazon Redshift Spectrum is a feature of Amazon Redshift that enables you to run queries against exabytes of unstructured data in Amazon S3 with no loading or ETL required.   OAI  Don’t make the S3 bucket public. You cannot attach IAM roles to the CloudFront distribution. S3 buckets don’t have security groups. Here you need to use an OAI. Read more here: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html   Restricting Access to Amazon S3 Content by Using an Origin Access Identity To restrict access to content that you serve from Amazon S3 buckets, you create CloudFront signed URLs or signed cookies to limit access to files in your Amazon S3 bucket, and then you create a special CloudFront user called an origin access identity (OAI) and associate it with your distribution. Then you configure permissions so that CloudFront can use the OAI to access and serve files to your users, but users can’t use a direct URL to the S3 bucket to access a file there. Taking these steps help you maintain secure access to the files that you serve through CloudFront.   In general, if you’re using an Amazon S3 bucket as the origin for a CloudFront distribution, you can either allow everyone to have access to the files there, or you can restrict access. If you limit access by using, for example, CloudFront signed URLs or signed cookies, you also won’t want people to be able to view files by simply using the direct URL for the file. Instead, you want them to only access the files by using the CloudFront URL, so your protections work. For more information about using signed URLs and signed cookies, see Serving Private Content with Signed URLs and Signed Cookies   S3 Q&amp;A  Amazon S3 now provides increased performance to support at least 3,500 requests per second to add data and 5,500 requests per second to retrieve data, which can save significant processing time for no additional charge. Each S3 prefix can support these request rates, making it simple to increase performance significantly.   Applications running on Amazon S3 today will enjoy this performance improvement with no changes, and customers building new applications on S3 do not have to make any application customizations to achieve this performance. Amazon S3’s support for parallel requests means you can scale your S3 performance by the factor of your compute cluster, without making any customizations to your application. Performance scales per prefix, so you can use as many prefixes as you need in parallel to achieve the required throughput. There are no limits to the number of prefixes.   This S3 request rate performance increase removes any previous guidance to randomize object prefixes to achieve faster performance. That means you can now use logical or sequential naming patterns in S3 object naming without any performance implications. This improvement is now available in all AWS Regions.   Option 1 is incorrect because it is an archival/long term storage solution, which is not optimal if you are serving objects frequently and fast retrieval is a must.   Option 2 is incorrect. Adding a random prefix is not required in this scenario because S3 can now scale automatically to adjust perfomance. You do not need to add a random prefix anymore for this purpose since S3 has increased performance to support at least 3,500 requests per second to add data and 5,500 requests per second to retrieve data, which covers the workload in the scenario.   Option 4 is incorrect because Amazon S3 already maintains an index of object key names in each AWS region. S3 stores key names in alphabetical order. The key name dictates which partition the key is stored in. Using a sequential prefix increases the likelihood that Amazon S3 will target a specific partition for a large number of your keys, overwhelming the I/O capacity of the partition.   Encryption  With SSE-C, your company can still provide the encryption key but let AWS do the encryption   EBS (Elastic Block Storage)  EBS is already redundant storage (replicated within an AZ) But what if you want to increase IOPS to say 100 000 IOPS?   RAID  RAID 0 (increase performance)  EC2 instance One logical volume either EBS Volume 1 • Combining 2 or more volumes and getting the total disk space and I/O • But one disk fails, all the data is failed   RAID 1 (increase fault tolerance)  EC2 instance One logical volume both • RAID 1 = Mirroring a volume to another • If one disk fails, our logical volume is still working • We have to send the data to two EBS volume at the same time (2x network)   EBS types  keeping as io1 but reducing the iops may interfere with the burst of performance we need. The EC2 instance type changes won’t affect the 90% of the costs that are incurred to us. CloudFormation is a free service to use. Therefore, gp2 is the right choice, allowing us to save on cost while keeping a burst in performance when needed   You can now choose between three Amazon EBS volume types to best meet the needs of your workloads: General Purpose (SSD), Provisioned IOPS (SSD), and Magnetic volumes.   General Purpose (SSD)  GP2 volumes are suitable for a broad range of workloads, including small to medium-sized databases, development and test environments, and boot volumes.  Provisioned IOPS (SSD)  Such volumes offer storage with consistent and low-latency performance, are designed for I/O-intensive applications such as large relational or NoSQL databases, and allow you to choose the level of performance you need.  Magnetic volumes  formerly known as Standard volumes, provide the lowest cost per gigabyte of all Amazon EBS volume types and are ideal for workloads where data is accessed infrequently and applications where the lowest storage cost is important.   Backed by Solid-State Drives (SSDs), General Purpose (SSD) volumes provide the ability to burst to 3,000 IOPS per volume, independent of volume size, to meet the performance needs of most applications and also deliver a consistent baseline of 3 IOPS/GB. General Purpose (SSD) volumes offer the same five nines of availability and durable snapshot capabilities as other volume types. Pricing and performance for General Purpose (SSD) volumes are simple and predictable. You pay for each GB of storage you provision, and there are no additional charges for I/O performed on a volume. Prices start as low as $0.10/GB.   EBS snapshot  While it is completing, an in-progress snapshot is not affected by ongoing reads and writes to the volume.   You can take a snapshot of an attached volume that is in use. However, snapshots only capture data that has been written to your Amazon EBS volume at the time the snapshot command is issued. This might exclude any data that has been cached by any applications or the operating system. If you can pause any file writes to the volume long enough to take a snapshot, your snapshot should be complete. However, if you can’t pause all file writes to the volume, you should unmount the volume from within the instance, issue the snapshot command, and then remount the volume to ensure a consistent and complete snapshot. You can remount and use your volume while the snapshot status is pending.   Save network cost  S3 would imply changing the application code, Glacier is not applicable as the files are frequently requested, Storage Gateway isn’t for distributing files to end users. CloudFront is the right answer, because we can put it in front of our ASG and leverage a Global Caching feature that will help us distribute the content reliably with dramatically reduced costs (the ASG won’t need to scale as much)   EFS  Instance Stores or EBS volumes are local disks and cannot be shared across instances. Here, we need a network file system (NFS), which is exactly what EFS is designed for.   Redshift  Creating a smaller cluster with the cold data would not decrease the storage cost of Redshift, which will increase as we keep on creating data. Moving the data to S3 glacier will prevent us from being able to query it. Redshift’s internal storage does not have “tiers”. Therefore, we should migrate the data to S3 IA and use Athena (serverless SQL query engine on top of S3) to analyze the cold data.   Amazon Redshift is a fast, scalable data warehouse that makes it simple and cost-effective to analyze all your data across your data warehouse and data lake. Redshift delivers ten times faster performance than other data warehouses by using machine learning, massively parallel query execution, and columnar storage on high-performance disk.   In this scenario, there is a requirement to have a storage service which will be used by a business intelligence application and where the data must be stored in a columnar fashion. Business Intelligence reporting systems is a type of Online Analytical Processing (OLAP) which Redshift is known to support. In addition, Redshift also provides columnar storage unlike the other options. Hence, the correct answer in this scenario is Option 1: Amazon Redshift.   RedShift Spectrum  Enables you to run queries against exabytes of data in S3 without having to load or transform any data. Redshift Spectrum doesn’t use Enhanced VPC Routing. If you store data in a columnar format, Redshift Spectrum scans only the columns needed by your query, rather than processing entire rows. If you compress your data using one of Redshift Spectrum’s supported compression algorithms, less data is scanned.   CloundFront   Origin  Until now, CloudFront could serve up content from Amazon S3. In content-distribution lingo, S3 was the only supported origin server. You would store your web objects (web pages, style sheets, images, JavaScript, and so forth) in S3, and then create a CloudFront distribution. Here is the basic flow:   Effective today we are opening up CloudFront and giving you the ability to use the origin server of your choice.   You can now create a CloudFront distribution using a custom origin. Each distribution will can point to an S3 or to a custom origin. This could be another storage service, or it could be something more interesting and more dynamic, such as an EC2 instance or even an Elastic Load Balancer:   CloudFormation   CloudFormation vs Elastic Beanstalk   Elastic Beanstalk provides an environment to easily deploy and run applications in the cloud. CloudFormation is a convenient provisioning mechanism for a broad range of AWS resources.   VPC  you can optionally connect to your own network, known as virtual private clouds (VPCs)   Amazon Web Services. Amazon Elastic Compute Cloud.   Amazon VPC lets you provision a logically isolated section of the Amazon Web Services (AWS) cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address ranges, creation of subnets, and configuration of route tables and network gateways. You can also create a hardware Virtual Private Network (VPN) connection between your corporate datacenter and your VPC and leverage the AWS cloud as an extension of your corporate datacenter.   Subnet  A subnet is a range of IP addresses in your VPC. You can launch AWS resources into a specified subnet. Use a public subnet for resources that must be connected to the internet, and a private subnet for resources that won’t be connected to the internet. To protect the AWS resources in each subnet, use security groups and network access control lists (ACL).   Remember that one subnet is mapped into one specific Availability Zone.   Subnet and Avaialability Zone and VPC  A VPC spans all the Availability Zones in the region. After creating a VPC, you can add one or more subnets in each Availability Zone. When you create a subnet, you specify the CIDR block for the subnet, which is a subset of the VPC CIDR block. Each subnet must reside entirely within one Availability Zone and cannot span zones. Availability Zones are distinct locations that are engineered to be isolated from failures in other Availability Zones. By launching instances in separate Availability Zones, you can protect your applications from the failure of a single location. AWS assigns a unique ID to each subnet.   Default subnet  By default, a “default subnet” of your VPC is actually a public subnet, because the main route table sends the subnet’s traffic that is destined for the internet to the internet gateway. You can make a default subnet into a private subnet by removing the route from the destination 0.0.0.0/0 to the internet gateway. However, if you do this, any EC2 instance running in that subnet can’t access the internet.   Instances that you launch into a default subnet receive both a public IPv4 address and a private IPv4 address, and both public and private DNS hostnames. Instances that you launch into a nondefault subnet in a default VPC don’t receive a public IPv4 address or a DNS hostname. You can change your subnet’s default public IP addressing behavior   By default, nondefault subnets have the IPv4 public addressing attribute set to false, and default subnets have this attribute set to true. An exception is a nondefault subnet created by the Amazon EC2 launch instance wizard — the wizard sets the attribute to true.   Newly created instance does not have a public IP address since it was deployed on a nondefault subnet. The other 4 instances are accessible over the Internet because they each have an Elastic IP address attached, unlike the last instance which only has a private IP address. An Elastic IP address is a public IPv4 address, which is reachable from the Internet. If your instance does not have a public IPv4 address, you can associate an Elastic IP address with your instance to enable communication with the Internet.   VPC Endpoint  You must remember that the two services that use a VPC Endpoint Gateway are Amazon S3 and DynamoDB. The rest are VPC Endpoint Interface   NACL (Network ACL)  NACL is stateless.   • NACL are like a firewall which control traffic from and to subnet • Default NACL allows everything outbound and everything inbound • One NACL per Subnet, new Subnets are assigned the Default NACL • Define NACL rules: • Rules have a number (1-32766) and higher precedence with a lower number • E.g. If you define #100 ALLOW  and #200 DENY  , IP will be allowed • Last rule is an asterisk (*) and denies a request in case of no rule match • AWS recommends adding rules by increment of 100 • Newly created NACL will deny everything • NACL are a great way of blocking a specific IP at the subnet level   NACL noteworhty points  Network ACL Basics      Your VPC automatically comes with a modifiable default network ACL. By default, it allows all inbound and outbound IPv4 traffic and, if applicable, IPv6 traffic.   IGW (Internet GateWay)  After creating an IGW, make sure the route tables are updated. Additionally, ensure the security group allow the ICMP protocol for ping requests   NAT  NAT Instances would work but won’t scale and you would have to manage them (as they’re EC2 instances). Egress-Only Internet Gateways are for IPv6, not IPv4. Internet Gateways must be deployed in a public subnet. Therefore you must use a NAT Gateway in your public subnet in order to provide internet access to your instances in your private subnets.   You do not need a NAT Gateway nor a NAT instance when the instances are already in public subnet. Remember that a NAT Gateway or a NAT instance is primarily used to enable instances in a private subnet to connect to the Internet or other AWS services, but prevent the Internet from initiating a connection with those instances.   How to prevent DDoS  AWS provides flexible infrastructure and services that help customers implement strong DDoS mitigations and create highly available application architectures that follow AWS Best Practices for DDoS Resiliency. These include services such as Amazon Route 53, Amazon CloudFront, Elastic Load Balancing, and AWS WAF to control and absorb traffic, and deflect unwanted requests. These services integrate with AWS Shield, a managed DDoS protection service that provides always-on detection and automatic inline mitigations to safeguard web applications running on AWS.   AWS Shield   AWS Shield is a managed DDoS protection service that is available in two tiers: Standard and Advanced. AWS Shield Standard applies always-on detection and inline mitigation techniques, such as deterministic packet filtering and priority-based traffic shaping, to minimize application downtime and latency.   AWS Shield Standard  It is included automatically and transparently to your Elastic Load Balancing load balancers, Amazon CloudFront distributions, and Amazon Route 53 resources at no additional cost. When you use these services that include AWS Shield Standard, you receive comprehensive availability protection against all known infrastructure layer attacks. Customers who have the technical expertise to manage their own monitoring and mitigation of application layer attacks can use AWS Shield together with AWS WAF rules to create a comprehensive DDoS attack mitigation strategy.   AWS Shield Advanced  It provides enhanced DDoS attack detection and monitoring for application-layer traffic to your Elastic Load Balancing load balancers, CloudFront distributions, Amazon Route 53 hosted zones and resources attached to an Elastic IP address, such Amazon EC2 instances. AWS Shield Advanced uses additional techniques to provide granular detection of DDoS attacks, such as resource-specific traffic monitoring to detect HTTP floods or DNS query floods. AWS Shield Advanced includes 24x7 access to the AWS DDoS Response Team (DRT), support experts who apply manual mitigations for more complex and sophisticated DDoS attacks, directly create or update AWS WAF rules, and can recommend improvements to your AWS architectures. AWS WAF is included at no additional cost for resources that you protect with AWS Shield Advanced.   AWS Shield Advanced includes access to near real-time metrics and reports, for extensive visibility into infrastructure layer and application layer DDoS attacks. You can combine AWS Shield Advanced metrics with additional, fine-tuned AWS WAF metrics for a more comprehensive CloudWatch monitoring and alarming strategy. Customers subscribed to AWS Shield Advanced can also apply for a credit for charges that result from scaling during a DDoS attack on protected Amazon EC2, Amazon CloudFront, Elastic Load Balancing, or Amazon Route 53 resources. See the AWS Shield Developer Guide for a detailed comparison of the two AWS Shield offerings.   CIDR  To add a CIDR block to your VPC, the following rules apply:   -The allowed block size is between a /28 netmask and /16 netmask. -The CIDR block must not overlap with any existing CIDR block that’s associated with the VPC. -You cannot increase or decrease the size of an existing CIDR block. -You have a limit on the number of CIDR blocks you can associate with a VPC and the number of routes you can add to a route table. You cannot associate a CIDR block if this results in you exceeding your limits. -The CIDR block must not be the same or larger than the CIDR range of a route in any of the VPC route tables. For example, if you have a route with a destination of 10.0.0.0/24 to a virtual private gateway, you cannot associate a CIDR block of the same range or larger. However, you can associate a CIDR block of 10.0.0.0/25 or smaller. -The first four IP addresses and the last IP address in each subnet CIDR block are not available for you to use, and cannot be assigned to an instance.   IPv4 CIDR block size should be between a /16 netmask (65,536 IP addresses) and /28 netmask (16 IP addresses). The first four IP addresses and the last IP address in each subnet CIDR block are NOT available for you to use, and cannot be assigned to an instance.   If you’re using AWS Direct Connect to connect to multiple VPCs through a direct connect gateway, the VPCs that are associated with the direct connect gateway must not have overlapping CIDR blocks.   Subnet Routing  Each subnet must be associated with a route table, which specifies the allowed routes for outbound traffic leaving the subnet. Every subnet that you create is automatically associated with the main route table for the VPC. You can change the association, and you can change the contents of the main route table. You can allow an instance in your VPC to initiate outbound connections to the internet over IPv4 but prevent unsolicited inbound connections from the internet using a NAT gateway or NAT instance. To initiate outbound-only communication to the internet over IPv6, you can use an egress-only internet gateway   Subnet Security  Security Groupss — control inbound and outbound traffic for your instances  You can associate one or more (up to five) security groups to an instance in your VPC. If you don’t specify a security group, the instance automatically belongs to the default security group. When you create a security group, it has no inbound rules. By default, it includes an outbound rule that allows all outbound traffic.  Security groups are associated with network interfaces.  Network Access Control Lists — control inbound and outbound traffic for your subnets Each subnet in your VPC must be associated with a network ACL. If none is associated, automatically associated with the default network ACL. You can associate a network ACL with multiple subnets; however, a subnet can be associated with only one network ACL at a time. A network ACL contains a numbered list of rules that is evaluated in order, starting with the lowest numbered rule, to determine whether traffic is allowed in or out of any subnet associated with the network ACL. The default network ACL is configured to allow all traffic to flow in and out of the subnets to which it is associated.   Amazon security groups and network ACLs don’t filter traffic to or from link-local addresses or AWS-reserved IPv4 addresses. Flow logs do not capture IP traffic to or from these addresses.   Security Group vs NetACL      Security group operates at the instance level while NetACL at the subnet level   Security group support allow rules only while ACL allows rules and deny rules   Security group is stateful, return traffic is automatcially allowed, regardless of any rules.  While ACL is stateless, return traffic must be explicitely allowed by rules   Security group evalaute all rules before deciding whether to allow traffic, while ACL process rules in number order when deciding whether to allow traffic.   Security group appiles to an insatnce only while ACL appies to all instances in teh subect it’s associated with.   VPC NetACL  A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC.   Network ACL Rules are evaluated by rule number, from lowest to highest, and executed immediately when a matching allow/deny rule is founds.   Internet access  To enable access to or from the Internet for instances in a VPC subnet, you must do the following:     Attach an Internet Gateway to your VPC   Ensure that your subnet’s route table points to the Internet Gateway.   Ensure that instances in your subnet have a globally unique IP address (public IPv4 address, Elastic IP address, or IPv6 address).   Ensure that your network access control and security group rules allow the relevant traffic to flow to and from your instance   NAT  Enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating connections with the instances. NAT Gateways You must specify the public subnet in which the NAT gateway should reside. You must specify an Elastic IP address to associate with the NAT gateway when you create it.   Accessing a Corporate or Home Network  You can optionally connect your VPC to your own corporate data center using an IPsec AWS managed VPN connection, making the AWS Cloud an extension of your data center. A VPN connection consists of:     a virtual private gateway (which is the VPN concentrator on the Amazon side of the VPN connection) attached to your VPC.   a customer gateway (which is a physical device or software appliance on your side of the VPN connection) located in your data center.   By default, instances that you launch into a virtual private cloud (VPC) can’t communicate with your own network. You can enable access to your network from your VPC by attaching a virtual private gateway to the VPC, creating a custom route table, updating your security group rules, and creating an AWS managed VPN connection.   Although the term VPN connection is a general term, in the Amazon VPC documentation, a VPN connection refers to the connection between your VPC and your own network. AWS supports Internet Protocol security (IPsec) VPN connections.   A customer gateway is a physical device or software application on your side of the VPN connection.   To create a VPN connection, you must create a customer gateway resource in AWS, which provides information to AWS about your customer gateway device. Next, you have to set up an Internet-routable IP address (static) of the customer gateway’s external interface.   The following diagram illustrates single VPN connections. The VPC has an attached virtual private gateway, and your remote network includes a customer gateway, which you must configure to enable the VPN connection. You set up the routing so that any traffic from the VPC bound for your network is routed to the virtual private gateway.   Site-to-Site VPN  With AWS Site-to-Site VPN, you can connect to an Amazon VPC in the cloud the same way you connect to your branches. AWS Site-to-Site VPN establishes secure and private sessions with IP Security (IPSec) and Transport Layer Security (TLS) tunnels. a VPN connection is that you will be able to connect your Amazon VPC to other remote networks securely.  Although it is true that a VPN provides a cost-effective, hybrid connection from your VPC to your on-premises data centers, it certainly does not bypasses the public Internet. A VPN connection actually goes through the public Internet, unlike the AWS Direct Connect connection which has a direct and dedicated connection to your on-premises network.   AWS Direct Connect  AWS Direct Connect connection which has a direct and dedicated connection to your on-premises network.   AWS PrivateLink  It enables you to privately connect your VPC to supported AWS services, services hosted by other AWS accounts (VPC endpoint services), and supported AWS Marketplace partner services. You do not require an internet gateway, NAT device, public IP address, AWS Direct Connect connection, or VPN connection to communicate with the service. Traffic between your VPC and the service does not leave the Amazon network.   You can create a VPC peering connection between your VPCs, or with a VPC in another AWS account, and enable routing of traffic between the VPCs using private IP addresses. You cannot create a VPC peering connection between VPCs that have overlapping CIDR blocks.   Applications in an Amazon VPC can securely access AWS PrivateLink endpoints across VPC peering connections. The support of VPC peering by AWS PrivateLink makes it possible for customers to privately connect to a service even if that service’s endpoint resides in a different Amazon VPC that is connected using VPC peering. AWS PrivateLink endpoints can now be accessed across both intra- and inter-region VPC peering connections.   HA for VPN  You can do the following to provide a highly available, fault-tolerant network connection:      Establish a hardware VPN over the Internet between the VPC and the on-premises network.   Establish another AWS Direct Connect connection and private virtual interface in the same AWS region.   Q&amp;A  First, the Network ACL should be properly set to allow communication between the two subnets. The security group should also be properly configured so that your web server can communicate with the database server. Hence, options 1 and 4 are the correct answers:   Check if all security groups are set to allow the application host to communicate to the database on the right port and protocol. Check the Network ACL if it allows communication between the two subnets.   Option 2 is incorrect because the EC2 instances do not need to be of the same class in order to communicate with each other.   Option 3 is incorrect because an Internet gateway is primarily used to communicate to the Internet.   Option 5 is incorrect because Placement Group is mainly used to provide low-latency network performance necessary for tightly-coupled node-to-node communication.   AWS WAF   AWS WAF is a web application firewall that helps protect web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. You can use AWS WAF to define customizable web security rules that control which traffic accesses your web applications. If you use AWS Shield Advanced, you can use AWS WAF at no extra cost for those protected resources and can engage the DRT to create WAF rules.   AWS WAF rules use conditions to target specific requests and trigger an action, allowing you to identify and block common DDoS request patterns and effectively mitigate a DDoS attack. These include size constraint conditions to block a web request based on the length of its query string or request body, and geographic match conditions to implement geo restriction (also known as geoblocking) on requests that originate from specific countries.   AWS SSM (Simple System Manager)   AWS SSM is parameter store.   ELB: Elastic Load Balancing   To automatically distribute incoming application traffic across multiple instances, use Elastic Load Balancing.   For HA, even though our ASG is deployed across 3 AZ, the minimum capacity to be highly available is 2. Finally, we can save costs by reserving these two instances as we know they’ll be up and running at any time   Application Load Balancer vs Network load balancer  Path based routing and host based routing are only available for the Application Load Balancer (ALB). Deploying an NGINX load balancer on EC2 would work but would suffer management and scaling issues. Read more here: https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/   ALB &amp; ASG  Adding the entire CIDR of the ALB would work, but wouldn’t guarantee that only the ALB can access the EC2 instances that are part of the ASG. Here, the right solution is to add a rule on the ASG security group to allow incoming traffic from the security group configured for the ALB.   SNI  support for multiple TLS/SSL certificates on Application Load Balancers (ALB) using Server Name Indication (SNI). You can now host multiple TLS secured applications, each with its own TLS certificate, behind a single load balancer. In order to use SNI, all you need to do is bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client. These new features are provided at no additional charge.   One of our most frequent requests on forums, reddit, and in my e-mail inbox has been to use the Server Name Indication (SNI) extension of TLS to choose a certificate for a client. Since TLS operates at the transport layer, below HTTP, it doesn’t see the hostname requested by a client. SNI works by having the client tell the server “This is the domain I expect to get a certificate for” when it first connects. The server can then choose the correct certificate to respond to the client. All modern web browsers and a large majority of other clients support SNI. In fact, today we see SNI supported by over 99.5% of clients connecting to CloudFront.   RDS  RDS stands for Relational Database Service   Read Replics  RDS Read Replicas for read scalability     Up to 5 Read Replicas   Within AZ, Cross AZ or Cross Region   Replication is ASYNC, so reads are eventually consistent   Replicas can be promoted to their own DB   Applications must update the connection string to leverage read replicas   Amazon RDS Multi-AZ Deployments  Amazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora), so that you can resume database operations as soon as the failover is complete. Since the endpoint for your DB Instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention.   Security  Use security groups to control what IP addresses or Amazon EC2 instances can connect to your databases on a DB instance. Run your DB instance in an Amazon Virtual Private Cloud (VPC) for the greatest possible network access control.   Working with a DB Instance in a VPC  Your VPC must have at least two subnets. These subnets must be in two different Availability Zones in the region where you want to deploy your DB instance. If you want your DB instance in the VPC to be publicly accessible, you must enable the VPC attributes DNS hostnames and DNS resolution.   performs a failover  Amazon RDS automatically performs a failover in the event of any of the following:   Loss of availability in primary Availability Zone Loss of network connectivity to primary Compute unit failure on primary Storage failure on primary   Elastic Cache  IAM Auth is not supported by ElastiCache   Amazon CloudWatch   To monitor basic statistics for your instances and Amazon EBS volumes, use Amazon CloudWatch.   Amazon Web Services. Amazon Elastic Compute Cloud .   Disabling the Termination from the ASG would prevent our ASG to be Elastic and impact our costs. Making a snapshot of the EC2 instance before it gets terminated could work but it’s tedious, not elastic and very expensive, as all we’re interested about are log files. Using AWS Lambda would be extremely hard to use for this task. Here, the natural and by far easiest solution would be to use the CloudWatch Logs agents on the EC2 instances to automatically send log files into CloudWatch, so we can analyze them in the future easily should any problems arise.   Auto terminate  Using Amazon CloudWatch alarm actions, you can create alarms that automatically stop, terminate, reboot, or recover your EC2 instances. You can use the stop or terminate actions to help you save money when you no longer need an instance to be running. You can use the reboot and recover actions to automatically reboot those instances or recover them onto new hardware if a system impairment occurs.   Q&amp;A  you can use Amazon CloudWatch to monitor the database and then Amazon SNS to send the emails to the Operations team. Take note that you should use SNS instead of SES (Simple Email Service) when you want to monitor your EC2 instances.   CloudWatch collects monitoring and operational data in the form of logs, metrics, and events, providing you with a unified view of AWS resources, applications, and services that run on AWS, and on-premises servers.   SNS is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications.   Option 1 is incorrect. SES is a cloud-based email sending service designed to send notification and transactional emails.   Option 3 is incorrect. SQS is a fully-managed message queuing service. It does not monitor applications nor send email notifications unlike SES.   Option 4 is incorrect. Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It does not monitor applications nor send email notifications.   API Gateway  Q: What API types are supported by Amazon API Gateway?   Amazon API Gateway offers two options to create RESTful APIs, HTTP APIs (Preview) and REST APIs, as well as an option to create WebSocket APIs.   HTTP API: HTTP APIs, currently available in Preview, are optimized for building APIs that proxy to AWS Lambda functions or HTTP backends, making them ideal for serverless workloads. They do not currently offer API management functionality.   REST API: REST APIs offer API proxy functionality and API management features in a single solution. REST APIs offer API management features such as usage plans, API keys, publishing, and monetizing APIs.   WebSocket API: WebSocket APIs maintain a persistent connection between connected clients to enable real-time message communication. With WebSocket APIs in API Gateway, you can define backend integrations with AWS Lambda functions, Amazon Kinesis, or any HTTP endpoint to be invoked when messages are received from the connected clients.   CloudTrail   To monitor the calls made to the Amazon EC2 API for your account, including calls made by the AWS Management Console, command line tools, and other services, use AWS CloudTrail.   In general, to analyze any API calls made within your AWS account, you should use CloudTrail   ​ Set up a new CloudTrail trail in a new S3 bucket using the AWS CLI and also pass both the –is-multi-region-trail and –include-global-service-events parameters then encrypt log files using KMS encryption. Apply Multi Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.   Charge  the first copy of management events is free.   Cloud Trail retention days  Management event activity is recorded by AWS CloudTrail for the last 90 days, and can be viewed and searched free of charge from the AWS CloudTrail console, or by using the AWS CLI.   AWS ECS  Summary: AWS ECS – Elastic Container Service  • ECS is a container orchestration service • ECS helps you run Docker containers on EC2 machines • ECS is complicated, and made of: • “ECS Core”: Running ECS on user-provisioned EC2 instances • Fargate: Running ECS tasks on AWS-provisioned compute (serverless)  • EKS: Running ECS on AWS-powered Kubernetes (running on EC2) • ECR: Docker Container Registry hosted by AWS • ECS &amp; Docker are very popular for microservices • For now, for the exam, only “ECS Core” &amp; ECR is in scope  • IAM security and roles at the ECS task level   AWS ECS – Concepts  • ECS cluster: set of EC2 instances • ECS service: applications definitions running on ECS cluster • ECS tasks + definition: containers running to create the application • ECS IAM roles: roles assigned to tasks to interact with AWS   AWS ECS – ALB integration  • Application Load Balancer (ALB) has a direct integration feature with ECS called “port mapping”, This allows you to run multiple instances of the same application on the same EC2 machine   Dynamic mapping  Dynamic Port Mapping is available for the Application Load Balancer. A reverse proxy solution would work but would be too much work to manage. Here the ALB has a feature that provides a direct dynamic port mapping feature and integration with the ECS service so we will leverage that. Read more here: https://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/   AWS ECS – ECS Setup &amp; Config file  • Run an EC2 instance, install the ECS agent with ECS config file • Or use an ECS-ready Linux AMI (still need to modify config file) • ECS Config file is at /etc/ecs/ecs.config   AWS ECR – Elastic Container Registry  • Store, managed and deploy your containers on AWS • Fully integrated with IAM &amp; ECS • Sent over HTTPS (encryption in flight) and encrypted at rest   Specifying Sensitive Data  Amazon ECS enables you to inject sensitive data into your containers by storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters and then referencing them in your container definition. This feature is supported by tasks using both the EC2 and Fargate launch types.   Required IAM Permissions for Amazon ECS Secrets   To use this feature, you must have the Amazon ECS task execution role and reference it in your task definition. This allows the container agent to pull the necessary AWS Systems Manager or Secrets Manager resources. For more information, see Amazon ECS Task Execution IAM Role.   To provide access to the AWS Systems Manager Parameter Store parameters that you create, manually add the following permissions as an inline policy to the task execution role. For more information, see Adding and Removing IAM Policies.   ssm:GetParameters—Required if you are referencing a Systems Manager Parameter Store parameter in a task definition.   secretsmanager:GetSecretValue—Required if you are referencing a Secrets Manager secret either directly or if your Systems Manager Parameter Store parameter is referencing a Secrets Manager secret in a task definition.   kms:Decrypt—Required only if your secret uses a custom KMS key and not the default key. The ARN for your custom key should be added as a resource.   Lambda  AWS Lambda functions time out after 15 minutes, and are not usually meant for long running jobs.   Lambda parameters Encryption  Although Lambda encrypts the environment variables in your function by default, the sensitive information would still be visible to other users who have access to the Lambda console. This is because Lambda uses a default KMS key to encrypt the variables, which is usually accessible by other users. The best option in this scenario is to use encryption helpers to secure your environment variables.   Enabling SSL would encrypt data only when in-transit. Your other teams would still be able to view the plaintext at-rest. Use AWS KMS instead.   Option 3 is incorrect since, as mentioned, Lambda does provide encryption functionality of environment variables.  Lambda functions  You upload your application code in the form of one or more Lambda functions. Lambda stores code in Amazon S3 and encrypts it at rest.  layer  A layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. Use layers to manage your function’s dependencies independently and keep your deployment package small.   Invoking Functions   Lambda supports synchronous and asynchronous invocation of a Lambda function. You can control the invocation type only when you invoke a Lambda function (referred to as on-demand invocation). An event source is the entity that publishes events, and a Lambda function is the custom code that processes the events. Event source mapping maps an event source to a Lambda function. It enables automatic invocation of your Lambda function when events occur.   Lambda deployment  If you’re using the AWS Lambda compute platform, you must choose one of the following deployment configuration types to specify how traffic is shifted from the original AWS Lambda function version to the new AWS Lambda function version:   -Canary: Traffic is shifted in two increments. You can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function version in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment. -Linear: Traffic is shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic shifted in each increment and the number of minutes between each increment. -All-at-once: All traffic is shifted from the original Lambda function to the updated Lambda function version at once.   Lambda@Edge  Lets you run Lambda functions to customize content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. The functions run in response to CloudFront events, without provisioning or managing servers.   Disaster Recovery  Overview  Need to define two terms: • RPO: Recovery Point Objective • RTO: Recover y Time Objective   Disaster Recovery Strategies • Backup and Restore • Pilot Light • Warm Standby • Hot Site / Multi Site Approach   Route53  Simple Records do not have health checks, here the most likely issue is that the TTL is still in effect so you have to wait until it expires for the new users to perform another DNS query and get the value for your new Load Balancer.   Database   ElastiCache / RDS / Neptune are not serverless databases. DynamoDB is serverless, single digit latency and horizontally scales.   DynamoDB  DynamoDB Streams will contain a stream of all the changes that happen to a DynamoDB table. It can be chained with a Lambda function that will be triggered to react to these changes, one of which being a developer’s milestone. DAX is a caching layer   DynamoDB Auto Scaling is primarily used to automate capacity management for your tables and global secondary indexes.   DAX  DAX will be transparent and won’t require an application refactoring, and will cache the “hot keys”. ElastiCache could also be a solution, but it will require a lot of refactoring work on the AWS Lambda side.   DynamoDB is horizontally scalable, has a DynamoDB streams capability and is multi AZ by default. On top of it, we can adjust the RCU and WCU automatically using Auto Scaling.   DynamoDB’s partition key   DynamoDB supports two types of primary keys:   Partition key: A simple primary key, composed of one attribute known as the partition key. Attributes in DynamoDB are similar in many ways to fields or columns in other database systems. Partition key and sort key: Referred to as a composite primary key, this type of key is composed of two attributes. The first attribute is the partition key, and the second attribute is the sort key. Following is an example.   Recommendations for partition keys   Use high-cardinality attributes. These are attributes that have distinct values for each item, like e-mailid, employee_no, customerid, sessionid, orderid, and so on.   Use composite attributes. Try to combine more than one attribute to form a unique key, if that meets your access pattern. For example, consider an orders table with customerid+productid+countrycode as the partition key and order_date as the sort key.   Cache the popular items when there is a high volume of read traffic using Amazon DynamoDB Accelerator (DAX). The cache acts as a low-pass filter, preventing reads of unusually popular items from swamping partitions. For example, consider a table that has deals information for products. Some deals are expected to be more popular than others during major sale events like Black Friday or Cyber Monday. DAX is a fully managed, in-memory cache for DynamoDB that doesn’t require developers to manage cache invalidation, data population, or cluster management. DAX also is compatible with DynamoDB API calls, so developers can incorporate it more easily into existing applications.   Add random numbers or digits from a predetermined range for write-heavy use cases. Suppose that you expect a large volume of writes for a partition key (for example, greater than 1000 1 K writes per second). In this case, use an additional prefix or suffix (a fixed number from predetermined range, say 1–10) and add it to the partition key.   Durability  When the word durability pops out, the first service that should come to your mind is Amazon S3. Since this service is not available in the answer options, we can look at the other data store available which is Amazon DynamoDB.   DynamoDB is durable, scalable, and highly available data store which can be used for real-time tabulation. You can also use AppSync with DynamoDB to make it easy for you to build collaborative apps that keep shared data updated in real time. You just specify the data for your app with simple code statements and AWS AppSync manages everything needed to keep the app data updated in real time. This will allow your app to access data in Amazon DynamoDB, trigger AWS Lambda functions, or run Amazon Elasticsearch queries and combine data from these services to provide the exact data you need for your app.   Option 2 is incorrect as Amazon Redshift is mainly used as a data warehouse and for online analytic processing (OLAP). Although this service can be used for this scenario, DynamoDB is still the top choice given its better durability and scalability.   Options 3 and 4 are possible answers in this scenario, however, DynamoDB is much more suitable for simple mobile apps which do not have complicated data relationships compared with enterprise web applications. The scenario says that the mobile app will be used from around the world, which is why you need a data storage service which can be supported globally. It would be a management overhead to implement multi-region deployment for your RDS and Aurora database instances compared to using the Global table feature of DynamoDB.   Aurora   Aurora Read Replicas can be deployed globally   Aurora endpoints  Amazon Aurora typically involves a cluster of DB instances instead of a single instance. Each connection is handled by a specific DB instance. When you connect to an Aurora cluster, the host name and port that you specify point to an intermediate handler called an endpoint. Aurora uses the endpoint mechanism to abstract these connections. Thus, you don’t have to hardcode all the hostnames or write your own logic for load-balancing and rerouting connections when some DB instances aren’t available.   For certain Aurora tasks, different instances or groups of instances perform different roles. For example, the primary instance handles all data definition language (DDL) and data manipulation language (DML) statements. Up to 15 Aurora Replicas handle read-only query traffic.   Using endpoints, you can map each connection to the appropriate instance or group of instances based on your use case. For example, to perform DDL statements you can connect to whichever instance is the primary instance. To perform queries, you can connect to the reader endpoint, with Aurora automatically performing load-balancing among all the Aurora Replicas. For clusters with DB instances of different capacities or configurations, you can connect to custom endpoints associated with different subsets of DB instances. For diagnosis or tuning, you can connect to a specific instance endpoint to examine details about a specific DB instance.   Types of Aurora Endpoints   An endpoint is represented as an Aurora-specific URL that contains a host address and a port. The following types of endpoints are available from an Aurora DB cluster.   Cluster endpoint  A cluster endpoint for an Aurora DB cluster that connects to the current primary DB instance for that DB cluster. This endpoint is the only one that can perform write operations such as DDL statements. Because of this, the cluster endpoint is the one that you connect to when you first set up a cluster or when your cluster only contains a single DB instance. Each Aurora DB cluster has one cluster endpoint and one primary DB instance.   You use the cluster endpoint for all write operations on the DB cluster, including inserts, updates, deletes, and DDL changes. You can also use the cluster endpoint for read operations, such as queries.   The cluster endpoint provides failover support for read/write connections to the DB cluster. If the current primary DB instance of a DB cluster fails, Aurora automatically fails over to a new primary DB instance. During a failover, the DB cluster continues to serve connection requests to the cluster endpoint from the new primary DB instance, with minimal interruption of service.   Reader endpoint  A reader endpoint for an Aurora DB cluster connects to one of the available Aurora Replicas for that DB cluster. Each Aurora DB cluster has one reader endpoint. If there is more than one Aurora Replica, the reader endpoint directs each connection request to one of the Aurora Replicas.   The reader endpoint provides load-balancing support for read-only connections to the DB cluster. Use the reader endpoint for read operations, such as queries. You can’t use the reader endpoint for write operations.   Custom endpoint  A custom endpoint for an Aurora cluster represents a set of DB instances that you choose. When you connect to the endpoint, Aurora performs load balancing and chooses one of the instances in the group to handle the connection. You define which instances this endpoint refers to, and you decide what purpose the endpoint serves.   An Aurora DB cluster has no custom endpoints until you create one. You can create up to five custom endpoints for each provisioned Aurora cluster. You can’t use custom endpoints for Aurora Serverless clusters.   The custom endpoint provides load-balanced database connections based on criteria other than the read-only or read-write capability of the DB instances. For example, you might define a custom endpoint to connect to instances that use a particular AWS instance class or a particular DB parameter group. Then you might tell particular groups of users about this custom endpoint. For example, you might direct internal users to low-capacity instances for report generation or ad hoc (one-time) querying, and direct production traffic to high-capacity instances.   Instead of using one DB instance for each specialized purpose and connecting to its instance endpoint, you can have multiple groups of specialized DB instances. In this case, each group has its own custom endpoint. This way, Aurora can perform load balancing among all the instances dedicated to tasks such as reporting or handling production or internal queries. The custom endpoints provide load balancing and high availability for each group of DB instances within your cluster. If one of the DB instances within a group becomes unavailable, Aurora directs subsequent custom endpoint connections to one of the other DB instances associated with the same endpoint.   Instance endpoint  An instance endpoint connects to a specific DB instance within an Aurora cluster. Each DB instance in a DB cluster has its own unique instance endpoint. So there is one instance endpoint for the current primary DB instance of the DB cluster, and there is one instance endpoint for each of the Aurora Replicas in the DB cluster.   The instance endpoint provides direct control over connections to the DB cluster, for scenarios where using the cluster endpoint or reader endpoint might not be appropriate. For example, your client application might require more fine-grained load balancing based on workload type. In this case, you can configure multiple clients to connect to different Aurora Replicas in a DB cluster to distribute read workloads.   Kinesis  Amazon Kinesis is the streaming data platform of AWS and has four distinct services under it: Kinesis Data Firehose, Kinesis Data Streams, Kinesis Video Streams, and Amazon Kinesis Data Analytics. For a specific use case of the requirement by the question, use Kinesis Data Firehose.   SQS FIFO will not work here as they cannot sustain thousands of messages per second. SNS cannot be used for data streaming. Lambda isn’t meant to retain data. Kinesis is the right answer here, with providing a partition key in our message we can guarantee ordering for a specific sensor, even if our stream is sharded   Amazon Macie  Amazon Macie is mainly used as a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. As a security feature of AWS, it does not meet the requirements of being able to load and stream data into data stores for analytics. You have to use Kinesis Data Firehose instead.   Concepts   Data Producer – An application that typically emits data records as they are generated to a Kinesis data stream. Data producers assign partition keys to records. Partition keys ultimately determine which shard ingests the data record for a data stream.  Data Consumer – A distributed Kinesis application or AWS service retrieving data from all shards in a stream as it is generated. Most data consumers are retrieving the most recent data in a shard, enabling real-time analytics or handling of data.  Data Stream – A logical grouping of shards. There are no bounds on the number of shards within a data stream. A data stream will retain data for 24 hours, or up to 7 days when extended retention is enabled.  Shard – The base throughput unit of a Kinesis data stream.  A shard is an append-only log and a unit of streaming capability. A shard contains an ordered sequence of records ordered by arrival time. Add or remove shards from your stream dynamically as your data throughput changes. One shard can ingest up to 1000 data records per second, or 1MB/sec. Add more shards to increase your ingestion capability. When consumers use enhanced fan-out, one shard provides 1MB/sec data input and 2MB/sec data output for each data consumer registered to use enhanced fan-out. When consumers do not use enhanced fan-out, a shard provides 1MB/sec of input and 2MB/sec of data output, and this output is shared with any consumer not using enhanced fan-out. You will specify the number of shards needed when you create a stream and can change the quantity at any time.  Data Record  A record is the unit of data stored in a Kinesis stream. A record is composed of a sequence number, partition key, and data blob. A data blob is the data of interest your data producer adds to a stream. The maximum size of a data blob is 1 MB.  Partition Key  A partition key is typically a meaningful identifier, such as a user ID or timestamp. It is specified by your data producer while putting data into a Kinesis data stream, and useful for consumers as they can use the partition key to replay or build a history associated with the partition key. The partition key is also used to segregate and route data records to different shards of a stream.  Sequence Number  A sequence number is a unique identifier for each data record. Sequence number is assigned by Kinesis Data Streams when a data producer calls PutRecord or PutRecords API to add data to a Kinesis data stream.   Monitoring  You can monitor shard-level metrics in Kinesis Data Streams. You can monitor your data streams in Amazon Kinesis Data Streams using CloudWatch, Kinesis Agent, Kinesis libraries. Log API calls with CloudTrail.   Kinesis firehose  You can specify a batch size or batch interval to control how quickly data is uploaded to destinations. Additionally, you can specify if data should be compressed.   You can configure Kinesis Data Firehose to prepare your streaming data before it is loaded to data stores. Kinesis Data Firehose provides pre-built Lambda blueprints for converting common data sources such as Apache logs and system logs to JSON and CSV formats. You can use these pre-built blueprints without any change, or customize them further, or write your own custom functions.   BeanStalk  When you create an AWS Elastic Beanstalk environment, you can specify an Amazon Machine Image (AMI) to use instead of the standard Elastic Beanstalk AMI included in your platform version. A custom AMI can improve provisioning times when instances are launched in your environment if you need to install a lot of software that isn’t included in the standard AMIs. Read more here: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.customenv.html   MQ  SNS, SQS and Kinesis are AWS’ proprietary technologies and do not come with MQTT compatibility. Here the only possible answer is Amazon MQ   X Ray  AWS X-Ray Analyze and debug production, distributed applications   AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components. You can use X-Ray to analyze both applications in development and in production, from simple three-tier applications to complex microservices applications consisting of thousands of services.   ElasticCache   Redis   What are Amazon ElastiCache for Redis nodes, clusters, and replications groups?   An ElastiCache for Redis node is the smallest building block of an Amazon ElastiCache for Redis deployment. Each ElastiCache for Redis node supports the Redis protocol and has its own DNS name and port. Multiple types of ElastiCache for Redis nodes are supported, each with varying amount of CPU capability, and associated memory. An ElastiCache for Redis node may take on a primary or a read replica role. A primary node can be replicated to multiple read replica nodes. An ElastiCache for Redis cluster is a collection of one or more ElastiCache for Redis nodes of the same role; the primary node will be in the primary cluster and the read replica node will be in a read replica cluster. At this time a cluster can only have one node. In the future, we will increase this limit. A cluster manages a logical key space, where each node is responsible for a part of the key space. Most of your management operations will be performed at the cluster level. An ElastiCache for Redis replication group encapsulates the primary and read replica clusters for a Redis installation. A replication group will have only one primary cluster and zero or many read replica clusters. All nodes within a replication group (and consequently cluster) will be of the same node type, and have the same parameter and security group settings.   Does Amazon ElastiCache for Redis support Multi-AZ operation?   Yes, with Amazon ElastiCache for Redis you can create a read replica in another AWS Availability Zone. Upon a failure of the primary node, we will provision a new primary node. In scenarios where the primary node cannot be provisioned, you can decide which read replica to promote to be the new primary.   What is Multi-AZ for an ElastiCache for Redis replication group?   An ElastiCache for Redis replication group consists of a primary and up to five read replicas. Redis asynchronously replicates the data from the primary to the read replicas. During certain types of planned maintenance, or in the unlikely event of ElastiCache node failure or Availability Zone failure, Amazon ElastiCache will automatically detect the failure of a primary, select a read replica, and promote it to become the new primary. ElastiCache also propagates the DNS changes of the promoted read replica, so if your application is writing to the primary node endpoint, no endpoint change will be needed.   How can I use encryption in-transit, at-rest, and Redis AUTH?   Encryption in-transit, encryption at-rest, and Redis AUTH are all opt-in features. At the time of Redis cluster creation via the console or command line interface, you can specify if you want to enable encryption and Redis AUTH and can proceed to provide an authentication token for communication with the Redis cluster. Once the cluster is setup with encryption enabled, ElastiCache seamlessly manages certificate expiration and renewal without requiring any additional action from the application. Additionally, the Redis clients need to support TLS to avail of the encrypted in-transit traffic.   Is Redis password functionality supported in Amazon ElastiCache for Redis?   Yes, Amazon ElastiCache for Redis supports Redis passwords via Redis AUTH feature. It is an opt-in feature available in ElastiCache for Redis version 3.2.6 onwards. You must enable encryption in-transit to use Redis AUTH on your ElastiCache for Redis cluster.   encryption  ElastiCache for Redis at-rest encryption is an optional feature to increase data security by encrypting on-disk data. When enabled on a replication group, it encrypts the following aspects:   Disk during sync, backup and swap operations   Backups stored in Amazon S3   ElastiCache for Redis offers default (service managed) encryption at rest, as well as ability to use your own symetric customer managed customer master keys in AWS Key Management Service (KMS).   KMS  Manage keys used for encrypted DB instances using the AWS KMS. KMS encryption keys are specific to the region that they are created in.   BYOIP  You can bring part or all of your public IPv4 address range from your on-premises network to your AWS account. You continue to own the address range, but AWS advertises it on the Internet. After you bring the address range to AWS, it appears in your account as an address pool. You can create an Elastic IP address from your address pool and use it with your AWS resources, such as EC2 instances, NAT gateways, and Network Load Balancers. This is also called “Bring Your Own IP Addresses (BYOIP)”.   To ensure that only you can bring your address range to your AWS account, you must authorize Amazon to advertise the address range and provide proof that you own the address range.   A Route Origin Authorization (ROA) is a document that you can create through your Regional internet registry (RIR), such as the American Registry for Internet Numbers (ARIN) or Réseaux IP Européens Network Coordination Centre (RIPE). It contains the address range, the ASNs that are allowed to advertise the address range, and an expiration date. Hence, Option 3 is the correct answer.   The ROA authorizes Amazon to advertise an address range under a specific AS number. However, it does not authorize your AWS account to bring the address range to AWS. To authorize your AWS account to bring an address range to AWS, you must publish a self-signed X509 certificate in the RDAP remarks for the address range. The certificate contains a public key, which AWS uses to verify the authorization-context signature that you provide. You should keep your private key secure and use it to sign the authorization-context message.   Option 1 is incorrect because you cannot map the IP address of your on-premises network, which you are migrating to AWS, to an EIP address of your VPC. To satisfy the requirement, you must authorize Amazon to advertise the address range that you own.   Option 2 is incorrect because the IP match condition in CloudFront is primarily used in allowing or blocking the incoming web requests based on the IP addresses that the requests originate from. This is the opposite of what is being asked in the scenario, where you have to migrate your suite of applications from your on-premises network and advertise the address range that you own in your VPC.   Option 4 is incorrect because you don’t need to submit an AWS request in order to do this. You can simply create a Route Origin Authorization (ROA) then once done, provision and advertise your whitelisted IP address range to your AWS account.   AWS IoT Core  It is a managed cloud service that lets connected devices easily and securely interact with cloud applications and other devices. AWS IoT Core provides secure communication and data processing across different kinds of connected devices and locations so you can easily build IoT applications.   SQS  You cannot set a priority to individual items in the SQS queue. In this scenario, it is stated that the SQS queue is configured with the maximum message retention period. The maximum message retention in SQS is 14 days, there will be no missing messages.  The queue can contain an unlimited number of messages, not just 10,000 messages.   In Amazon SQS, you can configure the message retention period to a value from 1 minute to 14 days. The default is 4 days. Once the message retention limit is reached, your messages are automatically deleted.   A single Amazon SQS message queue can contain an unlimited number of messages. However, there is a 120,000 limit for the number of inflight messages for a standard queue and 20,000 for a FIFO queue. Messages are inflight after they have been received from the queue by a consuming component, but have not yet been deleted from the queue.   SWF   SWF workflow defines all the activities in the workflow.   The purpose of a decision task tells the decider the state of the workflow execution. The decider can be viewed as a special type of worker. Like workers, it can be written in any language and asks Amazon SWF for tasks. However, it handles special tasks called decision tasks.   Amazon SWF issues decision tasks whenever a workflow execution has transitions such as an activity task completing or timing out. A decision task contains information on the inputs, outputs, and current state of previously initiated activity tasks. Your decider uses this data to decide the next steps, including any new activity tasks, and returns those to Amazon SWF. Amazon SWF in turn enacts these decisions, initiating new activity tasks where appropriate and monitoring them.   By responding to decision tasks in an ongoing manner, the decider controls the order, timing, and concurrency of activity tasks and consequently the execution of processing steps in the application. SWF issues the first decision task when an execution starts. From there on, Amazon SWF enacts the decisions made by your decider to drive your execution. The execution continues until your decider makes a decision to complete it.   An activity task tells the worker to perform a function   A single task in the workflow represents a single task in the workflow.   Distributed systems  Amazon Simple Queue Service (SQS) and Amazon Simple Workflow Service (SWF) are the services that you can use for creating a decoupled architecture in AWS. Decoupled architecture is a type of computing architecture that enables computing components or layers to execute independently while still interfacing with each other.   Amazon SQS offers reliable, highly-scalable hosted queues for storing messages while they travel between applications or microservices. Amazon SQS lets you move data between distributed application components and helps you decouple these components. Amazon SWF is a web service that makes it easy to coordinate work across distributed application components.   AppSync  Power your applications with the right data, from one or more data sources, at global scale AWS AppSync simplifies application development by letting you create a flexible API to securely access, manipulate, and combine data from one or more data sources. AppSync is a managed service that uses GraphQL to make it easy for applications to get exactly the data they need.   With AppSync, you can build scalable applications, including those requiring real-time updates, on a range of data sources such as NoSQL data stores, relational databases, HTTP APIs, and your custom data sources with AWS Lambda. For mobile and web apps, AppSync additionally provides local data access when devices go offline, and data synchronization with customizable conflict resolution, when they are back online.   Benefits Start effortlessly; scale with your business Get started in minutes directly from your IDE of choice (such as Xcode, Android Studio, VS Code), leverage the intuitive AWS AppSync management console, or use AWS Amplify CLI to automatically generate your API and client-side code. AWS AppSync integrates with Amazon DynamoDB, Amazon Aurora, Amazon Elasticsearch, AWS Lambda, and other AWS services, enabling you to create sophisticated applications, with virtually unlimited throughput and storage, that scale according to your business needs.   Real-time subscriptions and offline access AWS AppSync enables real-time subscriptions across millions of devices, as well as offline access to app data. When an offline device reconnects, AWS AppSync automatically syncs only the updates that occurred when the device was disconnected, and not the entire data set. AWS AppSync offers user-customizable server-side conflict detection and resolution that does the heavy lifting of managing data conflicts so you don’t have to.   Unify and secure access to your distributed data Perform complex queries and aggregation across multiple data sources with a single network call using GraphQL. AWS AppSync makes it easy to secure your app data using multiple concurrent authentication modes as well as allowing to define security and fine grained access control at the data definition level directly from your GraphQL schema.   Make Mobile Development Easier with Open Source Libraries for iOS and Android Build real-time, global mobile apps that can handle millions of requests per second using Amplify libraries. Read the blog  How it works product-page-diagram_AppSync@2x AWS AppSync is generally available. If you would like try building data driven mobile and web applications, watch the re:Invent session video to learn more and open the AWS AppSync console to get started. For pricing details, please see the pricing page. AWS AppSync is available in multiple regions. For details on region availability, please see the regions detail page.   AWS AppSync re:Invent session Customers using AWS AppSync 600x400_AmericanCommBargeLine 600x400_Puresec 600x400_IDT 600x400_ASU 600x400_PublicGood 600x400_cookpad 600x400_ALDO ticketmaster-clear-migration Use cases Real Time Collaboration Data Broadcasting You can use AWS AppSync to enable scalable, real-time collaboration use cases by broadcasting data from the backend to all connected clients (one-to-many) or broadcasting data between clients themselves (many-to-many). For example, you can build a second screen scenario where you broadcast the same data to all clients, and users then respond in real time by voting and commenting about what’s on the screen. Reference Architecture: Sample Code   product-page-diagram_AppSync_Data-Broadcasting@2x Chat Applications You can use AWS AppSync to power collaborative and conversational applications. For example, you can build a mobile and web application that supports multiple private chat rooms, offers access to conversation history, and enqueues outbound messages, even when the device is offline.   Reference Architecture: Sample Code   Product-Page-Diagram_AppSync_Chat-Applications_2@2x Internet of Things You can use AWS AppSync to access IoT device data sent to AWS IoT. For instance, you can build a real-time dashboard in a mobile or web application to visualize telemetry from a connected car. Product-Page-Diagram_AppSync_IoT@2x Data Layer Polyglot Backend Data Access You can retrieve or modify data from multiple data sources (SQL databases in Amazon Aurora Serverless, NoSQL tables in Amazon DynamoDB, search data in Amazon Elasticsearch Service, REST endpoints in Amazon API Gateway, or serverless backends in AWS Lambda) with a single call. Query and create relations between data sources using GraphQL connections. Provide real-time and offline capabilities to web and mobile clients.   Product-Page-Diagram_AppSync_Polyglot-Back-end-Data-Access@2x Microservices Access Layer You can use AWS AppSync as a single interface to access and combine data from multiple microservices in your application, even if they’re running in different environments such as containers in a VPC, behind a REST API on Amazon API Gateway, or behind a GraphQL API on another AWS AppSync endpoint. Product-Page-Diagram_AppSync_Microservices-Aggregation@2x Offline Offline Delta Sync You can use AppSync with Amplify DataStore, an on-device persistent storage engine that automatically synchronizes data between mobile/web apps and the cloud using GraphQL with a local-first and familiar programming model, leveraging AWS AppSync built-in support for data versioning with advanced conflict detection and resolution strategies such as auto-merge, optimistic concurrency or custom resolution with your own Lambda functions.   Q&amp;A      Lambda would time out after 15 minutes (2000*3=6000 seconds = 100 minutes). Glue is for performing ETL, but cannot run custom Python scripts. Kinesis Streams is for real time data (here we are in a batch setup), RDS could be used to run SQL queries on the data, but no Python script. The correct answer is EC2   Elastic Beanstalk cannot manage AWS Lambda functions, OpsWorks is for Chef / Puppet, and Trusted Advisor is to get recommendations regarding the 5 pillars of the well architected framework.   Kinesis cannot scale infinitely and we may have the same throughput issues. SNS won’t keep our data if it cannot be delivered, and DAX is used for caching reads, not to help with writes. Here, using SQS as a middleware will help us sustain the write throughput during write peaks   CloudFront is not a good solution here as the content is highly dynamic, and CloudFront will cache things. Dynamic applications cannot be deployed to S3, and Route53 does not help in scaling your application. ASG is the correct answer here   Network Load Balancers expose a fixed IP to the public web, therefore allowing your application to be predictably reached using these IPs, while allowing you to scale your application behind the Network Load Balancer using an ASG. Application and Classic Load Balancers expose a fixed DNS (=URL). Finally, the ASG does not have a dynamic Elastic IPs attachment feature   Hosting the master pack into S3 will require an application code refactor. Upgrading the EC2 instances won’t help save network cost and ELB does not have any caching capability. Here you need to create a CloudFront distribution to add a caching layer in front of your ELB. That caching layer will be very effective as the image pack is a static file, and tremendously save in network cost.   Adding Aurora Read Replicas would greatly increase the cost, switching to a Load Balancer would not improve the problems, and AWS Lambda has no native in memory caching capability. Here, using API Gateway Caching feature is the answer, as we can accept to serve stale data to our users   SQS will allow you to buffer the image compression requests and process them asynchronously. It also has a direct built-in mechanism for retries and scales seamlessly. To process these jobs, due to the unpredictable nature of their volume, and the desire to save on costs, Spot Instances are recommended.   Distributing the static content through S3 allows to offload most of the network usage to S3 and free up our applications running on ECS. EFS will not change anything as static content on EFS would still have to be distributed by our ECS instances   S3 does not work as overwrites are eventually consistent so the latest data will not always be read. Neptune is a graph database so it’s not a good fit, ElastiCache could work but it’s a better fit as a caching technology to enhance reads. Here, the best fit is RDS.        RDS Multi AZ helps with disaster recovery in case of an AZ failure. ElastiCache would definitely help with the read load, but would require a refactor of the application’s core logic. DynamoDB with DAX would also probably help with the read load, but once again it would require a refactor of the application’s core logic. Here, our only option to scale reads is to use RDS Read Replicas       Which of the following AWS services encrypts data at rest by default? (Choose 2) All data transferred between any type of gateway appliance and AWS storage is encrypted using SSL. By default, all data stored by AWS Storage Gateway in S3 is encrypted server-side with Amazon S3-Managed Encryption Keys (SSE-S3). Also, when using the file gateway, you can optionally configure each file share to have your objects encrypted with AWS KMS-Managed Keys using SSE-KMS. This is the reason why Option 1 is correct.   Data stored in Amazon Glacier is protected by default; only vault owners have access to the Amazon Glacier resources they create. Amazon Glacier encrypts your data at rest by default and supports secure data transit with SSL. This is the reason why Option 4 is correct.   Although Amazon RDS, ECS and Lambda all support encryption, you still have to enable and configure them first with tools like AWS KMS to encrypt the data at rest.   Resources     RDS cheatsheet: https://tutorialsdojo.com/aws-cheat-sheet-amazon-relational-database-service-amazon-rds/   VPC cheatsheet: https://tutorialsdojo.com/aws-cheat-sheet-amazon-vpc/   Amazon Kinesis cheatsheet: https://tutorialsdojo.com/aws-cheat-sheet-amazon-kinesis/  ","categories": [],
        "tags": ["AWS","Cloud"],
        "url": "/2019/08/25/AWS-Certificate.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Kafka In Spring",
        "excerpt":"Enable Kafka listener annotated endpoints that are created under the covers by a AbstractListenerContainerFactory. To be used on Configuration classes as follows:    @Configuration    @EnableKafka    public class AppConfig {    \t@Bean    \tpublic ConcurrentKafkaListenerContainerFactory myKafkaListenerContainerFactory() {    \t\tConcurrentKafkaListenerContainerFactory factory = new ConcurrentKafkaListenerContainerFactory();    \t\tfactory.setConsumerFactory(consumerFactory());    \t\tfactory.setConcurrency(4);    \t\treturn factory;    \t}    \t// other @Bean definitions    }   The KafkaListenerContainerFactory is responsible to create the listener container for a particular endpoint. Typical implementations, as the ConcurrentKafkaListenerContainerFactory used in the sample above, provides the necessary configuration options that are supported by the underlying MessageListenerContainer. @EnableKafka enables detection of KafkaListener annotations on any Spring-managed bean in the container. For example, given a class MyService:    package com.acme.foo;   public class MyService {    \t@KafkaListener(containerFactory = “myKafkaListenerContainerFactory”, topics = “myTopic”)    \tpublic void process(String msg) {    \t\t// process incoming message    \t}    }   The container factory to use is identified by the containerFactory attribute defining the name of the KafkaListenerContainerFactory bean to use. When none is set a KafkaListenerContainerFactory bean with name kafkaListenerContainerFactory is assumed to be present. the following configuration would ensure that every time a message is received from topic “myQueue”, MyService.process() is called with the content of the message:    @Configuration    @EnableKafka    public class AppConfig {    \t@Bean    \tpublic MyService myService() {    \t\treturn new MyService();    \t}      \t// Kafka infrastructure setup }   Alternatively, if MyService were annotated with @Component, the following configuration would ensure that its @KafkaListener annotated method is invoked with a matching incoming message:    @Configuration    @EnableKafka    @ComponentScan(basePackages = “com.acme.foo”)    public class AppConfig {    }   Note that the created containers are not registered with the application context but can be easily located for management purposes using the KafkaListenerEndpointRegistry. Annotated methods can use a flexible signature; in particular, it is possible to use the Message abstraction and related annotations, see KafkaListener Javadoc for more details. For instance, the following would inject the content of the message and the kafka partition header:    @KafkaListener(containerFactory = “myKafkaListenerContainerFactory”, topics = “myTopic”)    public void process(String msg, @Header(“kafka_partition”) int partition) {    \t// process incoming message    }   These features are abstracted by the MessageHandlerMethodFactory that is responsible to build the necessary invoker to process the annotated method. By default, DefaultMessageHandlerMethodFactory is used. When more control is desired, a @Configuration class may implement KafkaListenerConfigurer. This allows access to the underlying KafkaListenerEndpointRegistrar instance. The following example demonstrates how to specify an explicit default KafkaListenerContainerFactory    {    \t@code    \t@Configuration    \t@EnableKafka    \tpublic class AppConfig implements KafkaListenerConfigurer {    \t\t@Override    \t\tpublic void configureKafkaListeners(KafkaListenerEndpointRegistrar registrar) {    \t\t\tregistrar.setContainerFactory(myKafkaListenerContainerFactory());    \t\t}      \t\t@Bean    \t\tpublic KafkaListenerContainerFactory&lt;?, ?&gt; myKafkaListenerContainerFactory() {    \t\t\t// factory settings    \t\t}     \t\t@Bean    \t\tpublic MyService myService() {    \t\t\treturn new MyService();    \t\t}    \t} }   It is also possible to specify a custom KafkaListenerEndpointRegistry in case you need more control on the way the containers are created and managed. The example below also demonstrates how to customize the org.springframework.messaging.handler.annotation.support.DefaultMessageHandlerMethodFactory as well as how to supply a custom Validator so that payloads annotated with Validated are first validated against a custom Validator.    {    \t@code    \t@Configuration    \t@EnableKafka    \tpublic class AppConfig implements KafkaListenerConfigurer {    \t\t@Override    \t\tpublic void configureKafkaListeners(KafkaListenerEndpointRegistrar registrar) {    \t\t\tregistrar.setEndpointRegistry(myKafkaListenerEndpointRegistry());    \t\t\tregistrar.setMessageHandlerMethodFactory(myMessageHandlerMethodFactory);    \t\t\tregistrar.setValidator(new MyValidator());    \t\t}      \t\t@Bean    \t\tpublic KafkaListenerEndpointRegistry myKafkaListenerEndpointRegistry() {    \t\t\t// registry configuration    \t\t}     \t\t@Bean    \t\tpublic MessageHandlerMethodFactory myMessageHandlerMethodFactory() {    \t\t\tDefaultMessageHandlerMethodFactory factory = new DefaultMessageHandlerMethodFactory();    \t\t\t// factory configuration    \t\t\treturn factory;    \t\t}     \t\t@Bean    \t\tpublic MyService myService() {    \t\t\treturn new MyService();    \t\t}    \t} }   Implementing KafkaListenerConfigurer also allows for fine-grained control over endpoints registration via the KafkaListenerEndpointRegistrar. For example, the following configures an extra endpoint:    {    \t@code    \t@Configuration    \t@EnableKafka    \tpublic class AppConfig implements KafkaListenerConfigurer {    \t\t@Override    \t\tpublic void configureKafkaListeners(KafkaListenerEndpointRegistrar registrar) {    \t\t\tSimpleKafkaListenerEndpoint myEndpoint = new SimpleKafkaListenerEndpoint();    \t\t\t// … configure the endpoint    \t\t\tregistrar.registerEndpoint(endpoint, anotherKafkaListenerContainerFactory());    \t\t}      \t\t@Bean    \t\tpublic MyService myService() {    \t\t\treturn new MyService();    \t\t}     \t\t@Bean    \t\tpublic KafkaListenerContainerFactory&lt;?, ?&gt; anotherKafkaListenerContainerFactory() {    \t\t\t// …    \t\t}     \t\t// Kafka infrastructure setup    \t} }   Note that all beans implementing KafkaListenerConfigurer will be detected and invoked in a similar fashion. The example above can be translated in a regular bean definition registered in the context in case you use the XML configuration. See Also: KafkaListener, KafkaListenerAnnotationBeanPostProcessor, org.springframework.kafka.config.KafkaListenerEndpointRegistrar, org.springframework.kafka.config.KafkaListenerEndpointRegistry   spring-kafka-dist.spring-kafka.main   flush   If you wish to block the sending thread, to await the result, you can invoke the future’s get() method. You may wish to invoke flush() before waiting or, for convenience, the template has a constructor with an autoFlush parameter which will cause the template to flush() on each send. Note, however that flushing will likely significantly reduce performance.   Non Blocking (Async).  public void sendToKafka(final MyOutputData data) { final ProducerRecord&lt;String, String&gt; record = createRecord(data); ListenableFuture&lt;SendResult&lt;Integer, String&gt;&gt; future = template.send(record); future.addCallback(new ListenableFutureCallback&lt;SendResult&lt;Integer, String&gt;&gt;() { @Override public void onSuccess(SendResult&lt;Integer, String&gt; result) { handleSuccess(data); } @Override public void onFailure(Throwable ex) { handleFailure(data, record, ex); } }); }   Blocking (Sync).  public void sendToKafka(final MyOutputData data) { final ProducerRecord&lt;String, String&gt; record = createRecord(data); try { template.send(record).get(10, TimeUnit.SECONDS); handleSuccess(data); } catch (ExecutionException e) {         handleFailure(data, record, e.getCause());     } catch (TimeoutException | InterruptedException e) { handleFailure(data, record, e); } }   KafkaTransactionManager  The KafkaTransactionManager is an implementation of Spring Framework’s PlatformTransactionManager;   ","categories": [],
        "tags": [],
        "url": "/2019/09/02/Kafka-In-Spring.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Conversations with God",
        "excerpt":"Feelings is the language of the soul. If you want to know what’s true for you about something, look to how your’re feeling about.  ","categories": [],
        "tags": [],
        "url": "/2019/09/12/Conversations-with-God.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Algorithm notes from Leecode -- 1",
        "excerpt":"Algorithm Leetcode   Links           [https://www.dailycodingproblem.com/?ref=csdojo]{.underline}            [https://www.csdojo.io/#]{.underline}            https://github.com/mission-peace/interview/tree/master/src/com/interview/dynamic            [https://leetcode.com/discuss/general-discussion/458695/dynamic-programming-patterns]{.underline}            daily coding problem book pdf free download       [https://github.com/CyC2018/CS-Notes/blob/master/notes/Leetcode%20题解%20-%20目录.md]{.underline}   leetcodeGithub project in intelliJ   tasks to hands on           0/1 knapsack            fibnachi memoized and bottom up approaches            median of two sorted array            64 minimum path sum               Maximum sub array (kadane algorithm)          [Slide Window]   （1）没有重复字符的子字符的最大长度：给一个字符串，获得没有重复字符的最长子字符的长度   例子：   输入：\"abcbabcbb\"   输出：3   解释：因为没有重复字符的子字符是'abc'，所以长度是3   public class Solution {//时间复杂度O(2n)   //滑动窗口算法   public int [lengthOfLongestSubstring]{.underline}(String s) {   int n = s.length();   Set&lt;Character&gt; set = new HashSet&lt;&gt;();   int ans = 0, i = 0, j = 0;   while (i &lt; n &amp;&amp; j &lt; n) {//窗口的左边是i，右边是j，下列算法将窗口的左右移动，截取出其中一段   // try to extend the range [i, j]   if (!set.contains(s.charAt(j))){//如果set中不存在该字母，就将j+1，相当于窗口右边向右移动一格，左边不动   set.add(s.charAt(j++));   ans = Math.max(ans, j - i);//记录目前存在过的最大的子字符长度   }   else {//如果set中存在该字母，则将窗口左边向右移动一格，右边不动，直到该窗口中不存在重复的字符   set.remove(s.charAt(i++));   }   }   return ans;   }   }   作者：DrXu   链接：https://juejin.im/post/5c74a2e2f265da2dea053355   来源：掘金   著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。   ====   解法3：优化的滑动窗口算法   上面的滑动窗口算法最多需要2n的步骤，但这其实是能被优化为只需要n步。我们可以使用HashMap定义字符到索引之间的映射，然后，当我们发现子字符串中的重复字符时，可以直接跳过遍历过的字符了。   （2）public class Solution {//时间复杂度o(n)   public int lengthOfLongestSubstring(String s) {   int n = s.length(), ans = 0;   //使用hashmap记录遍历过的字符的索引，当发现重复的字符时，可以将窗口的左边直接跳到该重复字符的索引处   Map&lt;Character, Integer&gt; map = new HashMap&lt;&gt;(); // current index of character   // try to extend the range [i, j]   for (int j = 0, i = 0; j &lt; n; j++) {//j负责向右边遍历，i根据重复字符的情况进行调整   if (map.containsKey(s.charAt(j))) {//当发现重复的字符时,将字符的索引与窗口的左边进行对比，将窗口的左边直接跳到该重复字符的索引处   i = Math.max(map.get(s.charAt(j)), i);   }   //记录子字符串的最大的长度   ans = Math.max(ans, j - i + 1);   //map记录第一次遍历到key时的索引位置，j+1,保证i跳到不包含重复字母的位置   map.put(s.charAt(j), j + 1);   }   return ans;   }   }   作者：DrXu   链接：https://juejin.im/post/5c74a2e2f265da2dea053355   来源：掘金   著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。   [Slide Window]   Min Windows   ------------- best solution---   （3）public static String minWindowBetter(String s, String t){   if(s==null||t==null|s.length()==0||t.length()==0){   return \"\";   }   int left=0,right=0,count=0,min=Integer.MAX_VALUE;   int pool[] = new int[256];   String rtn=\"\";   for(int i =0;i&lt;t.length();i++){   pool[t.charAt(i)]++;   }   while(right&lt;s.length()){   if(pool[s.charAt(right++)]--&gt;0){//[!]   // (a) if(pool[s.charAt(right++)]--&gt;=0), rather than if(pool[right++]--&gt;=0)   // (b) this is \"&gt;0\", but not \"&gt;=0\"   count++;   }   while(count==t.length()){   if((right-left)&lt;min){   min=right-left;   rtn=s.substring(left,right);   }   //shrink window   if(++pool[s.charAt(left++)]&gt;0){   count--;   }   }   }   return rtn;   }   10 lines code to solve most “substring” problem   I will first give the solution then show you the magic template.   The code of solving this problem is below. It might be the shortest among all solutions provided in Discuss.   string minWindow(string s, string t) {   vector&lt;int&gt; map(128,0);   for(auto c: t) map[c]++;   int counter=t.size(), begin=0, end=0, d=INT_MAX, head=0;   while(end&lt;s.size()){   if(map[s[end++]]--&gt;0) counter--; //in t   while(counter==0){ //valid   if(end-begin&lt;d) d=end-(head=begin);   if(map[s[begin++]]++==0) counter++; //make it invalid   }   }   return d==INT_MAX? \"\":s.substr(head, d);   }   Here comes the template.   For most substring problem, we are given a string and need to find a substring of it which satisfy some restrictions. A general way is to use a hashmap assisted with two pointers. The template is given below.   int findSubstring(string s){   vector&lt;int&gt; map(128,0);   int counter; // check whether the substring is valid   int begin=0, end=0; //two pointers, one point to tail and one head   int d; //the length of substring   for() { /* initialize the hash map here */ }   while(end&lt;s.size()){   if(map[s[end++]]-- ?){ /* modify counter here */ }   while(/* counter condition */){   /* update d here if finding minimum*/   //increase begin to make it invalid/valid again   if(map[s[begin++]]++ ?){ /*modify counter here*/ }   }   /* update d here if finding maximum*/   }   return d;   }   One thing needs to be mentioned is that when asked to find maximum substring, we should update maximum after the inner while loop to guarantee that the substring is valid. On the other hand, when asked to find minimum substring, we should update minimum inside the inner while loop.   The code of solving [Longest Substring with At Most Two Distinct Characters]{.underline} is below:   （4）int lengthOfLongestSubstringTwoDistinct(string s) {   vector&lt;int&gt; map(128, 0);   int counter=0, begin=0, end=0, d=0;   while(end&lt;s.size()){   if(map[s[end++]]++==0) counter++;   while(counter&gt;2) if(map[s[begin++]]--==1) counter--;   d=max(d, end-begin);   }   return d;   }   The code of solving [Longest Substring Without Repeating Characters]{.underline} is below:   Update 01.04.2016, thanks \\@weiyi3 for advice.   [（5）]{.underline}int lengthOfLongestSubstring(string s) {   vector&lt;int&gt; map(128,0);   int counter=0, begin=0, end=0, d=0;   while(end&lt;s.size()){   if(map[s[end++]]++&gt;0) counter++;   while(counter&gt;0) if(map[s[begin++]]--&gt;1) counter--;   d=max(d, end-begin); //while valid, update d   }   return d;   }   I think this post deserves some upvotes! : )   [[sliding window ]]{.underline}   string minWindow(string s, string t) {   unordered_map&lt;char, int&gt; m;   // Statistic for count of char in t   for (auto c : t) m[c]++;   // counter represents the number of chars of t to be found in s.   size_t start = 0, end = 0, counter = t.size(), minStart = 0, minLen = INT_MAX;   size_t size = s.size();   // Move to find a valid window.   while (end &lt; size) {   // If char in s exists in t, decrease counter   if (m[s[end]] &gt; 0)   counter--;   // Decrease m[s[end]]. If char does not exist in t, m[s[end]] will be negative.   m[s[end]]--;   end++;   // When we find a valid window, the move starts to find a smaller window.   while (counter == 0) {   if (end - start &lt; minLen) {   minStart = start;   minLen = end - start;   }   m[s[start]]++;   // When char exists in t, increase the counter.   if (m[s[start]] &gt; 0)   counter++;   start++;   }   }   if (minLen != INT_MAX)   return s.substr(minStart, minLen);   return \"\";   }   ～～～～java version～～   public String minWindow(String s, String t) {   HashMap&lt;Character,Integer&gt; map = new HashMap();   for(char c : s.toCharArray())   map.put(c,0);   for(char c : t.toCharArray())   {   if(map.containsKey(c))   map.put(c,map.get(c)+1);   else   return \"\";   }   int start =0, end=0, minStart=0,minLen = Integer.MAX_VALUE, counter = t.length();   while(end &lt; s.length())   {   char c1 = s.charAt(end);   if(map.get(c1) &gt; 0)   counter--;   map.put(c1,map.get(c1)-1);   end++;   while(counter == 0)   {   if(minLen &gt; end-start)   {   minLen = end-start;   minStart = start;   }   char c2 = s.charAt(start);   map.put(c2, map.get(c2)+1);   if(map.get(c2) &gt; 0)   counter++;   start++;   }   }   return minLen == Integer.MAX_VALUE ? \"\" : s.substring(minStart,minStart+minLen);   }   [DP]   [（6]{.underline}）longestCommonSubsequence   public int longestCommonSubsequence(String text1, String text2) {   // xx-est is meant for dynamic programming   // x keys for DP   // 1st, declare a DP table for bottom up   // 2nd set global value   // ================   // for top down, use memo   int m = text1.length(); //[!!!!] it's \"length()\" method for String   int n = text2.length();   int[][] memo = new int[m+1][n+1];   for(int i=0;i&lt;m;i++){   for(int j=0;j&lt;n;j++){   // if(i==0||j==0){ //[!!!!!222] here is no need, because default value in array is zero   // // 1st col or 1st row, set to 0   // memo[i][j]=0;   // }else{   if(text1.charAt(i)==text2.charAt(j)){   memo[i+1][j+1] = 1 + memo[i][j];   }else{   // current char is different, so go to carry previous biggest value from either left or up   memo[i+1][j+1] = Math.max(memo[i+1][j],memo[i][j+1]);   }   // }   }   }   return memo[m][n];   }   [DP]   LengthOfLIS   [（7]{.underline}）public class LengthOfLIS {   System.out.println(\"===test failed case (DP) :\"+inst.lengthOfLIS_tail(new int[]{4,10,4,3,8,9}));   System.out.println(\"===test failed case (DP) :\"+inst.lengthOfLIS_tail(new int[]{2,2})); //expect output \"1\"   public int lengthOfLIS_naive(int[] nums){   //edge case   if(nums.length&lt;0){   return 0;   }   int m=nums.length;   int max=0; // global max   int[] dp=new int[m];   //embedded loop to search max value brute forcely   for (int i = 0; i &lt;m ; i++) {   // loop each digits   int localMax=0; // holder for MAX length of increase sequence before i   for (int j = 0; j &lt; i; j++) {   // loop to find all increasing BEFORE this number   if(dp[j]&gt;localMax &amp;&amp; nums[j]&lt;nums[i]){   // previous number is SMALLER than i and greater than local max, that means it's increasing   localMax=dp[j];   }   }   // after looped ALL previous numbers, add current one   dp[i]=localMax+1;   max = Math.max(max,dp[i]);   }   return max;   }   public int [lengthOfLIS_tail]{.underline}(int[] nums){   int m=nums.length;   if(m==0) return 0;   int[] dp=new int[m]; // dp[x]=y : value \"y\" of dp stores \"the last number\" (tail) of increasing sequence whose length is \"x\"   int maxLen=0;   dp[0]=nums[0];   //for loop each number in array   for (int i = 1; i &lt; m; i++) { //[!!!!!!!!1111111] it should start with \"1\", as \"0\" is already setup   // there are 3 scenarios we need to update dp   if(nums[i]&lt;dp[0]){   // current number is even smaller than most smallest LIS, update it   dp[0]=nums[i];   }else if(nums[i]&gt;dp[maxLen]){   //current number is greater than 'tail' of largest LIS, then update the last LIS   dp[++maxLen]=nums[i];   }else{   // current number is in the middle, so we go to find the *correct* position to locate the LIS in DP   dp[binarySearchLIS(dp,0,maxLen,nums[i])]=nums[i];   }   }   return maxLen+1; // because dp is zero based, so add one for result   }   public int binarySearchLIS(int[] dp, int min, int max, int target){   while(min&lt;=max){   int middle =min + (max-min)/2; //[!!!!!!!] don't forget to add prefix \"min +\" in front of (max-min)/2   if(dp[middle]==target){   return middle;   }else if(dp[middle]&gt;target){   max=middle-1;   } else if(dp[middle]&lt;target){   min=middle+1;   }   }   return min;   }   }   [Graph]   RottingOrange   [（8]{.underline}）public class GraphRottingOrange {   public static void main(String[] args) {   /*   In a given grid, each cell can have one of three values:   the value 0 representing an empty cell;   the value 1 representing a fresh orange;   the value 2 representing a rotten orange.   Every minute, any fresh orange that is adjacent (4-directionally) to a rotten orange becomes rotten.   Return the minimum number of minutes that must elapse until no cell has a fresh orange. If this is impossible, return -1 instead.   Example 1:   Input: [[2,1,1],[1,1,0],[0,1,1]]   Output: 4   Example 2:   Input: [[2,1,1],[0,1,1],[1,0,1]]   Output: -1   Explanation: The orange in the bottom left corner (row 2, column 0) is never rotten, because rotting only happens 4-directionally.   Example 3:   Input: [[0,2]]   Output: 0   Explanation: Since there are already no fresh oranges at minute 0, the answer is just 0.   Note:   1 &lt;= grid.length &lt;= 10   1 &lt;= grid[0].length &lt;= 10   grid[i][j] is only 0, 1, or 2.   */   GraphRottingOrange inst = new GraphRottingOrange();   deleted 2-D array due to hexo error   System.out.println(\"===output of findRottenMinutes:\" + inst.orangesRotting(grid));   }   public int orangesRotting(int[][] grid) {   int m = grid.length;   int n = grid[0].length;   List&lt;String&gt; listRotten = new ArrayList&lt;&gt;();   List&lt;String&gt; listFresh = new ArrayList&lt;&gt;();   int nMinutes = 0;   //firstly, find and enlist rotten ones   for (int i = 0; i &lt; m; i++) {   for (int j = 0; j &lt; n; j++) {   if (grid[i][j] == 2) {   //current cell is a rotten tomato, so check adjacent and contract them   listRotten.add(i + \"\" + j);   } else if (grid[i][j] == 1) {   // fresh tomato, to record it , so check zero of this list to confirm ALL tomato got infected   listFresh.add(i + \"\" + j);   }   }   }   // loop until empty of fresh ones   while (!listFresh.isEmpty()) {   List&lt;String&gt; infected = new ArrayList&lt;&gt;();   for (String strRotten : listRotten) {   int x = strRotten.charAt(0) - '0';   int y = strRotten.charAt(1) - '0';   //to search 4 directions both vertically and horizontally   deleted 2-D array due to hexo error   for (int[] direction : directions) {   int newX = x + direction[0];   int newY = y + direction[1];   String newLoc = newX + \"\" + newY;   if (listFresh.contains(newLoc)) {   // make new tomato as rotten   listFresh.remove(newLoc);   // listRotten.add(newLoc);   infected.add(newLoc); // add to infected, rather than Rotten to avoid \"ConcurrentModificationException\" as it's our loop list   }   }   }   // return -1 in case no more been infected   if (infected.isEmpty()) {   return -1;   }   // assign infected to listRotten to further check   listRotten=infected;   ++nMinutes;   }   return nMinutes;   }   }   public int orangesRotting_Iterative(int[][] grid) {   if(grid == null || grid.length == 0) return 0;   int rows = grid.length;   int cols = grid[0].length;   Queue&lt;int[]&gt; queue = new LinkedList&lt;&gt;();   int count_fresh = 0;   //Put the position of all rotten oranges in queue   //count the number of fresh oranges   for(int i = 0 ; i &lt; rows ; i++) {   for(int j = 0 ; j &lt; cols ; j++) {   if(grid[i][j] == 2) {   queue.offer(new int[]{i , j});   }   else if(grid[i][j] == 1) {   count_fresh++;   }   }   }   //if count of fresh oranges is zero --&gt; return 0   if(count_fresh == 0) return 0;   int count = 0;   deleted 2-D array due to hexo error   //bfs starting from initially rotten oranges   while(!queue.isEmpty()) {   ++count;   int size = queue.size();   for(int i = 0 ; i &lt; size ; i++) {   int[] point = queue.poll();   for(int dir[] : dirs) {   int x = point[0] + dir[0];   int y = point[1] + dir[1];   //if x or y is out of bound   //or the orange at (x , y) is already rotten   //or the cell at (x , y) is empty   //we do nothing   if(x &lt; 0 || y &lt; 0 || x &gt;= rows || y &gt;= cols || grid[x][y] == 0 || grid[x][y] == 2) continue;   //mark the orange at (x , y) as rotten   grid[x][y] = 2;   //put the new rotten orange at (x , y) in queue   queue.offer(new int[]{x , y});   //decrease the count of fresh oranges by 1   count_fresh--;   }   }   }   return count_fresh == 0 ? count-1 : -1;   }   [DP]   DecodeWays   package algo;   [（9]{.underline}）public class DecodeWays {   public static void main(String[] args) {   /*   Similar questions:   62. Unique Paths   70. Climbing Stairs   509. Fibonacci Number   91. Decode Ways   A message containing letters from A-Z is being encoded to numbers using the following mapping:   'A' -&gt; 1   'B' -&gt; 2   ...   'Z' -&gt; 26   Given a non-empty string containing only digits, determine the total number of ways to decode it.   Example 1:   Input: \"12\"   Output: 2   Explanation: It could be decoded as \"AB\" (1 2) or \"L\" (12).   Example 2:   Input: \"226\"   Output: 3   Explanation: It could be decoded as \"BZ\" (2 26), \"VF\" (22 6), or \"BBF\" (2 2 6).   */   DecodeWays inst = new DecodeWays();   System.out.println(\" decode ways: \"+ inst.numDecodings(\"12\"));   }   /*   \"\"\"   s = 123   build up from right =&gt;   num_ways (\"\") =&gt; 1 (empty string can be represented by empty string) (i.e. num_ways[n] = 1) NOTE: only for build up with a valid string. Empty string on it's own doesn't need to be decoded.   num_ways (\"3\") =&gt; 1 (only one way), i.e. num_ways[n-1] = 1   num_ways (\"23\") =&gt; \"23\" or \"2\"-\"3\",   num_ways (\"33\") =&gt; \"3\"\"3\"   num_ways (\"123\") =&gt; \"12\"(num_ways(\"3\")) + \"1\"(\"num_ways(\"23\")) (i.e. num_ways[i+2] + num_ways[i+1])   num_ways (\"323\") =&gt; \"3\"(num_ways(\"23\")) (i.e. num_ways[i+1])   so basically if s[i:i+1] (both included) &lt;= 26,   num_ways[i+2] + num_ways[i+1]   else:   num_ways[i+1]   case with 0:   num_ways (\"103\")   num_ways (\"3\") =&gt; 1 (only one way)   num_ways (\"03\") =&gt; 0 (can't decode 0)   num_ways (\"003\") =&gt; \"00\"(num_ways(\"3\")) + \"0\"(num_ways(\"03\")) =&gt; no way to decode \"00\" = 0 + 0   num_ways (\"103\") =&gt; \"10\"(num_ways(\"3\")) + \"1\"(num_ways(\"03\")) =&gt; num_ways[i+2] + num_ways[i+1](= 0 in this case)   num_ways (\"1003\") =&gt; \"10\"(num_ways(\"03\")) + \"1\"(num_ways(\"003\")) =&gt; same eq = 0(no way to decode \"03\") + 0(no way to decode 003)   Therefore, if i = '0', let memo[i] = 0, also implements for a string where the ith character == '0', string[i:end] can be decoded in 0 ways.   \"\"\"   */   // public class Solution {   public int numDecodings(String s) {   int n = s.length();   if (n == 0) return 0;   int[] memo = new int[n+1];   memo[n] = 1;   memo[n-1] = s.charAt(n-1) != '0' ? 1 : 0;   for (int i = n - 2; i &gt;= 0; i--)   if (s.charAt(i) == '0') continue;   else memo[i] = (Integer.parseInt(s.substring(i,i+2))&lt;=26) ? memo[i+1]+memo[i+2] : memo[i+1];   return memo[0];   }   // }   /*   Thank you so much for this clean and intuitive solution!!   I wrote some notes for myself reference, hope it might help someone to understand this solution.   dp[i]: represents possible decode ways to the ith char(include i), whose index in string is i-1   Base case: dp[0] = 1 is just for creating base; dp[1], when there's one character, if it is not zero, it can only be 1 decode way. If it is 0, there will be no decode ways.   Here only need to look at at most two digits before i, cuz biggest valid code is 26, which has two digits.   For dp[i]: to avoid index out of boundry, extract substring of (i-1,i)- which is the ith char(index in String is i-1) and substring(i-2, i)   First check if substring (i-1,i) is 0 or not. If it is 0, skip it, continue right to check substring (i-2,i), cuz 0 can only be decode by being together with the char before 0.   Second, check if substring (i-2,i) falls in 10~26. If it does, means there are dp[i-2] more new decode ways.   Time: should be O(n), where n is the length of String   Space: should be O(n), where n is the length of String   */   public int numDecodings_v2(String s) {   // this is one DP question, so create DP matrxi first   int[] dp = new int[s.length()+1];   // base case   dp[0]=1;   // for only one char, if first char is 0, which is not in the mapping list, so return 0, otherwise return 1   dp[1]=s.charAt(0)=='0'?0:1;   int m=s.length();   for (int i = 2; i &lt;=m ; i++) {   int digitOne=Integer.valueOf(s.substring(i-1,i));   int digitTwo=Integer.valueOf(s.substring(i-2,i));   if(digitOne&gt;=1){   dp[i] = dp[i] +dp[i-1]; // add one to DP as take this single digit into account   }   if(digitTwo&gt;=10 &amp;&amp; digitTwo&lt;=26){   dp[i] = dp[i] + dp[i-2];   }   }   return dp[m];   }   }   [DP]   class Solution {   public int coinChange(int[] coins, int amount) {   // this is one DP problem, so create matrix for number of fewest numbers of coins to form the   int[] dp = new int[amount+1]; // index of array is the amount to be calculated   Arrays.fill(dp,amount+1); // fill DP with *invalid* value so we can update it to valid one late   //base case   dp[0]=0;   for(int i=0;i&lt;=amount;i++){ //[!!!] should be \"&lt;=\", rather than \"&lt;\"   for(int j=0;j&lt;coins.length;j++){   // if current coin is not greater than i (current amount to calculate fewest number)   if(coins[j]&lt;=i){   // two options, do not take current change OR take current change   dp[i] = Math.min(dp[i], 1+dp[i-coins[j]]);   }   }   }   // if dp[amount] &gt; amount, it means it's amount+1, which is invalid   return dp[amount] &gt; amount ? -1:dp[amount];   }   }   [[Recursive]]{.underline}   [Combination sum II]{.underline}   Each number in candidates may only be used once in the combination.   Example 1:   Input: candidates = [10,1,2,7,6,1,5], target = 8,   A solution set is:   [   [1, 7],   [1, 2, 5],   [2, 6],   [1, 1, 6]   ]   class Solution {   public List&lt;List&lt;Integer&gt;&gt; combinationSum2(int[] candidates, int target) {   List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;();   Arrays.sort(candidates); // here is key to make array increasing   findCombination(candidates,0,target,new ArrayList&lt;&gt;(),result);   return result;   }   public void findCombination(int[] candidates, int idx, int target, List&lt;&gt; current, List&lt;List&lt;&gt;&gt; result){   //base case   if(target == 0){   // found correct combination   result.add(current);   return; // should return right away after add   }   // base case 2   if(target&lt;0){   // last element lead to combination&gt;target   return;   }   for(int i=idx;i&lt;candidates.length;i++){   // loop to try combination by DFS   if(i==idx || candidates[i]!=candidates[i-1]){   // here is key for \"non dup element\"   // as first loop is always unique, no dup   // for non first loop, check it with previous value   current.add(candidates[i]); // Not same as previous one   findCombination(candidates,i+1, target-candidates[i], current, result); // here will DFS try to keep on adding new element to current   current.remove(candidates.length-1);// when above line returned, it means last element is too big   }   }   }   }   [reverse integer]{.underline}   class Solution {   public int reverse(int x) {   long res = 0;   while (x != 0) {   res *= 10;   res += x % 10;   x /= 10;   }   return (int)res == res ? (int)res : 0;   }   }   public class Solution {   public int reverse(int x) {   long result =0;   while(x != 0)   {   result = (result*10) + (x%10);   if(result &gt; Integer.MAX_VALUE) return 0;   if(result &lt; Integer.MIN_VALUE) return 0;   x = x/10;   }   return (int)result;   }   }   Find median of two sorted array   &lt;1&gt; Set imin = 0, imax = m, then start searching in [imin, imax]   &lt;2&gt; Set i = (imin + imax)/2, j = (m + n + 1)/2 - i   &lt;3&gt; Now we have len(left_part)==len(right_part). And there are only 3 situations   that we may encounter:   &lt;a&gt; B[j-1] &lt;= A[i] and A[i-1] &lt;= B[j]   Means we have found the object `i`, so stop searching.   &lt;b&gt; B[j-1] &gt; A[i]   Means A[i] is too small. We must `ajust` i to get `B[j-1] &lt;= A[i]`.   Can we `increase` i?   Yes. Because when i is increased, j will be decreased.   So B[j-1] is decreased and A[i] is increased, and `B[j-1] &lt;= A[i]` may   be satisfied.   Can we `decrease` i?   `No!` Because when i is decreased, j will be increased.   So B[j-1] is increased and A[i] is decreased, and B[j-1] &lt;= A[i] will   be never satisfied.   So we must `increase` i. That is, we must ajust the searching range to   [i+1, imax]. So, set imin = i+1, and goto &lt;2&gt;.   &lt;c&gt; A[i-1] &gt; B[j]   Means A[i-1] is too big. And we must `decrease` i to get `A[i-1]&lt;=B[j]`.   That is, we must ajust the searching range to [imin, i-1].   So, set imax = i-1, and goto &lt;2&gt;.   When the object i is found, the median is:   max(A[i-1], B[j-1]) (when m + n is odd)   or (max(A[i-1], B[j-1]) + min(A[i], B[j]))/2 (when m + n is even)   Number of distinct Islands   private static int rows, cols;   deleted 2-D array due to hexo error   public int numDistinctIslands(int[][] grid) {   cols = grid[0].length;   rows = grid.length;   Set&lt;String&gt; uniqueShapes = new HashSet&lt;&gt;(); // Unique shpes.   StringBuilder shape;   for (int i = 0; i &lt; rows; i++) {   for (int j = 0; j &lt; cols; j++) {   if (grid[i][j] == 1) {   grid[i][j] = 0; // mark it as 'visited'   shape = new StringBuilder(\"s\"); //'s' indicate Start   dfsTraversal(i, j, grid, shape);   uniqueShapes.add(shape.toString());   }   }   }   return uniqueShapes.size();   }   private static void dfsTraversal(int x, int y, int[][] matrix, StringBuilder shape) {   for (int i = 0; i &lt; directions.length; i++) {   int nx = x + directions[i][0];   int ny = y + directions[i][1];   if (nx &gt;= 0 &amp;&amp; ny &gt;= 0 &amp;&amp; nx &lt; rows &amp;&amp; ny &lt; cols) {   if (matrix[nx][ny] == 1) {   matrix[nx][ny] = 0; // mark as 'visited'   shape.append(i);   dfsTraversal(nx, ny, matrix, shape);   }   }   }   shape.append(\"_\");   }   //=======   class Solution {  deleted 2-D array due to hexo error   public int numDistinctIslands(int[][] grid) {   Set&lt;String&gt; set= new HashSet&lt;&gt;();   int res=0;   for(int i=0;i&lt;grid.length;i++){   for(int j=0;j&lt;grid[0].length;j++){   if(grid[i][j]==1) {   StringBuilder sb= new StringBuilder();   helper(grid,i,j,0,0, sb);   String s=sb.toString();   if(!set.contains(s)){   res++;   set.add(s);   }   }   }   }   return res;   }   public void helper(int[][] grid,int i,int j, int xpos, int ypos,StringBuilder sb){   grid[i][j]=0;   sb.append(xpos+\"\"+ypos);   for(int[] dir : dirs){   int x=i+dir[0];   int y=j+dir[1];   if(x&lt;0 || y&lt;0 || x&gt;=grid.length || y&gt;=grid[0].length || grid[x][y]==0) continue;   helper(grid,x,y,xpos+dir[0],ypos+dir[1],sb);   }   }   }   UPDATE: We can use direction string instead of using number string in set.   Below is \\@wavy code using direction string.   public int numDistinctIslands(int[][] grid) {   Set&lt;String&gt; set = new HashSet&lt;&gt;();   for(int i = 0; i &lt; grid.length; i++) {   for(int j = 0; j &lt; grid[i].length; j++) {   if(grid[i][j] != 0) {   StringBuilder sb = new StringBuilder();   dfs(grid, i, j, sb, \"o\"); // origin   grid[i][j] = 0;   set.add(sb.toString());   }   }   }   return set.size();   }   private void dfs(int[][] grid, int i, int j, StringBuilder sb, String dir) {   if(i &lt; 0 || i == grid.length || j &lt; 0 || j == grid[i].length   || grid[i][j] == 0) return;   sb.append(dir);   grid[i][j] = 0;   dfs(grid, i-1, j, sb, \"u\");   dfs(grid, i+1, j, sb, \"d\");   dfs(grid, i, j-1, sb, \"l\");   dfs(grid, i, j+1, sb, \"r\");   sb.append(\"b\"); // back   }      In a complete binary tree every level, *except possibly the            last, is completely filled, and all nodes in the last level are as far left as possible. It can have between 1 and 2^h^* nodes at the last level h.[^[18]^](https://en.wikipedia.org/wiki/Binary_tree#cite_note-complete_binary_tree-18) An alternative definition is a perfect tree whose rightmost leaves (perhaps all) have been removed. Some authors use the term complete to refer instead to a perfect binary tree as defined below, in which case they call this type of tree (with a possibly not filled last level) an almost complete binary tree or nearly complete binary tree.^[19][20]^ A complete binary tree can be efficiently represented using an array.[^[18]^](https://en.wikipedia.org/wiki/Binary_tree#cite_note-complete_binary_tree-18)               {width=”2.2916666666666665in” height=”1.1944444444444444in”}     A complete binary tree (that is not full)       A perfect binary tree is a binary tree in which all interior            nodes have two children and all leaves have the same depth or same level.[^[21]^](https://en.wikipedia.org/wiki/Binary_tree#cite_note-21) An example of a perfect binary tree is the (non-incestuous) ancestry chart of a person to a given depth, as each person has exactly two biological parents (one mother and one father). Provided the ancestry chart always displays the mother and the father on the same side for a given node, their sex can be seen as an analogy of left and right children, children being understood here as an algorithmic term. A perfect tree is therefore always complete but a complete tree is not necessarily perfect.            Heap Tree is a special balanced binary tree data structure where root node is compared with its children and averaged accordingly. There are two type of trees, min heap tree and map heap tree.   For Min heap tree, it’s parent is either smaller or equals its childers.   Get Tree Height   static int getHeight_recursive(TreeNode root){   if(root==null){   return 0;   }   return Math.max(getHeight_recursive(root.left),getHeight_recursive(root.right))+1; //[!!!!!] Here is the key point, it should add \"1\" at last   }   /*   The basic idea:   1. traverse layer by layer   2. For each layer, firslty get number of element,   3. Then add its left &amp; right child for each element   4. Increase height once all element of current layer finished   */   static int getHeight_Iteratively(TreeNode root) {   int height=0;   Stack&lt;TreeNode&gt; stack=new Stack&lt;&gt;();   stack.add(root);   while(!stack.isEmpty()){   int numberOfSibling=stack.size();   // loop in all element in this layer till none is left   while(numberOfSibling-- &gt;0){   root = stack.pop();   // add current element's children   if(root.left!=null) stack.push(root.left);   if(root.right!=null) stack.push(root.right);   }   height++;   }   return height;   }   InvertTree   package algo;   public class TreeInvertBST {   public static void main(String[] args) {   System.out.printf(\"===start===\");   TreeNode root = invertTree(TreeNode.buildBSTTree());   System.out.printf(\"invert tree: \"+ root);   }   static TreeNode invertTree(TreeNode root){   if(root==null) return null;   TreeNode tmpLeft = root.left;   root.left=invertTree(root.right);   root.right=invertTree(tmpLeft);   return root;   }   }   [Number of islands]{.underline}   static int numberOfIslands(char[][] grid){   int number = 0;   if(grid==null || grid.length &lt;0 || grid[0].length&lt;0 ) {   return 0;   }   for (int i = 0; i &lt; grid.length; i++) {   for (int j = 0; j &lt;grid[i].length ; j++) {   if(grid[i][j]=='1') {   // DFS to clear adjacent \"1\" to avoid dup counting   DFS(grid, i, j);   ++number;   }   }   }   return number;   }   // the main purpose of calling DFS is to set “0” for all adjacent “1” cells. As they all together to form one island   static void DFS(char[][] grid, int x, int y){   //edge case   if(grid==null || x&lt;0 || x &gt;= grid.length || y&lt;0 || y&gt;=grid[0].length || grid[x][y]=='0') { //[!!!!] should &gt;= length, not \"&gt;\"   // if(grid==null || x&lt;0 || x &gt; grid.length || y&lt;0 || y&gt;grid[0].length || grid[x][y]==0) {   // return if cursor node is NOT 1   return;   }   // means current cursor node is \"1\"   grid[x][y]='0'; // mark this cell as visited   // check all adjacent cells   DFS(grid, x-1, y);   DFS(grid, x+1, y);   DFS(grid, x, y-1);   DFS(grid, x, y+1);   }   ---------   Is a same tree:   static boolean isSameTree(TreeNode tree1, TreeNode tree2) {   // check base case, null checking   if(tree1==null || tree2 ==null){   return tree1 == tree2; // true when both null, false when only one is null   }   /* same tress should be :   1. node data is same   2. left sub tree is same   4. right sub tree is same   */   return tree1.val==tree2.val &amp;&amp; isSameTree(tree1.left,tree2.left) &amp;&amp; isSameTree(tree1.right,tree2.right);   }   Search BST   public static TreeNode searchBST(TreeNode root, int val) {   if(root==null){   return null;   }   if(root.val==val){   return root;   }else if(val &gt; root.val){   return searchBST(root.right, val);   } else {   return searchBST(root.left,val);   }   }   public static TreeNode searchBST_Iterative(TreeNode root, int val) {   // recursive approach means recursively assgin/update variables   while(root != null &amp;&amp; root.val != val){   root = val&lt;root.val? root.left:root.right;   }   return root;   }   }   Tree Traverse:   static public List&lt;Integer&gt; inorderTraversal_better(TreeNode root) {   List&lt;Integer&gt; listRtn = new ArrayList&lt;&gt;();   // for inorder trave iteratively we'll push/pop stacks   Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;();   // to determine when to push stack   // for inorder traverse, to push left first, then pop root, then last right   while(root!=null || !stack.empty()) {   while(root!=null){   stack.push(root);   // keep on assign left to root for in order traverse   root=root.left;   }   root = stack.pop(); // pop up value of root   listRtn.add(root.val);   root=root.right;   }   return listRtn;   }   /*   This one is more intuitive   */   static public List&lt;Integer&gt; preorderTraversal_better(TreeNode root) {   List&lt;Integer&gt; listRtn = new ArrayList&lt;&gt;();   Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;();   stack.push(root);   while (!stack.empty()) {   root = stack.pop(); // pop up value of root   if (root != null) {   listRtn.add(root.val);   stack.push(root.right);   stack.push(root.left);   }   }   return listRtn;   }   static List&lt;Integer&gt; postOrderTraversal_stack_better(TreeNode node) {   List&lt;Integer&gt; list = new ArrayList&lt;&gt;();   if(node==null) {   return list;   }   Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;();   stack.push(node);   while(!stack.isEmpty()){   node= stack.pop();   list.add(0, node.val); //[!!!!!!!!!!] here is key logic, add item at postion \"0\" means at the begining   if(node.left!=null) stack.push(node.left);   if(node.right!=null) stack.push(node.right);   }   return list;   }   binaryTreeIsBST   /*   The key logic are:   1. assign two boundaries (lower , upper) for each node,   2. update upper to current node for its left child and lower for its right child   3. recursively check each node   */   static boolean binaryTreeIsBST(TreeNode node, int lower, int upper){   // for recursive, base case   // Number 1: base case is null return true   if(node==null) return true;   // Number 2: check data   if(node.val &lt; lower || node.val&gt;upper) {   System.out.printf(\"%s failed in BST check [%d,%d]: \", node, lower,upper);   return false;   }   // for left child node, it's value should between current's node's lower boundary and current node's value   // for right child node, it's value should between current node's value and current's node's upper boundary   return binaryTreeIsBST(node.left,lower,node.val) &amp;&amp; binaryTreeIsBST(node.right,node.val, upper);   }   Iteratively check binary tree is BST: (use inOrder search , only replace list.add with checking pre)   public boolean isValidBST(TreeNode root) {   if (root == null) return true;   Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;();   TreeNode pre = null;   while (root != null || !stack.isEmpty()) {   while (root != null) {   stack.push(root);   root = root.left;   }   root = stack.pop();   if(pre != null &amp;&amp; root.val &lt;= pre.val) return false;   pre = root;   root = root.right;   }   return true;   }   ======   Kth smallest element   public int kthSmallest(TreeNode root, int k) {   Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;();   while(root != null || !stack.isEmpty()) {   while(root != null) {   stack.push(root);   root = root.left;   }   root = stack.pop();   if(--k == 0) break;   root = root.right;   }   return root.val;   }   contrapositive   contradiction   cases   induction   Interview tips:           Do not silent, ask” can I think for a second “            Think out loud            Use examples       Ask “ does that sound a good strategy” rather than write code right            away            Better naming variable. For dynamic programing. If you use memoized            solution, better to name array as “// memorized solitoon memo[]            编辑距离问题就是给我们两个字符串 s1 和 s2，只能用三种操作，让我们把 s1 变成 s2，求最少的操作数。需要明确的是，不管是把 s1 变成 s2 还是反过来，结果都是一样的，所以后文就以 s1 变成 s2 举例。   前文「」说过，解决两个字符串的动态规划问题，一般都是用两个指针 i,j 分别指向两个字符串的最后，然后一步步往前走，缩小问题的规模。   一、动态规划解法   动态规划的核心设计思想是数学归纳法   总结一下动态规划的设计流程：   首先明确 dp 数组所存数据的含义。这步很重要，如果不得当或者不够清晰，会阻碍之后的步骤。   然后根据 dp 数组的定义，运用数学归纳法的思想，假设 $dp[0...i-1]$ 都已知，想办法求出 $dp[i]$，一旦这一步完成，整个题目基本就解决了。   但如果无法完成这一步，很可能就是 dp 数组的定义不够恰当，需要重新定义 dp 数组的含义；或者可能是 dp 数组存储的信息还不够，不足以推出下一步的答案，需要把 dp 数组扩大成二维数组甚至三维数组。   最后想一想问题的 base case 是什么，以此来初始化 dp 数组，以保证算法正确运行。   最长递增子序列（Longest Increasing Subsequence，简写 LIS）是比较经典的一个问题，比较容易想到的是动态规划解法，时间复杂度 O(N\\^2)，   我们的定义是这样的：dp[i] 表示以 nums[i] 这个数结尾的最长递增子序列的长度。   根据刚才我们对 dp 数组的定义，现在想求 dp[5] 的值，也就是想求以 nums[5] 为结尾的最长递增子序列。   nums[5] = 3，既然是递增子序列，我们只要找到前面那些结尾比 3 小的子序列，然后把 3 接到最后，就可以形成一个新的递增子序列，而且这个新的子序列长度加一。   当然，可能形成很多种新的子序列，但是我们只要最长的，把最长子序列的长度作为 dp[5] 的值即可。   还有一个细节问题，dp 数组应该全部初始化为 1，因为子序列最少也要包含自己，所以长度最小为 1。下面我们看一下完整代码：   public int [lengthOfLIS]{.underline}(int[] nums) {   int[] dp = new int[nums.length];   // dp 数组全都初始化为 1   Arrays.fill(dp, 1);   for (int i = 0; i &lt; nums.length; i++) {   for (int j = 0; j &lt; i; j++) {   if (nums[i] &gt; nums[j])   dp[i] = Math.max(dp[i], dp[j] + 1);   }   }   int res = 0;   for (int i = 0; i &lt; dp.length; i++) {   res = Math.max(res, dp[i]);   }   return res;   }   public int lengthOfLIS(int[] nums) {   int[] top = new int[nums.length];   // 牌堆数初始化为 0   int piles = 0;   for (int i = 0; i &lt; nums.length; i++) {   // 要处理的扑克牌   int poker = nums[i];   /***** 搜索左侧边界的二分查找 *****/   int left = 0, right = piles;   while (left &lt; right) {   int mid = (left + right) / 2;   if (top[mid] &gt; poker) {   right = mid;   } else if (top[mid] &lt; poker) {   left = mid + 1;   } else {   right = mid;   }   }   /*********************************/   // 没找到合适的牌堆，新建一堆   if (left == piles) piles++;   // 把这张牌放到牌堆顶   top[left] = poker;   }   // 牌堆数就是 LIS 长度   return piles;   }   至此，二分查找的解法也讲解完毕。   这个解法确实很难想到。首先涉及数学证明，谁能想到按照这些规则执行，就能得到最长递增子序列呢？其次还有二分查找的运用，要是对二分查找的细节不清楚，给了思路也很难写对   /* Dynamic Programming Java implementation of LIS problem */   class LIS   {   /* lis() returns the length of the longest increasing   subsequence in arr[] of size n */   static int lis(int arr[],int n)   {   int lis[] = new int[n];   int i,j,max = 0;   /* Initialize LIS values for all indexes */   for ( i = 0; i &lt; n; i++ )   lis[i] = 1;   /* Compute optimized LIS values in bottom up manner */   for ( i = 1; i &lt; n; i++ )   for ( j = 0; j &lt; i; j++ )   if ( arr[i] &gt; arr[j] &amp;&amp; lis[i] &lt; lis[j] + 1)   lis[i] = lis[j] + 1;   /* Pick maximum of all LIS values */   for ( i = 0; i &lt; n; i++ )   if ( max &lt; lis[i] )   max = lis[i];   return max;   }   public static void main(String args[])   {   int arr[] = { 10, 22, 9, 33, 21, 50, 41, 60 };   int n = arr.length;   System.out.println(\"Length of lis is \" + lis( arr, n ) + \"\\n\" );   }   }   /*This code is contributed by Raja   -----   Find number of days between two given dates   Given two dates, find total number of days between them. The count of days must be calculated in O(1) time and O(1) auxiliary space.   Examples:   Input: dt1 = {10, 2, 2014}   dt2 = {10, 3, 2015}   Output: 393   dt1 represents \"10-Feb-2014\" and dt2 represents \"10-Mar-2015\"   The difference is 365 + 28   Input: dt1 = {10, 2, 2000}   dt2 = {10, 3, 2000}   Output: 29   Note that 2000 is a leap year   Input: dt1 = {10, 2, 2000}   dt2 = {10, 2, 2000}   Output: 0   Both dates are same   Input: dt1 = {1, 2, 2000};   dt2 = {1, 2, 2004};   Output: 1461   Number of days is 365*4 + 1   One Naive Solution is to start from dt1 and keep counting days till dt2 is reached. This solution requires more than O(1) time.   A Better and Simple solution is to count total number of days before dt1 from i.e., total days from 00/00/0000 to dt1, then count total number of days before dt2. Finally return the difference between two counts.   Let the given two dates be \"1-Feb-2000\" and \"1-Feb-2004\"   dt1 = {1, 2, 2000};   dt2 = {1, 2, 2004};   Count number of days before dt1. Let this count be n1.   Every leap year adds one extra day (29 Feb) to total days.   n1 = 2000*365 + 31 + 1 + Number of leap years   Count of leap years for a date 'd/m/y' can be calculated   using following formula:   Number leap years   = y/4 - y/100 + y/400 if m &gt; 2   = (y-1)/4 - (y-1)/100 + (y-1)/400 if m &lt;= 2   All above divisions must be done using integer arithmetic   so that the remainder is ignored.   For 01/01/2000, leap year count is 1999/4 - 1999/100   + 1999/400 which is 499 - 19 + 4 = 484   Therefore n1 is 2000*365 + 31 + 1 + 484   Similarly, count number of days before dt2. Let this   count be n2.   Finally return n2-n1     // Java program two find number of   // days between two given dates   class GFG   {   // A date has day 'd', month 'm' and year 'y'   static class Date   {   int d, m, y;   public Date(int d, int m, int y)   {   this.d = d;   this.m = m;   this.y = y;   }   };   // To store number of days in   // all months from January to Dec.   static int monthDays[] = {31, 28, 31, 30, 31, 30,   31, 31, 30, 31, 30, 31};   // This function counts number of   // leap years before the given date   static int countLeapYears(Date d)   {   int years = d.y;   // Check if the current year needs to be considered   // for the count of leap years or not   if (d.m &lt;= 2)   {   years--;   }   // An year is a leap year if it is a multiple of 4,   // multiple of 400 and not a multiple of 100.   return years / 4 - years / 100 + years / 400;   }   // This function returns number   // of days between two given dates   static int getDifference(Date dt1, Date dt2)   {   // COUNT TOTAL NUMBER OF DAYS BEFORE FIRST DATE 'dt1'   // initialize count using years and day   int n1 = dt1.y * 365 + dt1.d;   // Add days for months in given date   for (int i = 0; i &lt; dt1.m - 1; i++)   {   n1 += monthDays[i];   }   // Since every leap year is of 366 days,   // Add a day for every leap year   n1 += countLeapYears(dt1);   // SIMILARLY, COUNT TOTAL NUMBER OF DAYS BEFORE 'dt2'   int n2 = dt2.y * 365 + dt2.d;   for (int i = 0; i &lt; dt2.m - 1; i++)   {   n2 += monthDays[i];   }   n2 += countLeapYears(dt2);   // return difference between two counts   return (n2 - n1);   }   // Driver code   public static void main(String[] args)   {   Date dt1 = new Date(1, 2, 2000);   Date dt2 = new Date(1, 2, 2004);   System.out.println(\"Difference between two dates is \" +   getDifference(dt1, dt2));   }   }   Last Edit: 6 hours ago   karansingh1559   karansingh1559   179   I am trying to compile a list of DP questions commonly asked in interviews. This will help me and others trying to get better at DP. The list will be sorted by difficulty. If you've come across DP questions, do mention them in the comments.   EASY:   121. Best time to buy and sell stock   198. House Robber   256. Paint House   MEDIUM:   63. Unique Paths II   64. Minimum Path Sum   91. Decode Ways   139. Word Break   221. Maximal Square   300. Longest Increasing Subsequence   322. Coin Change   464. Can I Win   474. Ones and Zeroes   516. Longest Palindromic Subsequence   698. Partition to K Equal Sum Subsets   787. Cheapest Flights Within K Stops   1027. Longest Arithmetic Sequence   1049. Last Stone Weight II   1105. Filling Bookcase Shelves   1143. Longest Common Subsequence   1155. Dice Roll Sum   HARD:   32. Longest Valid Parantheses   44. Wildcard Matching   72. Edit Distance   123. Best Time to Buy and Sell Stock III   312. Burst Balloons   1000. Minimum Cost to Merge Stones   1335. Minimum Difficulty of a Job Schedule   dynamic programming   Minimum (Maximum) Path to Reach a Target   Generate problem statement for this pattern   Statement   Given a target find minimum (maximum) cost / path / sum to reach the target.   Approach   Choose minimum (maximum) path among all possible paths before the current state, then add value for the current state.   routes[i] = min(routes[i-1], routes[i-2], ... , routes[i-k]) + cost[i]   Generate optimal solutions for all values in the target and return the value for the target.   for (int i = 1; i &lt;= target; ++i) {   for (int j = 0; j &lt; ways.size(); ++j) {   if (ways[j] &lt;= i) {   dp[i] = min(dp[i], dp[i - ways[j]]) + cost / path / sum;   }   }   }   return dp[target]   Similar Problems   746. [Min Cost Climbing Stairs Easy]{.underline}   for (int i = 2; i &lt;= n; ++i) {   dp[i] = min(dp[i-1], dp[i-2]) + (i == n ? 0 : cost[i]);   }   return dp[n]   64. Minimum Path Sum Medium   for (int i = 1; i &lt; n; ++i) {   for (int j = 1; j &lt; m; ++j) {   grid[i][j] = min(grid[i-1][j], grid[i][j-1]) + grid[i][j];   }   }   return grid[n-1][m-1]   322. Coin Change Medium   for (int j = 1; j &lt;= amount; ++j) {   for (int i = 0; i &lt; coins.size(); ++i) {   if (coins[i] &lt;= j) {   dp[j] = min(dp[j], dp[j - coins[i]] + 1);   }   }   }   Tree   Find minimum path   class Solution {   public int minDepth(TreeNode root) {   if(root == null) return 0; // base case   int left = minDepth(root.left); // get depth of left   int right = minDepth(root.right); // get depth of right   if(root.left == null) return right + 1; // leaf nodes are in right subtree   if(root.right == null) return left + 1; // leaf nodes are in left subtree   // if left/right subtrees both contains leaf nodes   return Math.min(left, right) + 1;   }   }   Get min depth in Iterative approach   public int minDepth(TreeNode root) {   if(root == null)   return 0;   Queue&lt;TreeNode&gt; que = new LinkedList();   int level =1;   que.add(root);   while(!que.isEmpty()){   int size = que.size();   while(size&gt;0){   TreeNode node =que.poll();   if(node.left == null &amp;&amp; node.right ==null)   return level;   if(node.left != null)   que.add(node.left);   if(node.right != null)   que.add(node.right);   size--;   }   level++;   }   return level;   }   =================   two solutions with explanation: DFS &amp; BFS:   /** Solution 1: DFS   * Key point:   * if a node only has one child -&gt; MUST return the depth of the side with child, i.e. MAX(left, right) + 1   * if a node has two children on both side -&gt; return min depth of two sides, i.e. MIN(left, right) + 1   * */   public int minDepth(TreeNode root) {   if (root == null) {   return 0;   }   int left = minDepth(root.left);   int right = minDepth(root.right);   if (left == 0 || right == 0) {   return Math.max(left, right) + 1;   }   else {   return Math.min(left, right) + 1;   }   }   /** Solution 2: BFS level order traversal */   public int minDepth2(TreeNode root) {   if (root == null) {   return 0;   }   Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;();   queue.offer(root);   int level = 1;   while (!queue.isEmpty()) {   int size = queue.size();   for (int i = 0; i &lt; size; i++) {   TreeNode curNode = queue.poll();   if (curNode.left == null &amp;&amp; curNode.right == null) {   return level;   }   if (curNode.left != null) {   queue.offer(curNode.left);   }   if (curNode.right != null) {   queue.offer(curNode.right);   }   }   level++;   }   return level;   }   Preorder Traversal   In preorder traversal, we traverse the root first, then the left and right subtrees.   We can simply implement preorder traversal using recursion:   public void traversePreOrder(Node node) {   if (node != null) {   visit(node.value);   traversePreOrder(node.left);   traversePreOrder(node.right);   }   }   We can also implement preorder traversal without recursion.   To implement an iterative preorder traversal, we'll need a Stack, and we'll go through these steps:   Push root in our stack   While stack is not empty   Pop current node   Visit current node   Push right child, then left child to stack   public void traversePreOrderWithoutRecursion() {   Stack&lt;Node&gt; stack = new Stack&lt;Node&gt;();   Node current = root;   stack.push(root);   while(!stack.isEmpty()) {   current = stack.pop();   visit(current.value);   if(current.right != null) {   stack.push(current.right);   }   if(current.left != null) {   stack.push(current.left);   }   }   }   Convert Sorted List to Binary Search Tree   Medium   1503   79   Add to List   Share   Given a singly linked list where elements are sorted in ascending order, convert it to a height balanced BST.   For this problem, a height-balanced binary tree is defined as a binary tree in which the depth of the two subtrees of every node never differ by more than 1.   Example:   Given the sorted linked list: [-10,-3,0,5,9],   One possible answer is: [0,-3,9,-10,null,5], which represents the following height balanced BST:   0   / \\   -3 9   / /   -10 5   breadth first search   First of all, let's reuse the algorithm from above, adapted to the new structure:   public static &lt;T&gt; Optional&lt;Node&lt;T&gt;&gt; search(T value, Node&lt;T&gt; start) {   Queue&lt;Node&lt;T&gt;&gt; queue = new ArrayDeque&lt;&gt;();   queue.add(start);   Node&lt;T&gt; currentNode;   while (!queue.isEmpty()) {   currentNode = queue.remove();   if (currentNode.getValue().equals(value)) {   return Optional.of(currentNode);   } else {   queue.addAll(currentNode.getNeighbors());   }   }   return Optional.empty();   }   [Binary search]{.underline}   分析二分查找的一个技巧是：不要出现 else，而是把所有情况用 else if 写清楚，这样可以清楚地展现所有细节。   ------   Sliding window   In any sliding window based problem we have two pointers. One right pointer whose job is to expand the current window and then we have the left pointer whose job is to contract a given window. At any point in time only one of these pointers move and the other one remains fixed.   Smallest window contains sub string   public class Solution {   public String minWindow(String s, String t) {   if(s == null || s.length() &lt; t.length() || s.length() == 0){   return \"\";   }   HashMap&lt;Character,Integer&gt; map = new HashMap&lt;Character,Integer&gt;();   for(char c : t.toCharArray()){   if(map.containsKey(c)){   map.put(c,map.get(c)+1);   }else{   map.put(c,1);   }   }   int left = 0;   int minLeft = 0;   int minLen = s.length()+1;   int count = 0;   for(int right = 0; right &lt; s.length(); right++){   if(map.containsKey(s.charAt(right))){   map.put(s.charAt(right),map.get(s.charAt(right))-1);   if(map.get(s.charAt(right)) &gt;= 0){   count ++;   }   while(count == t.length()){   if(right-left+1 &lt; minLen){   minLeft = left;   minLen = right-left+1;   }   if(map.containsKey(s.charAt(left))){   map.put(s.charAt(left),map.get(s.charAt(left))+1);   if(map.get(s.charAt(left)) &gt; 0){   count --;   }   }   left ++ ;   }   }   }   if(minLen&gt;s.length())   {   return \"\";   }   return s.substring(minLeft,minLeft+minLen);   }   }   --   public static String minWindowOp(String s, String t) {   int [] map = new int[128];//map to track number of occurrence of each character of sub string   for (char c : t.toCharArray()) {   map[c]++;   }   int start = 0, end = 0, minStart = 0, minLen = Integer.MAX_VALUE, counter = t.length();   // counter is number of distinct chars in sub string   while (end &lt; s.length()) {   final char c1 = s.charAt(end);// walk through each char in source string   if (map[c1] &gt; 0) {   counter--; // if cached char number greater than 0, decrease counter   }   map[c1]--;//decrease cached char number, for chars not in substring, it will be negative   end++; //move right pointer   while (counter == 0) { //counter is zero means all chars found   if (minLen &gt; end - start) { //to find and cache minimum sliding window length and minimum start   minLen = end - start;   minStart = start;   }   final char c2 = s.charAt(start);   map[c2]++;// A is -2， B is 1   if (map[c2] &gt; 0) {   counter++; //if current char exist in cache, increase counter, otherwise keep counter zero   }   start++;   }   }   return minLen == Integer.MAX_VALUE ? \"\" : s.substring(minStart, minStart + minLen);   }   ------   I agree with your code, but I prefer this code when count == t.length(),   class Solution {   public String minWindow(String s, String t) {   // corner case   if(s == null || t == null || s.length() == 0 || t.length() == 0 || s.length() &lt; t.length()) return \"\";   // construct model   int minLeft = 0;   int minRight = 0;   int min = s.length();   boolean flag = false;   Map&lt;Character, Integer&gt; map = new HashMap&lt;&gt;();   int count = t.length(); // the number of characters that I need to match   for(char c : t.toCharArray()) map.put(c, map.getOrDefault(c, 0) + 1);   // unfixed sliding window, 2 pointers   int i = 0;   int j = 0;   while(j &lt; s.length()){   char c = s.charAt(j);   if(map.containsKey(c)){   map.put(c, map.get(c) - 1);   if(map.get(c) &gt;= 0) count--; // if still unmatched characters, then count--   }   // if found a susbtring   while(count == 0 &amp;&amp; i &lt;= j){   // update global min   flag = true;   int curLen = j + 1 - i;   if(curLen &lt;= min){   minLeft = i;   minRight = j;   min = curLen;   }   // shrink left pointer   char leftC = s.charAt(i);   if(map.containsKey(leftC)){   map.put(leftC, map.get(leftC) + 1);   if(map.get(leftC) &gt;= 1) count++;   }   i++;   }   j++;   }   return flag == true ? s.substring(minLeft, minRight + 1): \"\";   }   }   First part: when the right pointer is getting incremented we are decrementing the map count of char if it's part of 't' string. When we see that the map count of that char after decrementing is positive/zero means that the right ptr has found a useful char and hence we increment the 'count' variable (which is keeping track of the number of useful chars)   Second part: when the left pointer is getting incremented we are essentially making the window smaller and giving back the chars to the map (i.e. incrementing the map count). If we find that for the particular char the map count has now become positive means that we actually gave back a useful char and hence the 'count' is to be decremented.   At this point then we again start increasing our window and see each time if the count has become equal to the number of chars in 't' string.   ----   Generally, there are following steps:      create a hashmap for each character in t and count their frequency            in t as the value of hashmap.            Find the first window in S that contains T. But how? there the            author uses the count.            Checking from the leftmost index of the window and to see if it            belongs to t. The reason we do so is that we want to shrink the size of the window.  3-1) If the character at leftmost index does not belong to t, we can directly remove this leftmost value and update our window(its minLeft and minLen value)  3-2) If the character indeed exists in t, we still remove it, but in the next step, we will increase the right pointer and expect the removed character. If find so, repeat step 3.            public String minWindow(String s, String t) {   HashMap&lt;Character, Integer&gt; map = new HashMap();   for(char c : t.toCharArray()){   if(map.containsKey(c)){   map.put(c, map.get(c)+1);   }   else{   map.put(c, 1);   }   }   int left = 0, minLeft=0, minLen =s.length()+1, count = 0;   for(int right = 0; right&lt;s.length(); right++){   char r = s.charAt(right);   if(map.containsKey(r)){//the goal of this part is to get the first window that contains whole t   map.put(r, map.get(r)-1);   if(map.get(r)&gt;=0) count++;//identify if the first window is found by counting frequency of the characters of t showing up in S   }   while(count == t.length()){//if the count is equal to the length of t, then we find such window   if(right-left+1 &lt; minLen){//jsut update the minleft and minlen value   minLeft = left;   minLen = right-left+1;   }   char l = s.charAt(left);   if(map.containsKey(l)){//starting from the leftmost index of the window, we want to check if s[left] is in t. If so, we will remove it from the window, and increase 1 time on its counter in hashmap which means we will expect the same character later by shifting right index. At the same time, we need to reduce the size of the window due to the removal.   map.put(l, map.get(l)+1);   if(map.get(l)&gt;0) count--;   }   left++;//if it doesn't exist in t, it is not supposed to be in the window, left++. If it does exist in t, the reason is stated as above. left++.   }   }   return minLen==s.length()+1?\"\":s.substring(minLeft, minLeft+minLen);   ------------- best solution---   public static String minWindowBetter(String s, String t){   if(s==null||t==null|s.length()==0||t.length()==0){   return \"\";   }   int left=0,right=0,count=0,min=Integer.MAX_VALUE;   int pool[] = new int[256];   String rtn=\"\";   for(int i =0;i&lt;t.length();i++){   pool[t.charAt(i)]++;   }   while(right&lt;s.length()){   if(pool[s.charAt(right++)]--&gt;0){//[!]   // (a) if(pool[s.charAt(right++)]--&gt;=0), rather than if(pool[right++]--&gt;=0)   // (b) this is \"&gt;0\", but not \"&gt;=0\"   count++;   }   while(count==t.length()){   if((right-left)&lt;min){   min=right-left;   rtn=s.substring(left,right);   }   //shrink window   if(++pool[s.charAt(left++)]&gt;0){   count--;   }   }   }   return rtn;   }   while(right &lt; length of s){      Deincrement characters frequence at right pointer in String s from bank     Right -- (expand window)     If that character was inside of t, increase count     while(count equal to length of t - condition){     Check if right-left less than min, if so, update min and curr string     Increate characters frequences at left pointer in string, s from bank     Left++ (shift window)     If bank[character at left pointer]&gt;=0, then decrease count.    }   [knapsack 0/1 背包]{.underline}   用子问题定义状态：即f[i][v]表示前i件物品恰放入一个容量为v的背包可以获得的最大价值。则其状态转移方程便是：   f[i][v]=max{f[i-1][v],f[i-1][v-c[i]]+w[i]}   这个方程非常重要，基本上所有跟背包相关的问题的方程都是由它衍生出来的。所以有必要将它详细解释一下：”将前i件物品放入容量为v的背包中”这个子问题，若只考虑第i件物品的策略（放或不放），那么就可以转化为一个只牵扯前i-1件物品的问题。如果不放第i件物品，那么问题就转化为”前i-1件物品放入容量为v的背包中”，价值为f[i-1][v]；如果放第i件物品，那么问题就转化为”前i-1件物品放入剩下的容量为v-c[i]的背包中”，此时能获得的最大价值就是f[i-1][v-c[i]]再加上通过放入第i件物品获得的价值w[i]。   private static int knapsack01(int[] weights, int[] value, int quota) {   // we are using dynamic programming bottom up   // one tab to keep track of value, size is quota + 1   int[][] dp = new int[value.length+1][quota+1];   // as size is actual size + 1, so here is \"&lt;=\" , rather than \"&lt;\"   for(int i=0;i&lt;=value.length;i++){   for (int j =0;j&lt;=quota;j++){   //base value   if(i==0 || j==0){   // initilize first line and first column to '0'   dp[i][j] = 0;   continue;   }   // non zero   if(j&gt;=weights[i-1]){   // current weight not bigger than current quota   // so add it to our backtrack   // get the max one of (1) Not include , (2) include this node   dp[i][j]= Math.max(dp[i-1][j], dp[i-1][j-weights[i-1]]+value[i-1]);   }else{   // required weight is less than provided, so skip this   dp[i][j] = dp[i-1][j]; //use value (j) of previous (i-1   }   }   }   return dp[value.length][quota];   }   [KMP 算法]{.underline}   KMP 算法永不回退txt的指针i，不走回头路（不会重复扫描txt），而是借助dp数组中储存的信息把pat移到正确的位置继续匹配，时间复杂度只需 O(N)，用空间换时间，所以我认为它是一种动态规划算法。   // 暴力匹配（伪码）   int search(String pat, String txt) {   int M = pat.length;   int N = txt.length;   for (int i = 0; i &lt; N - M; i++) {   int j;   for (j = 0; j &lt; M; j++) {   if (pat[j] != txt[i+j])   break;   }   // pat 全都匹配了   if (j == M) return i;   }   // txt 中不存在 pat 子串   return -1;   ---   dynamic programming   public class KMP {   private int[][] dp;   private String pat;   public KMP(String pat) {   this.pat = pat;   // 通过 pat 构建 dp 数组   // 需要 O(M) 时间   }   public int search(String txt) {   // 借助 dp 数组去匹配 txt   // 需要 O(N) 时间   }   }   为了描述状态转移图，我们定义一个二维 dp 数组，它的含义如下：   dp[j][c] = next   0 &lt;= j &lt; M，代表当前的状态   0 &lt;= c &lt; 256，代表遇到的字符（ASCII 码）   0 &lt;= next &lt;= M，代表下一个状态   dp[4]['A'] = 3 表示：   当前是状态 4，如果遇到字符 A，   pat 应该转移到状态 3   dp[1]['B'] = 2 表示：   当前是状态 1，如果遇到字符 B，   pat 应该转移到状态 2   根据我们这个 dp 数组的定义和刚才状态转移的过程，我们可以先写出 KMP 算法的 search 函数代码：   public int search(String txt) {   int M = pat.length();   int N = txt.length();   // pat 的初始态为 0   int j = 0;   for (int i = 0; i &lt; N; i++) {   // 当前是状态 j，遇到字符 txt[i]，   // pat 应该转移到哪个状态？   j = dp[j][txt.charAt(i)];   // 如果达到终止态，返回匹配开头的索引   if (j == M) return i - M + 1;   }   // 没到达终止态，匹配失败   return -1;   }   for 0 &lt;= j &lt; M: # 状态   for 0 &lt;= c &lt; 256: # 字符   dp[j][c] = next   这个 next 状态应该怎么求呢？显然，如果遇到的字符c和pat[j]匹配的话，状态就应该向前推进一个，也就是说next = j + 1，我们不妨称这种情况为状态推进：   如果遇到的字符c和pat[j]不匹配的话，状态就要回退（或者原地不动），我们不妨称这种情况为状态重启：   那么，如何得知在哪个状态重启呢？解答这个问题之前，我们再定义一个名字：影子状态（我编的名字），用变量X表示。所谓影子状态，就是和当前状态具有相同的前缀。比如下面这种情况：   当前状态j = 4，其影子状态为X = 2，它们都有相同的前缀 \"AB\"。因为状态X和状态j存在相同的前缀，所以当状态j准备进行状态重启的时候（遇到的字符c和pat[j]不匹配），可以通过X的状态转移图来获得最近的重启位置。   比如说刚才的情况，如果状态j遇到一个字符 \"A\"，应该转移到哪里呢？首先状态 4 只有遇到 \"C\" 才能推进状态，遇到 \"A\" 显然只能进行状态重启。状态j会把这个字符委托给状态X处理，也就是dp[j]['A'] = dp[X]['A']：   int X # 影子状态   for 0 &lt;= j &lt; M:   for 0 &lt;= c &lt; 256:   if c == pat[j]:   # 状态推进   dp[j][c] = j + 1   else:   # 状态重启   # 委托 X 计算重启位置   dp[j][c] = dp[X][c]   ---   影子状态X是如何得到的呢？下面先直接看完整代码吧。   public class KMP {   private int[][] dp;   private String pat;   public KMP(String pat) {   this.pat = pat;   int M = pat.length();   // dp[状态][字符] = 下个状态   dp = new int[M][256];   // base case   dp[0][pat.charAt(0)] = 1;   // 影子状态 X 初始为 0   int X = 0;   // 当前状态 j 从 1 开始   for (int j = 1; j &lt; M; j++) {   for (int c = 0; c &lt; 256; c++) {   if (pat.charAt(j) == c)   dp[j][c] = j + 1;   else   dp[j][c] = dp[X][c];   }   // 更新影子状态   X = dp[X][pat.charAt(j)];   }   }   public int search(String txt) {...}   }   先解释一下这一行代码：   // base case   dp[0][pat.charAt(0)] = 1;   这行代码是 base case，只有遇到 pat[0] 这个字符才能使状态从 0 转移到 1，遇到其它字符的话还是停留在状态 0（Java 默认初始化数组全为 0）。   影子状态X是先初始化为 0，然后随着j的前进而不断更新的。下面看看到底应该如何更新影子状态X：   int X = 0;   for (int j = 1; j &lt; M; j++) {   ...   // 更新影子状态   // 当前是状态 X，遇到字符 pat[j]，   // pat 应该转移到哪个状态？   X = dp[X][pat.charAt(j)];   }   更新X其实和search函数中更新状态j的过程是非常相似的：   int j = 0;   for (int i = 0; i &lt; N; i++) {   // 当前是状态 j，遇到字符 txt[i]，   // pat 应该转移到哪个状态？   j = dp[j][txt.charAt(i)];   ...   }   其中的原理非常微妙，注意代码中 for 循环的变量初始值，可以这样理解：后者是在txt中匹配pat，前者是在pat中匹配pat[1:]，状态X总是落后状态j一个状态，与j具有最长的相同前缀。所以我把X比喻为影子状态，似乎也有一点贴切。   另外，构建 dp 数组是根据 base casedp[0][..]向后推演。这就是我认为 KMP 算法就是一种动态规划算法的原因。   至此，KMP 算法就已经再无奥妙可言了！看下 KMP 算法的完整代码吧：   public class KMP {   private int[][] dp;   private String pat;   public KMP(String pat) {   this.pat = pat;   int M = pat.length();   // dp[状态][字符] = 下个状态   dp = new int[M][256];   // base case   dp[0][pat.charAt(0)] = 1;   // 影子状态 X 初始为 0   int X = 0;   // 构建状态转移图（稍改的更紧凑了）   for (int j = 1; j &lt; M; j++) {   for (int c = 0; c &lt; 256; c++) {   dp[j][c] = dp[X][c];   dp[j][pat.charAt(j)] = j + 1;   // 更新影子状态   X = dp[X][pat.charAt(j)];   }   }   public int search(String txt) {   int M = pat.length();   int N = txt.length();   // pat 的初始态为 0   int j = 0;   for (int i = 0; i &lt; N; i++) {   // 计算 pat 的下一个状态   j = dp[j][txt.charAt(i)];   // 到达终止态，返回结果   if (j == M) return i - M + 1;   }   // 没到达终止态，匹配失败   return -1;   }   }   经过之前的详细举例讲解，你应该可以理解这段代码的含义了，当然你也可以把 KMP 算法写成一个函数。核心代码也就是两个函数中 for 循环的部分，数一下有超过十行吗？   [labuladong]{.underline}   你只要把住两点就行了：   1、遍历的过程中，所需的状态必须是已经计算出来的。   2、遍历的终点必须是存储结果的那个位置。   PS：但凡遇到需要递归的问题，最好都画出递归树，这对你分析算法的复杂度，寻找算法低效的原因都有巨大帮助   int fib(int n) {   if (n == 2 || n == 1)   return 1;   int prev = 1, curr = 1;   for (int i = 3; i &lt;= n; i++) {   int sum = prev + curr;   prev = curr;   curr = sum;   }   return curr;   }   首先，这个问题是动态规划问题，因为它具有「最优子结构」的。要符合「最优子结构」，子问题间必须互相独立。啥叫相互独立？你肯定不想看数学证明，我用一个直观的例子来讲解。   比如说，你的原问题是考出最高的总成绩，那么你的子问题就是要把语文考到最高，数学考到最高…… 为了每门课考到最高，你要把每门课相应的选择题分数拿到最高，填空题分数拿到最高…… 当然，最终就是你每门课都是满分，这就是最高的总成绩。   得到了正确的结果：最高的总成绩就是总分。因为这个过程符合最优子结构，”每门科目考到最高”这些子问题是互相独立，互不干扰的。   但是，如果加一个条件：你的语文成绩和数学成绩会互相制约，此消彼长。这样的话，显然你能考到的最高总成绩就达不到总分了，按刚才那个思路就会得到错误的结果。因为子问题并不独立，语文数学成绩无法同时最优，所以最优子结构被破坏。   PS：为啥 dp 数组初始化为 amount + 1 呢，因为凑成 amount 金额的硬币数最多只可能等于 amount（全用 1 元面值的硬币），所以初始化为 amount + 1 就相当于初始化为正无穷，便于后续取最小值   最优子结构并不是动态规划独有的一种性质，能求最值的问题大部分都具有这个性质；但反过来，最优子结构性质作为动态规划问题的必要条件，一定是让你求最值的，以后碰到那种恶心人的最值题，思路往动态规划想就对了，这就是套路。   动态规划不就是从最简单的 base case 往后推导吗，可以想象成一个链式反应，以小博大。但只有符合最优子结构的问题，才有发生这种链式反应的性质   ----------   「状态」很明显，就是当前拥有的鸡蛋数K和需要测试的楼层数N。随着测试的进行，鸡蛋个数可能减少，楼层的搜索范围会减小，这就是状态的变化。   「选择」其实就是去选择哪层楼扔鸡蛋。回顾刚才的线性扫描和二分思路，二分查找每次选择到楼层区间的中间去扔鸡蛋，而线性扫描选择一层层向上测试。不同的选择会造成状态的转移。   现在明确了「状态」和「选择」，动态规划的基本思路就形成了：肯定是个二维的dp数组或者带有两个状态参数的dp函数来表示状态转移；外加一个 for 循环来遍历所有选择，择最优的选择更新结果 ：   SuperEggDrop   Drop eggs is a very classical problem.   Some people may come up with idea O(KN\\^2)   where dp[K][N] = 1 + max(dp[K - 1][i - 1],dp[K][N - i]) for i in 1...N.   However this idea is very brute force, for the reason that you check all possiblity.   So I consider this problem in a different way:   dp[M][K]means that, given K eggs and M moves,   what is the maximum number of floor that we can check.   The dp equation is:   dp[m][k] = dp[m - 1][k - 1] + dp[m - 1][k] + 1,   which means we take 1 move to a floor,   if egg breaks, then we can check dp[m - 1][k - 1] floors.   if egg doesn't breaks, then we can check dp[m - 1][k] floors.   dp[m][k] is similar to the number of combinations and it increase exponentially to N   public int superEggDrop(int K, int N) {   int[][] floors = new int[N + 1][K + 1];   int move = 0;   while (floors[move][K] &lt; N) {   ++m;   for (int egg = 1; egg &lt;= K; ++egg)   floors[move][egg] = floors[move - 1][egg] + 1 + floors[move - 1][egg - 1];   }   return m;   }   The dp equation is:   dp[m][k] = dp[m - 1][k - 1] + dp[m - 1][k] + 1,   assume, dp[m-1][k-1] = n0, dp[m-1][k] = n1   the first floor to check is n0+1.   if egg breaks, F must be in [1,n0] floors, we can use m-1 moves and k-1 eggs to find out F is which one.   if egg doesn't breaks and F is in [n0+2, n0+n1+1] floors, we can use m-1 moves and k eggs to find out F is which one.   So, with m moves and k eggs, we can find out F in n0+n1+1 floors, whichever F is.   ---   Great, I understand this solution too.   The key concept of original O(KN\\^2) solution is to try all the floor to get the min cost min(max(broke, not broke)) as the answer.   This solution is somehow a reverse thinking:      No matter which floor you try, egg will only break or not break, if            break, go to downstairs, if not break, go to upstairs.            No matter you go up or go down, the num of all the floors is always            upstairs + downstairs + the floor you try, which is dp[m][k] = dp[m - 1][k - 1] + dp[m - 1][k] + 1.            ====   the logic of \"dp[m][k] = dp[m - 1][k - 1] + dp[m - 1][k] + 1\", my confusion is, if dp[m - 1][k - 1] and dp[m - 1][k] are two different case break or not break; why we combine them together, instead of use the smaller one? would you please explain more?   →   This one move will separate the floors into two non-overlapping groups, below or above (the current level we choose to drop the egg); so no matter what happened to the egg, we only need to check one of those two group. If we need to check the level below the current level, then it means the egg is break, so the maximum level we are able to check is dp[m - 1][k - 1]. Otherwise if we need to check the level above or equal o the current level, it means the egg is not break, so the maximum level we can check is dp[m - 1][k], we should only return dp[m - 1][k - 1] + dp[m - 1][k]; however, we count the level from 0, instead of 1, so we need to add the extra one level (i.e; if dp[m - 1][k - 1] = 1 and dp[m - 1][k] = 2, means we can check (2 + 3 == 5) levels, so we need to return 4; which is dp[m - 1][k - 1] + dp[m     1][k] + 1)   Notes:      Find max “1” matrix in a square   Instead to create a cache and initialise all element by copying when I,j =0.   Better solution is to clone input “matrix” this will lead to faster and cleaner code   If(I=0 || j=0) {// do nothing because those element remain unchanged in matrix copy}      For Two sum, should raise Exception e.g. no findings rather than            return “null”            public int[] [twoSum]{.underline}(int[] nums, int target) {   for (int i = 0; i &lt; nums.length; i++) {   for (int j = i + 1; j &lt; nums.length; j++) { // I used int j=i, which is wrong as it may cause one item to be used twice   if (nums[j] == target - nums[i]) {   return new int[] { i, j };   }   }   }   throw new IllegalArgumentException(\"No two sum solution\");   }      Return specific Exception, e.g.   throw new IllegalArgumentException(\"No two sum solution\");      Error of two sums      if(map1.containsKey(diff)) {     return new int[]{i, map1.get(diff)};     } else{     // be careful put number itself (rather than supplement) to map     map1.put(nums[i], i);     }            Summary:                       If possible, make use of HashMap to increase search performance                        If there are two loops, try to reduce to use one in-flight                     hashmap lookup                                 Multiply string              Naiive solution              ublic String [multiply]{.underline}(String num1, String num2) {     String n1 = new StringBuilder(num1).reverse().toString();     String n2 = new StringBuilder(num2).reverse().toString();     int[] d = new int[num1.length()+num2.length()];     //multiply each digit and sum at the corresponding positions     for(int i=0; i&lt;n1.length(); i++){     for(int j=0; j&lt;n2.length(); j++){     d[i+j] += (n1.charAt(i)-'0') * (n2.charAt(j)-'0');     }     }     StringBuilder sb = new StringBuilder();     //calculate each digit     for(int i=0; i&lt;d.length; i++){     int mod = d[i]%10;     int carry = d[i]/10;     if(i+1&lt;d.length){     d[i+1] += carry;     }     sb.insert(0, mod);     }     //remove front 0's     while(sb.charAt(0) == '0' &amp;&amp; sb.length()&gt; 1){     sb.deleteCharAt(0);     }     return sb.toString();     }     -----Best Multiply String---     private static String multiply(String num1, String num2) {     int nLen1 = num1.length(), nLen2=num2.length();     int[] result = new int[nLen1+nLen2];     for(int c1=nLen1-1;c1&gt;=0;c1--) {     for(int c2=nLen2-1;c2&gt;=0;c2--){     int nMulti= (num1.charAt(c1) - '0') * (num2.charAt(c2) - '0');     int sum = nMulti + result[c1+c2+1];     result[c1+c2] += sum / 10; //This is for “carry”, so must to be “+” in front of “=”     result[c1+c2+1] = sum % 10; // This is a reminder, so this only be assigned without “+”     }     }     StringBuffer buff = new StringBuffer();     for(int p:result) {     if(!(result.length==0 &amp;&amp; p==0)) {     //remove prefix 0     buff.append(p);     }     }     return buff.toString();     }       greedy algorithm      [Greedy]     const int N = 5;     int Count[N] = {5,2,2,3,5};//每一张纸币的数量     int Value[N] = {1,5,10,50,100};     int [solve]{.underline}(int money) {     int num = 0;     for(int i = N-1;i&gt;=0;i--) {     int c = min(money/Value[i],Count[i]);//每一个所需要的张数     money = money-c*Value[i];     num += c;//总张数     }     if(money&gt;0) num=-1;     return num;     }       Add One:      private static int[] plusOneBest(int[] ary) {     for (int i = ary.length - 1; i &gt;= 0; i--) {     if (ary[i] != 9) {     ary[i]++; //[!] Here is key step, for two cases: (1) last digit, add one then exit (2) Next digit with carry, add on and exit     break;     } else {     ary[i] = 0;     }     }     if (ary[0] == 0) {     int[] aryRtn = new int[ary.length + 1];     System.arraycopy(aryRtn, 1, ary, 0, ary.length);     aryRtn[0] = 1;     return aryRtn;     } else {     return ary;     }     }       Add two Single nodes      /**     * Definition for singly-linked list.     * public class ListNode {     * int val;     * ListNode next;     * ListNode(int x) { val = x; }     * }     */     class Solution {     public ListNode addTwoNumbers(ListNode l1, ListNode l2) {     ListNode lResult = new ListNode(0);     int carry=0,sum =0;     ListNode itNode = lResult; // [!] to setup an iterator     while(l1!=null || l2!=null){     // sum = l1.val + l2.val;     sum = (l1!=null?l1.val:0) + (l2!=null?l2.val:0); // to void null in get reference     sum += carry; // to accumulate 'carry'     carry = sum /10;     // lResult.val = sum % 10;     itNode.next = new ListNode(sum % 10);     itNode = itNode.next; // [!] this is the key step     if(l1!=null) {     l1 = l1.next;     }     if(l2!=null){     l2 = l2.next;     }     if(carry&gt;0) itNode.next = new ListNode(carry);     }     return lResult.next;     }     }            Longest unique characters              Naive approach              class Solution {     public int lengthOfLongestSubstring(String s) {     // analysis:     // embeded two loops to foreach every element and inner loop step from current element of 1st loop     // check whether each sub-string is all unique     // if so, get length and compare with global temp max length     int maxLen=0;     for(int i=0;i&lt;s.length();i++) {     //for inner loop, it start from i+1 (rather than i)     for(int j=i+1;i&lt;=s.length();j++){     if(noDup(s, i , j)){     maxLen = Math.max(maxLen, (j-i));     }     }     }     return maxLen;     }     private boolean noDup(String sub, int start, int end){     //check whether thsi sub string is unique     // char[] aryOccurance=new char[127];     // int[] aryOccurance=new int[127];     Set&lt;Character&gt; set=new HashSet&lt;&gt;();     for(int k=start;k&lt;end;k++){     Character c = sub.charAt(k);     if(set.contains(c)){     return false;     }else{     set.add(c);     }     }     return true;     }     }       Sliding window      [https://developpaper.com/share-several-algorithmic-interview-questions-related-to-sliding-window/]{.underline}     [Longest Substring Without Repeating Characters]{.underline}     public int lengthOfLongestSubstring(String s) {     Map&lt;Character, Integer&gt; map= new HashMap&lt;&gt;();// map to cache position of each occruance     int start=0, len=0;     // abba     for(int i=0; i&lt;s.length(); i++) {     char c = s.charAt(i);     if (map.containsKey(c)) {// here is the key step for “without repeating chars” in questions.     if (map.get(c) &gt;= start)     start = map.get(c) + 1;// found duplicate, so get started a new round, assign start from 1st occurance of ‘duplicate char’ plus one.     }     len = Math.max(len, i-start+1);     map.put(c, i);     }     return len;     }     My solution vs leetcode one, latter one is much more consice     // better solution leveraging slide window     // Runtime: 12 ms, faster than 26.95% of Java online submissions for Longest Substring Without Repeating Characters.     public int lengthOfLongestSubstring(String s){     Set&lt;Character&gt; set =new HashSet&lt;&gt;();     int maxLen = 0, left=0,right =-1, n=s.length();     while(left&lt;n) {     if((right+1)&lt;n &amp;&amp; !set.contains(s.charAt(right+1))){     // not in slide window     right++;//expand slide window     set.add(s.charAt(right));     }else{     // dup with existing slide window     // shrink window     set.remove(s.charAt(left));     left++;     }     maxLen = Math.max(maxLen, right - left +1);// [!] be carefulf there is \"+1\" as this is for getting count     }     return maxLen;     }     -----leetcode solution-----     public int lengthOfLongestSubstring(String s){     int i=0,j=0,n=s.length(),rtn=0;     Set&lt;Character&gt; set = new HashSet&lt;&gt;();     while(i&lt;n &amp;&amp; j&lt;n) {     if(!set.contains(s.charAt(j))) {     set.add(s.charAt(j++));     rtn = Math.max(rtn, j-i) ;     }else{     set.remove(s.charAt(i++));     }     }     return rtn;     }     -----------       Palindrome integer (not string)      class Solution {     public boolean isPalindrome(int x) {     //first of all, boundary (or edge case)     if(x&lt;0 || (x!=0 &amp;&amp; x%10==0)) {     return false;     }     int reverse =0;     while(x&gt; reverse) {     reverse = reverse * 10 + x%10;     x /= 10;     }     // When the length is an odd number, we can get rid of the middle digit by revertedNumber/10     // For example when the input is 12321, at the end of the while loop we get x = 12, revertedNumber = 123,     // since the middle digit doesn't matter in palindrome(it will always equal to itself), we can simply get rid of it.     return x == reverse || x == reverse/10;     }     }       Find longest common sub array among two arrays   [DP]      public static int findLength_dp(int[] A, int[] B) {     // for dynamic programing, normally it compare itself with its sibling, using max/min     // try to construct a matrix to keep track of path     int m=A.length,n=B.length,max=0;     int[][] memo = new int[m+1][n+1]; // \"+1\" to keep extra space     for(int i = 0;i &lt;= m;i++) {     for (int j = 0; j &lt;= n; j++) {     //for DP, firstly to setup begin point     if(i==0 || j==0) {     memo[i][j]=0;     }else{     if(A[i-1]==B[j-1]){ // it they are same     memo[i][j] = 1+ memo[i-1][j-1]; // increase one to cache     max = Math.max(max,memo[i][j]); // get global max     }     }     }     }     return max;     }       Dynamic programing:   重叠子问题、最优子结构、状态转移方程就是动态规划三要素。   What is dynamic programming?   Simply put, dynamic programming is an optimization technique that we can use to solve problems where the same work is being repeated over and over. You know how a web server may use caching? Dynamic programming is basically that.   However, dynamic programming doesn’t work for every problem. There are a lot of cases in which dynamic programming simply won’t help us improve the runtime of a problem at all. If we aren’t doing repeated work, then no amount of caching will make any difference.   A problem can be optimized using dynamic programming if it:           has an optimal substructure.            has overlapping subproblems       [Optimal substructure]{.underline} simply means that you can find the optimal solution to a problem by considering the optimal solution to its subproblems.   Overlapping Subproblems   Overlapping subproblems is the second key property that our problem must have to allow us to optimize using dynamic programming. Simply put, having overlapping subproblems means we are computing the same problem more than once.   Imagine you have a server that caches images. If the same image gets requested over and over again, you’ll save a ton of time. However, if no one ever requests the same image more than once, what was the benefit of caching them?   [Dynamic Programming Methods]{.underline}   DP offers two methods to solve a problem:   1. Top-down with Memoization   In this approach, we try to solve the bigger problem by recursively finding the solution to smaller sub-problems. Whenever we solve a sub-problem, we cache its result so that we don’t end up solving it repeatedly if it’s called multiple times. Instead, we can just return the saved result. This technique of storing the results of already solved subproblems is called Memoization.   2. Bottom-up with Tabulation   Tabulation is the opposite of the top-down approach and avoids recursion. In this approach, we solve the problem “bottom-up” (i.e. by solving all the related sub-problems first). This is typically done by filling up an n-dimensional table. Based on the results in the table, the solution to the top/original problem is then computed.   Tabulation is the opposite of Memoization, as in Memoization we solve the problem and maintain a map of already solved sub-problems. In other words, in memoization, we do it top-down in the sense that we solve the top problem first (which typically recurses down to solve the sub-problems).   Let’s apply Tabulation to our example of Fibonacci numbers. Since we know that every Fibonacci number is the sum of the two preceding numbers, we can use this fact to populate our table.   Here is the code for our bottom-up dynamic programming approach:   class Fibonacci {   public int CalculateFibonacci(int n) {   int dp[] = new int[n+1];   //base cases   dp[0] = 0;   dp[1] = 1;   for(int i=2; i&lt;=n; i++)   dp[i] = dp[i-1] + dp[i-2];   return dp[n];   }   public static void main(String[] args) {   Fibonacci fib = new Fibonacci();   System.out.println(\"5th Fibonacci is ---&gt; \" + fib.CalculateFibonacci(5));   System.out.println(\"6th Fibonacci is ---&gt; \" + fib.CalculateFibonacci(6));   System.out.println(\"7th Fibonacci is ---&gt; \" + fib.CalculateFibonacci(7));   }   }   Generally speaking, dynamic programming is the technique of storing repeated computations in memory, rather than recomputing them every time you need them. The ultimate goal of this process is to improve runtime. Dynamic programming allows you to use more space to take less time.   Dynamic programming relies on overlapping subproblems, because it uses memory to save the values that have already been computed to avoid computing them again. The more overlap there is, the more computational time is saved.   Top-down and bottom-up   Top-down and bottom-up refer to two general approaches to dynamic programming. A top-down solution starts with the final result and recursively breaks it down into subproblems. The bottom-up method does the opposite. It takes an iterative approach to solve the subproblems first and then works up to the desired solution.   both solutions are equally valid and that one solution can be determined from the other. In an interview situation, although bottom-up solutions often result in more concise code, either approach is appropriate. I recommend that you use whatever solution makes the most sense to you.   The important point is that top-down = recursive and bottom-up = iterative.      There are four steps in the FAST method:            First solution            Analyze the first solution            Identify the Subproblems            Turn the solution around       First solution   This is an important step for any interview question but is particularly important for dynamic programming. This step finds the first possible solution. This solution will be brute force and recursive. The goal is to solve the problem without concern for efficiency. It means that if you need to find the biggest/ smallest/longest/shortest something, you should write code that goes through every possibility and then compares them all to find the best one.   Your solution must also meet these restrictions:      The recursive calls must be self-contained. That means no global            variables.            You cannot do tail recursion. Your solution must compute the results            to each subproblem and then combine them afterwards.            Do not pass in unnecessary variables. Eg. If you can count the depth            of your recursion as you return, don’t pass a count variable into your recursive function.               Analyze the first solution    In this step, we will analyze the first solution that you came up with. This involves determining the time and space complexity of your first solution and asking whether there is obvious room for improvement.   // Compute the nth Fibonacci number recursively. // Optimized by caching subproblem results public int fib(int n) {      if (n &lt; 2) return n;     // Create cache and initialize to -1     int[] cache = new int[n+1];     for (int i = 0; i &lt; cache.length; i++) {     cache[i] = -1;     }     // Fill initial values in cache     cache[0] = 0;     cache[1] = 1;     return fib(n, cache);    }   // Overloaded private method   private int fib(int n, int[] cache) {   // If value is set in cache, return      if (cache[n] &gt;= 0) return cache[n];     // Compute and add to cache before returning     cache[n] = fib(n-1, cache) + fib(n-2, cache);     return cache[n];     }    Fig 3. Top-down dynamic Fibonacci solution      Turn the solution around    Since we now have a top-down solution, it is possible to reverse the process and solve it from the bottom up. This [can be done by starting with the base cases and building up the solution from there by computing the results of each subsequent subproblem, until we reach our result.]{.underline}   In this problem, [our base cases are fib(0) = 0 and fib(1) = 1. From these two values, we can compute the next largest Fibonacci number, fib(2) = fib(0) + fib(1). Once we have the value of fib(2), we can calculate fib(3) etc. As we successively compute each Fibonacci number, the previous values are saved and referred to as necessary, eventually reaching fib(n).]{.underline}   Our code for this process is fairly straightforward (fig 5).   This process yields a bottom-up solution. Since we iterate through all of the numbers from 0 to n once, our time complexity will be O(n) and our space will also be O(n), since we create a 1D array from 0 to n. This makes our current solution comparable to the top-down solution, although without recursion. This code is likely easier to understand.   // Compute the nth Fibonacci number iteratively      public int fib(int n) {     if (n == 0) return 0;     // Initialize cache     int[] cache = new int[n+1];     cache[1] = 1;     // Fill cache iteratively     for (int i = 2; i &lt;= n; i++) {     cache[i] = cache[i-1] + cache[i-2];     }     return cache[n];     }     Fig 5. Bottom-up dynamic Fibonacci solution     it is possible to improve our solution further. During the computation process, we only refer to the most recent two subproblems (cache[i-1] and cache[i-2]) to compute the value of the current subproblem. Therefore, cache[0] through cache[i-3] are unnecessary and do not need to be kept in memory.    We can, therefore, improve the space complexity of our solution to O(1) by only caching the most recent two values.      // Compute the nth Fibonacci number iteratively // with constant space. We only need to save // the two most recently computed values     public int fib(int n) {     if (n &lt; 2) return n;     int n1 = 1, n2 = 0;     for (int i = 2; i &lt; n; i++) {     int n0 = n1 + n2;     n2 = n1;     n1 = n0;     }     return n1 + n2;     }    For any problem where you are asked [to find the most/least/ largest/smallest]{.underline} etc, an excellent technique [is to compare every possible combination]{.underline}. Although it will be inefficient, efficiency is not the most important current consideration and a solution of that nature is easy to make dynamic.      Make change     // Brute force solution. Go through every     // combination of coins that sum up to c to // find the minimum number     public static int makeChange(int c) {     int[] coins = new int[]{10, 6, 1};     if (c == 0) return 0;     int minCoins = Integer.MAX_VALUE;     // Try removing each coin from the total and // see how many more coins are required     for (int coin : coins) {     // Skip a coin if it’s value is greater     // than the amount remaining     if (c - coin &gt;= 0) {     int currMinCoins = makeChange(c - coin);     if (currMinCoins &lt; minCoins)     minCoins = currMinCoins;     } }     // Add back the coin removed recursively     return minCoins + 1;     }       How to convert one naive loop solution to dynamic programming            top-down approach            Based on this understanding, we can turn our solution into a top-down dynamic solution. We can cache the results as they are computed. That means that we will cache the minimum number of coins needed to make various smaller amounts of change.   Like the Fibonacci problem, our code doesn’t actually have to change very much. It’s only necessary to overload our function with another that can initialize the cache. Then we update the original function in order to check the cache before doing the computation and saving the result to the cache afterwards   // Top down dynamic solution. Cache the values as we compute them   // transform naive approach to top-down need:   // overload existing method with new one accept cache   // while existing one do two tasks: (1) initialize cache (2) call new method passing in cache   public int makeChange_top_down(int c) {   // Initialize cache with values as -1   int[] cache = new int[c + 1];   for (int i = 1; i &lt; c + 1; i++)   cache[i] = -1;   return makeChange_top_down(c, cache);   }   // Overloaded recursive function   private int makeChange_top_down(int c, int[] cache) {   int[] coins = new int[]{10, 6, 1};   // Return the value if it’s in the cache   if (cache[c] &gt;= 0) return cache[c];   int minCoins = Integer.MAX_VALUE; //declare result oppositely, e.g. question is \"min\", so init return value would be Integer.MAX_VALUE   // Find the best coin   for (int coin : coins) {   if (c - coin &gt;= 0) {   int currMinCoins =   makeChange_top_down(c - coin, cache);   if (currMinCoins &lt; minCoins)   minCoins = currMinCoins;   } }   // Save the value into the cache   cache[c] = minCoins + 1; // add one to the return value   return cache[c];   }   Turn the solution around   Once the top-down solution is completed, it’s possible to flip it around. We do this by solving the same subproblems in reverse order. Rather than starting with our result in mind, we start with no change and work our way up until we reach the solution.   The next step is to determine the subproblems that must be solved, in order to solve successive subproblems. If we want to compute makeChange(c), then we will have n different subproblems. If our coins are {10, 6, 1}, we need to have the solutions for makeChange(c - 10), makeChange(c - 6), and makeChange(c - 1).   Once makeChange() is solved for 0 through c - 1, it will be easy to compute the value of makeChange(c). This is done by using the first value, 0 as our base case. We can then compute the remaining values from the previously computed values.   // Bottom up dynamic programming solution. // Iteratively compute number of coins for // larger and larger amounts of change   public int makeChange(int c) {   int[] cache = new int[c + 1];   for (int i = 1; i &lt;= c; i++) {   int minCoins = Integer.MAX_VALUE;   // Try removing each coin from the total   // and see which requires the fewest   // extra coins   for (int coin : coins) {   if (i - coin &gt;= 0) {   int currCoins = cache[i-coin] + 1;   if (currCoins &lt; minCoins) {   minCoins = currCoins;   }   } }   cache[i] = minCoins;   }   return cache[c];   }  ","categories": [],
        "tags": ["Leetcode","Algorithm"],
        "url": "/2020/03/03/Algorithm-Leecode-1.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Java Concurrent",
        "excerpt":"This blog is about noteworthy pivot points about Java Concurrent Framework     Back to Java old days there were wait()/notify() which is error prone, while from Java 5.0 there was Concurrent framework being introduced, this page list some pivot points.    CountDownLatch     CountDownLatch in Java is a kind of synchronizer which allows one Thread  to wait for one or more Threads before starts processing.   You can also implement same functionality using  wait and notify mechanism in Java but it requires lot of code and getting it write in first attempt is tricky,  With CountDownLatch it can  be done in just few lines.   One of the disadvantage of CountDownLatch is that its not reusable once count reaches to zero you can not use CountDownLatch any more, but don’t worry Java concurrency API has another concurrent utility called CyclicBarrier for such requirements.     When to use CountDownLatch      Classical example of using CountDownLatch in Java  is any server side core Java application which uses services architecture,  where multiple services is provided by multiple threads and application can not start processing  until all services have started successfully as shown in our CountDownLatch example.       Summary     Main Thread wait on Latch by calling CountDownLatch.await() method while other thread calls CountDownLatch.countDown() to inform that they have completed.   CyclicBarrier     there is different you can not reuse CountDownLatch once the count reaches zero while you can reuse CyclicBarrier by calling reset() method which resets Barrier to its initial State. What it implies that CountDownLatch is a good for one-time events like application start-up time and CyclicBarrier can be used to in case of the recurrent event e.g. concurrently calculating a solution of the big problem etc.   a simple example of CyclicBarrier in Java on which we initialize CyclicBarrier with 3 parties, means in order to cross barrier, 3 thread needs to call await() method. each thread calls await method in short duration but they don’t proceed until all 3 threads reached the barrier, once all thread reach the barrier, barrier gets broker and each thread started their execution from that point.   Sample can be found at CyclicBarrierDemo.java     Use cases:       To implement multi player game which can not begin until all player has joined.   Perform lenghty calculation by breaking it into smaller individual tasks. In general, to implement Map-Reduce technique.   CyclicBarrier can perform a completion task once all thread reaches to the barrier, This can be provided while creating CyclicBarrier.   If CyclicBarrier is initialized with 3 parties means 3 thread needs to call await method to break the barrier.   The thread will block on await() until all parties reach to the barrier, another thread interrupt or await timed out.   CyclicBarrier.reset() put Barrier on its initial state, other thread which is waiting or not yet reached barrier will terminate with java.util.concurrent.BrokenBarrierException.   ThreadLocal     ThreadLocal in Java is another way to achieve thread-safety apart from writing immutable classes.   ThreadLocal in Java is a different way to achieve thread-safety, it doesn’t address synchronization requirement, instead it eliminates sharing by providing explicitly copy of Object to each thread.   Since Object is no more shared there is no requirement of Synchronization which can improve scalability and performance of application.   One of the classic example of ThreadLocal is sharing SimpleDateForamt. Since SimpleDateFormat is not thread safe, having a global formatter may not work but having per Thread formatter will certainly work. but it can be source of severe memory leak and java.lang.OutOfMemoryError if not used carefully. so avoid until you don’t have any other option.   Semaphone     Semaphore provides two main method acquire() and release() for getting permits and releasing permits. acquire() method blocks until permit is available.   Semaphore provides both blocking method as well as unblocking method to acquire permits. This Java concurrency tutorial focus on a very simple example of Binary Semaphore and demonstrate how mutual exclusion can be achieved using Semaphore in Java.     Binary Semaphone      a Counting semaphore with one permit is known as binary semaphore because it has only two state permit available or permit unavailable. Binary semaphore can be used to implement mutual exclusion or critical section where only one thread is allowed to execute. Thread will wait on acquire() until Thread inside critical section release permit by calling release() on semaphore.       Scenarios usage  1) To implement better Database connection pool which will block if no more connection is available instead of failing and handover Connection as soon as its available. 2) To put a bound on collection classes. by using semaphore you can implement bounded collection whose bound is specified by counting semaphore. 3)  That’s all on Counting semaphore example in Java. Semaphore is real nice concurrent utility which can greatly simply design and implementation of bounded resource pool. Java 5 has added several useful  concurrent utility and deserve a better attention than casual look.   Race condition     Race conditions occurs when two thread operate on same object without proper synchronization and there operation interleaves on each other. Classical example of Race condition is incrementing a counter since increment is not an atomic operation and can be further divided into three steps like read, update and write. if two threads tries to increment count at same time and if they read same value because of interleaving of read operation of one thread to update operation of another thread, one count will be lost when one thread overwrite increment done by other thread.   I found that two code patterns namely “check and act” and “read modify write” can suffer race condition if not synchronized properly.            classical example of “check and act” race condition in Java is getInstance() method of Singleton Class,       put if absent scenario. consider below code           if(!hashtable.contains(key)){   hashtable.put(key,value);   }  Fix race condition:  -In order to fix this race condition in Java you need to wrap this code inside synchronized block which makes them atomic together because no thread can go inside synchronized block if one thread is already there.     IllegalMonitorStateException in Java which will occur if we don’t call wait (), notify () or notifyAll () method from synchronized context.   Any potential race condition between wait and notify method in Java   Thread in Java  details:     A thread is essentialy a subdivision of a process, or LWP: lightweight process.   Crucially, each process has its own memory space.   A thread is a subdivision that shares the memory space of its parent process.   Threads belonging to a process usually share a few other key resources as well, such as their working directory, environment variables, file handles etc.   On the other hand, each thread has its own private stack and registers, including program counter.  program counter (PC) register keeps track of the current instruction executing at any moment. That is like a pointer to the current instruction in sequence of instructions in a program.   Method area: In general, method area is a logical part of heap area. But that is left to the JVM implementers to decide.  Method area has per class structures and fields. Nothing but static fields and structures.   Depending on the OS, threads may have some other private resources too, such as thread-local storage (effectively, a way of referring to “variable number X”, where each thread has its own private value of X).   Wait &amp; Notify     Since wait method is not defined in Thread class, you cannot simply call Thread.wait(), that won’t work but since many Java developers are used to calling Thread.sleep() they try the same thing with wait() method and stuck.   You need to call wait() method on the object which is shared between two threads, in producer-consumer problem its the queue which is shared between producer and consumer threads.     synchronized(lock){   while(!someCondition){       lock.wait();   } }           Tips     Always call wait(), notify() and notifyAll() methods from synchronized method or synchronized block otherwise JVM will throw IllegalMonitorStateException.   Always call wait and notify method from a loop and never from if() block, because loop test waiting condition before and after sleeping and handles notification even if waiting for the condition is not changed.   Always call wait in shared object e.g. shared queue in this example.   Prefer notifyAll() over notify() method due to reasons given in this article.   Fork-Join     Fork/join tasks is “pure” in-memory algorithms in which no I/O operations come into picture.it is based on a work-stealing algorithm.   Java’s most attractive part is it makes things easier and easier.   its really challenging where several threads are working together to accomplish a large task so again java has tried to make things easy and simplifies this concurrency using Executors and Thread Queue.   it work on divide and conquer algorithm and create sub-tasks and communicate with each other to complete.   New fork-join executor framework has been created which is responsible for creating one new task object which is again responsible for creating new sub-task object and waiting for sub-task to be completed.internally it maintains a thread pool and executor assign pending task to this thread pool to complete when one task is waiting for another task to complete. whole Idea of fork-join framework is to leverage multiple processors of advanced machine.   Thread.yield()     This static method is essentially used to notify the system that the current thread is willing to “give up the CPU” for a while. The general idea is that: The thread scheduler will select a different thread to run instead of the current one. However, the details of how yielding is implemented by the thread scheduler differ from platform to platform. In general, you shouldn’t rely on it behaving in a particular way. Things that differ include:            when, after yielding, the thread will get an opportunity to run again;       whether or not the thread foregoes its remaining quantum.           Windows     In the Hotspot implementation, the way that Thread.yield() works has changed between Java 5 and Java 6.   In Java 5, Thread.yield() calls the Windows API call Sleep(0). This has the special effect of clearing the current thread’s quantum and putting it to the end of the queue for its priority level. In other words, all runnable threads of the same priority (and those of greater priority) will get a chance to run before the yielded thread is next given CPU time. When it is eventually re-scheduled, it will come back with a full quantum, but doesn’t “carry over” any of the remaining quantum from the time of yielding. This behaviour is a little different from a non-zero sleep where the sleeping thread generally loses 1 quantum value (in effect, 1/3 of a 10 or 15ms tick).   In Java 6, this behaviour was changed. The Hotspot VM now implements Thread.yield() using the Windows SwitchToThread() API call. This call makes the current thread give up its current timeslice, but not its entire quantum. This means that depending on the priorities of other threads, the yielding thread can be scheduled back in one interrupt period later.   Linux     Under Linux, Hotspot simply calls sched_yield(). The consequences of this call are a little different, and possibly more severe than under Windows:            a yielded thread will not get another slice of CPU until all other threads have had a slice of CPU;       (at least in kernel 2.6.8 onwards), the fact that the thread has yielded is implicitly taken into account by the scheduler’s heuristics on its recent CPU allocation — thus, implicitly, a thread that has yielded could be given more CPU when scheduled in the future.           When to use yield()?     I would say practically never. Its behaviour isn’t standardly defined and there are generally better ways to perform the tasks that you might want to perform with yield():   if you’re trying to use only a portion of the CPU, you can do this in a more controllable way by estimating how much CPU the thread has used in its last chunk of processing, then sleeping for some amount of time to compensate: see the sleep() method;   if you’re waiting for a process or resource to complete or become available, there are more efficient ways to accomplish this, such as by using join() to wait for another thread to complete, using the wait/notify mechanism to allow one thread to signal to another that a task is complete, or ideally by using one of the Java 5 concurrency constructs such as a Semaphore or blocking queue.   Thread Scheduling     thread scheduler, part of the OS (usually) that is responsible for sharing the available CPUs out between the various threads. How exactly the scheduler works depends on the individual platform, but various modern operating systems (notably Windows and Linux) use largely similar techniques that we’ll describe here.   Note that we’ll continue to talk about a single thread scheduler. On multiprocessor systems, there is generally some kind of scheduler per processor, which then need to be coordinated in some way.   Across platforms, thread scheduling tends to be based on at least the following criteria:            a priority, or in fact usually multiple “priority” settings that we’ll discuss below;       a quantum, or number of allocated timeslices of CPU, which essentially determines the amount of CPU time a thread is allotted before it is forced to yield the CPU to another thread of the same or lower priority (the system will keep track of the remaining quantum at any given time, plus its default quantum, which could depend on thread type and/or system configuration);       a state, notably “runnable” vs “waiting”;       metrics about the behaviour of threads, such as recent CPU usage or the time since it last ran (i.e. had a share of CPU), or the fact that it has “just received an event it was waiting for”.           Most systems use what we might dub priority-based round-robin scheduling to some extent. The general principles are:            a thread of higher priority (which is a function of base and local priorities) will preempt a thread of lower priority;       otherwise, threads of equal priority will essentially take turns at getting an allocated slice or quantum of CPU;       there are a few extra “tweaks” to make things work.           States   Depending on the system, there are various states that a thread can be in. Probably the two most interesting are:     runnable, which essentially means “ready to consume CPU”; being runnable is generally the minimum requirement for a thread to actually be scheduled on to a CPU;   waiting, meaning that the thread currently cannot continue as it is waiting for a resource such as a lock or I/O, for memory to be paged in, for a signal from another thread, or simply for a period of time to elapse (sleep).   Other states include terminated, which means the thread’s code has finished running but not all of the thread’s resources have been cleared up, and a new state, in which the thread has been created, but not all resources necessary for it to be runnable have been created.   Quanta and clock ticks     Each thread has a quantum, which is effectively how long it is allowed to keep hold of the CPU if:            it remains runnable;       the scheduler determines that no other thread needs to run on that CPU instead.           Thread quanta are generally defined in terms of some number of clock ticks. If it doesn’t otherwise cease to be runnable, the scheduler decides whether to preempt the currently running thread every clock tick. As a rough guide:            a clock tick is typically 10-15 ms under Windows; under Linux, it is 1ms (kernel 2.6.8 onwards);       a quantum is usually a small number of clock ticks, depending on the OS: either 2, 6 or 12 clock ticks on Windows, depending on whether Windows is running in “server” mode:                          Windows mode       Foreground process       Non-foreground process                       Normal       6 ticks       2 ticks                 Server       12 ticks       12 ticks           between 10-200 clock ticks (i.e. 10-200 ms) under Linux, though some granularity is introduced in the calculation— see below. a thread is usually allowed to “save up” unused quantum, up to some limit and granularity.     In Windows, a thread’s quantum allocation is fairly stable. In Linux, on the other hand, a thread’s quantum is dynamically adjusted when it is scheduled, depending partly on heuristics about its recent resource usage and partly on a nice value   Switching and scheduling algorithms      At key moments, the thread scheduler considers whether to switch the thread that is currently running on a CPU. These key moments are usually:            periodically, via an interrupt routine, the scheduler will consider whether the currently running thread on each CPU has reached the end of its allotted quantum;       at any time, a currently running thread could cease to be runnable (e.g. by needing to wait, reaching the end of its execution or being forcibly killed);       when some other attribute of the thread changes (e.g. its priority or processor affinity4) which means that which threads are running needs to be re-assessed.           At these decision points, the scheduler’s job is essentially to decide, of all the runnable threads, which are the most appropriate to actually be running on the available CPUs. Potentially, this is quite a complex task. But we don’t want the scheduler to waste too much time deciding “what to do next”. So in practice, a few simple heuristics are used each time the scheduler needs to decide which thread to let run next:            there’s usually a fast path for determining that the currently running thread is still the most appropriate one to continue running (e.g. storing a bitmask of which priorities have runnable threads, so the scheduler can quickly determine that there’s none of a higher priority than that currently running);       if there is a runnable thread of higher priority than the currently running one, then the higher priority one will be scheduled in3;       if a thread is “preempted” in this way, it is generally allowed to keep its remaining quantum and continue running when the higher-priority thread is scheduled out again;       when a thread’s quantum runs out, the thread is “put to the back of the queue” of runnable threads with the given priority and if there’s no queued (runnable) thread of higher priority, then next thread of the same priority will be scheduled in;       at the end of its quantum, if there’s “nothing better to run”, then a thread could immediately get a new quantum and continue running;       a thread typically gets a temporary boost to its quantum and/or priority at strategic points.           Quantum and priority boosting Both Windows and Linux (kernel 2.6.8 onwards) implement temporary boosting. Strategic points at which a thread may be given a “boost” include:            when it has just finished waiting for a lock/signal or I/O5;       when it has not run for a long time (in Windows, this appears to be a simple priority boost after a certain time; in Linux, there is an ongoing calculation based on the thread’s nice value and its recent resource usage);       when a GUI event occurs;       while it owns the focussed window (recent versions of Windows give threads of the owning process a larger quantum; earlier versions give them a priority boost).           Context switching     context switching. Roughly speaking, this is the procedure that takes place when the system switches between threads running on the available CPUs.   the thread scheduler must actually manage the various thread structures and make decisions about which thread to schedule next where, and every time the thread running on a CPU actually changes— often referred to as a context switch   switching between threads of different processes (that is, switching to a thread that belongs to a different process from the one last running on that CPU) will carry a higher cost, since the address-to-memory mappings must be changed, and the contents of the cache almost certainly will be irrelevant to the next process.   Context switches appear to typically have a cost somewhere between 1 and 10 microseconds (i.e. between a thousandth and a hundredth of a millisecond) between the fastest and slowest cases (same-process threads with little memory contention vs different processes). So the following are acceptable: 1 nanoseconds is billionth of one second,  1 microsecond is millionth of one second, 1 millisecond is thousandth of one second   What causes too many slow context switches in Java?     Every time we deliberately change a thread’s status or attributes (e.g. by sleeping, waiting on an object, changing the thread’s priority etc), we will cause a context switch. But usually we don’t do those things so many times in a second to matter. Typically, the cause of excessive context switching comes from contention on shared resources, particularly synchronized locks:            rarely, a single object very frequently synchronized on could become a bottleneck;       more frequently, a complex application has several different objects that are each synchronized on with moderate frequency, but overall, threads find it difficult to make progress because they keep hitting different contended locks at regular intervals.           Avoiding contention and context switches in Java      Firstly, before hacking with your code, a first course of action is upgrading your JVM, particularly if you are not yet using Java 6. Most new Java JVM releases have come with improved synchronization optimisation.   Then, a high-level solution to avoiding synchronized lock contention is generally to use the various classes from the Java 5 concurrency framework (see the java.util.concurrent package). For example, instead of using a HashMap with appropriate synchronization, a ConcurrentHashMap can easily double the throughput with 4 threads and treble it with 8 threads (see the aforementioned link for some ConcurrentHashMap performance measurements). A replacement to synchronized with often better concurrency is offered with various explicit lock classes (such as ReentrantLock).   Java thread priority     Lower-priority threads are given CPU when all higher priority threads are waiting (or otherwise unable to run) at that given moment.   Thread priority isn’t very meaningful when all threads are competing for CPU.   The number should lie in the range of two constants MIN_PRIORITY and MAX_PRIORITY defined on Thread, and will typically reference NORM_PRIORITY, the default priority of a thread if we don’t set it to anything else.   For example, to give a thread a priority that is “half way between normal and maximum”, we could call:     thr.setPriority((Thread.MAX_PRIORITY - Thread.NORM_PRIORITY) / 2);           ####### Some points about thread property     depending on your OS and VM version, Thread.setPriority() may actually do nothing at all (see below for details);   what thread priorities mean to the thread scheduler differs from scheduler to scheduler, and may not be what you intuitively presume; in particular: Priority may not indicate “share of the CPU”. As we’ll see below, it turns out that “priority” is more or less an indication of CPU distribution on UNIX systems, but not under Windows.   thread priorities are usually a combination of “global” and “local” priority settings, and Java’s setPriority() method typically works only on the local priority— in other words, you can’t set priorities across the entire range possible (this is actually a form of protection— you generally don’t want, say, the mouse pointer thread or a thread handling audio data to be preempted by some random user thread);   the number of distinct priorities available differs from system to system, but Java defines 10 (numbered 1-10 inclusive), so you could end up with threads that have different priorities under one OS, but the same priority (and hence unexpected behaviour) on another;   most operating systems’ thread schedulers actually perform temporary manipulations to thread priorities at strategic points (e.g. when a thread receives an event or I/O it was waiting for), and often “the OS knows best”; trying to manually manipulate priorities could just interfere with this system;   your application doesn’t generally know what threads are running in other processes, so the effect on the overall system of changing the priority of a thread may be hard to predict. So you might find, for example, that your low-priority thread designed to “run sporadically in the background” hardly runs at all due to a virus dection program running at a slightly higher (but still ‘lower-than-normal’) priority, and that the performance unpredictably varies depending on which antivirus program your customer is using. Of course, effects like these will always happen to some extent or other on modern systems.   Thread scheduling implications in Java   Thread Control     the granularity and responsiveness of the Thread.sleep() method is largely determined by the scheduler’s interrupt period and by how quickly the slept thread becomes the “chosen” thread again;   the precise function of the setPriority() method depends on the specific OS’s interpretation of priority (and which underlying API call Java actually uses when several are available): for more information, see the more detailed section on thread priority;   the behaviour of the Thread.yield() method is similarly determined by what particuar underlying API calls do, and which is actually chosen by the VM implementation.   “Granularity” of threads     Although our introduction to threading focussed on how to create a thread, it turns out that it isn’t appropriate to create a brand new thread just for a very small task. Threads are actually quite a “coarse-grained” unit of execution, for reasons that are hopefully becoming clear from the previous sections.   Overhead and limits of creating and destroying threads     creating and tearing down threads isn’t free: there’ll be some CPU overhead each time we do so;   there may be some moderate limit on the number of threads that can be created, determined by the resources that a thread needs to have allocated (if a process has 2GB of address space, and each thread as 512K of stack, that means a maximum of a few thousands threads per process).   Avoiding thread overhead in Java     In applications such as servers that need to continually execute short, multithreaded tasks, the usual way to avoid the overhead of repeated thread creation is to create a thread pool.   Dinnig Philosophers problem     The problem was designed to illustrate the challenges of avoiding deadlock, a system state in which no progress is possible. To see that a proper solution to this problem is not obvious, consider a proposal in which each philosopher is instructed to behave as follows:            think until the left fork is available; when it is, pick it up;       think until the right fork is available; when it is, pick it up;       when both forks are held, eat for a fixed amount of time;       then, put the right fork down;       then, put the left fork down;       repeat from the beginning.           This attempted solution fails because it allows the system to reach a deadlock state, in which no progress is possible. This is a state in which each philosopher has picked up the fork to the left, and is waiting for the fork to the right to become available, vice versa. With the given instructions, this state can be reached, and when it is reached, the philosophers will eternally wait for each other to release a fork   Resource starvation might also occur independently of deadlock if a particular philosopher is unable to acquire both forks because of a timing problem. For example, there might be a rule that the philosophers put down a fork after waiting ten minutes for the other fork to become available and wait a further ten minutes before making their next attempt.   This scheme eliminates the possibility of deadlock (the system can always advance to a different state) but still suffers from the problem of livelock. If all five philosophers appear in the dining room at exactly the same time and each picks up the left fork at the same time the philosophers will wait ten minutes until they all put their forks down and then wait a further ten minutes before they all pick them up again.     Solutions      Arbitrator solution      Another approach is to guarantee that a philosopher can only pick up both forks or none by introducing an arbitrator, e.g., a waiter. In order to pick up the forks, a philosopher must ask permission of the waiter. The waiter gives permission to only one philosopher at a time until the philosopher has picked up both of their forks. Putting down a fork is always allowed. The waiter can be implemented as a mutex. In addition to introducing a new central entity (the waiter), this approach can result in reduced parallelism. if a philosopher is eating and one of their neighbors is requesting the forks, all other philosophers must wait until this request has been fulfilled even if forks for them are still available.       Queue  What is the difference between poll() and remove() method of Queue interface? (answer)     Though both poll() and remove() method from Queue is used to remove the object and returns the head of the queue, there is a subtle difference between them. If Queue is empty() then a call to remove() method will throw Exception, while a call to poll() method returns null.   What is the difference between fail-fast and fail-safe Iterators?     Fail-fast Iterators throws ConcurrentModificationException when one Thread is iterating over collection object and other thread structurally modify Collection either by adding, removing or modifying objects on underlying collection. They are called fail-fast because they try to immediately throw Exception when they encounter failure. On the other hand fail-safe Iterators works on copy of collection instead of original collection   To remove entry from collection     you need to use Iterator’s remove() method. This method removes current element from Iterator’s perspective. If you use Collection’s or List’s remove() method during iteration then your code will throw ConcurrentModificationException. That’s why it’s advised to use Iterator remove() method to remove objects from Collection.   What is the difference between Synchronized Collection and Concurrent Collection?     One Significant difference is that Concurrent Collections has better performance than synchronized Collection ** because they **lock only a portion of Map to achieve concurrency and Synchronization.   When do you use ConcurrentHashMap in Java     ConcurrentHashMap is better suited for situation where you have multiple readers and one Writer or fewer writers since Map gets locked only during the write operation. If you have an equal number of reader and writer than ConcurrentHashMap will perform in the line of Hashtable or synchronized HashMap.   Sorting collections     Sorting is implemented using Comparable and Comparator in Java and when you call Collections.sort() it gets sorted based on the natural order specified in compareTo() method while Collections.sort(Comparator) will sort objects based on compare() method of Comparator.   Hashmap vs Hasset     HashSet implements java.util.Set interface and that’s why only contains unique elements, while HashMap allows duplicate values.  In fact, HashSet is actually implemented on top of java.util.HashMap.   What is NavigableMap in Java     NavigableMap Map was added in Java 1.6, it adds navigation capability to Map data structure. It provides methods like lowerKey() to get keys which is less than specified key, floorKey() to return keys which is less than or equal to specified key, ceilingKey() to get keys which is greater than or equal to specified key and higherKey() to return keys which is greater specified key from a Map. It also provide similar methods to get entries e.g. lowerEntry(), floorEntry(), ceilingEntry() and higherEntry(). Apart from navigation methods, it also provides utilities to create sub-Map e.g. creating a Map from entries of an exsiting Map like tailMap, headMap and subMap. headMap() method returns a NavigableMap whose keys are less than specified, tailMap() returns a NavigableMap whose keys are greater than the specified and subMap() gives a NavigableMap between a range, specified by toKey to fromKey   Array vs ArrayList     Array is fixed length data structure, once created you can not change it’s length. On the other hand, ArrayList is dynamic, it automatically allocate a new array and copies content of old array, when it resize.   Another reason of using ArrayList over Array is support of Generics.   Can we replace Hashtable with ConcurrentHashMap?     Since Hashtable locks whole Map instead of a portion of Map, compound operations like if(Hashtable.get(key) == null) put(key, value) works in Hashtable but not in concurrentHashMap. instead of this use putIfAbsent() method of ConcurrentHashMap   What is CopyOnWriteArrayList, how it is different than ArrayList and Vector     CopyOnWriteArrayList is new List implementation introduced in Java 1.5 which provides better concurrent access than Synchronized List. better concurrency is achieved by Copying ArrayList over each write and replace with original instead of locking. Also CopyOnWriteArrayList doesn’t throw any ConcurrentModification Exception. Its different than ArrayList because its thread-safe and ArrayList is not thread-safe and it’s different than Vector in terms of Concurrency. CopyOnWriteArrayList provides better Concurrency by reducing contention among readers and writers.   Why ListIterator has added() method but Iterator doesn’t or Why to add() method is declared in ListIterator and not on Iterator. (answer)     ListIterator has added() method because of its ability to traverse or iterate in both direction of the collection. it maintains two pointers in terms of previous and next call and in a position to add a new element without affecting current iteration.   What is BlockingQueue, how it is different than other collection classes? (answer)     BlockingQueue is a Queue implementation available in java.util.concurrent package. It’s one of the concurrent Collection class added on Java 1.5, main difference between BlockingQueue and other collection classes is that apart from storage, it also provides flow control. It can be used in inter-thread communication and also provides built-in thread-safety by using happens-before guarantee. You can use BlockingQueue to solve Producer Consumer problem, which is what is needed in most of concurrent applications.   You have thread T1, T2 and T3, how will you ensure that thread T2 run after T1 and thread T3 run after T2     To use join method.   Happen before     In computer science, the happened-before relation (denoted: → {\\displaystyle \\to \\;} \\to \\;) is a relation between the result of two events, such that if one event should happen before another event, the result must reflect that, even if those events are in reality executed out of order (usually to optimize program flow).   In Java specifically, a happens-before relationship is a guarantee that memory written to by statement A is visible to statement B, that is, that statement A completes its write before statement B starts its read   Concurrent framework     The advantage of using Callable over Runnable is that Callable can explicitly return a value.   Executors are a big step forward compared to plain old threads because executors ease the management of concurrent tasks.   Some types of algorithms exist that require tasks to create subtasks and communicate with each other to complete. Those are the “divide and conquer” algorithms, which are also referred to as “map and reduce,” in reference to the eponymous functions in functional languages.   The fork/join framework added to the java.util.concurrent package in Java SE 7 through Doug Lea’s efforts fills that gap. The Java SE 5 and Java SE 6 versions of java.util.concurrent helped in dealing with concurrency, and the additions in Java SE 7 help with parallelism.   First and foremost, fork/join tasks should operate as “pure” in-memory algorithms in which no I/O operations come into play. Also, communication between tasks through shared state should be avoided as much as possible, because that implies that locking might have to be performed.   The core addition is a new ForkJoinPool executor that is dedicated to running instances implementing ForkJoinTask. ForkJoinTask objects support the creation of subtasks plus waiting for the subtasks to complete. With those clear semantics, the executor is able to dispatch tasks among its internal threads pool by “stealing” jobs when a task is waiting for another task to complete and there are pending tasks to be run.   ForkJoinTask objects feature two specific methods:            The fork() method allows a ForkJoinTask to be planned for asynchronous execution. This allows a new ForkJoinTask to be launched from an existing one.       In turn, the join() method allows a ForkJoinTask to wait for the completion of another one.           There are two types of ForkJoinTask specializations:            Instances of RecursiveAction represent executions that do not yield a return value.       In contrast, instances of RecursiveTask yield return values. In general, RecursiveTask is preferred because most divide-and-conquer algorithms return a value from a computation over a data set.           The fork and join principle consists of two steps which are performed recursively. These two steps are the fork step and the join step.   A task that uses the fork and join principle can fork (split) itself into smaller subtasks which can be executed concurrently. This is illustrated in the diagram below:   By splitting itself up into subtasks, each subtask can be executed in parallel by different CPUs, or different threads on the same CPU.   The limit for when it makes sense to fork a task into subtasks is also called a threshold. It is up to each task to decide on a sensible threshold. It depends very much on the kind of work being done.   Once the subtasks have finished executing, the task may join (merge) all the results into one result.   Of course, not all types of tasks may return a result. If the tasks do not return a result then a task just waits for its subtasks to complete. No result merging takes place then.   The ForkJoinPool is a special thread pool which is designed to work well with fork-and-join task splitting. The ForkJoinPool located in the java.util.concurrent package, so the full class name is java.util.concurrent.ForkJoinPool.   You create a ForkJoinPool using its constructor. As a parameter to the ForkJoinPool constructor you pass the indicated level of parallelism you desire.   The parallelism level indicates how many threads or CPUs you want to work concurrently on on tasks passed to the ForkJoinPool.   You submit tasks to a ForkJoinPool similarly to how you submit tasks to an ExecutorService. You can submit two types of tasks. A task that does not return any result (an “action”), and a task which does return a result (a “task”).   Fork/Join framework details     ForkJoinPool is consists of ForkJoinTask array and ForkJoinWorkerThread array.            ForkJoinTask array contains tasks submitted to ForkJoinPool       ForkJoinWorkerThread array in charge of executing those tasks       When you call fork method on ForkJoinTask, program will call “pushTask” asynchronously of ForkJoinWorkerThread, and then return result right away.       “pushTask” will put current task into ForkJoinTask array queue, then execute “signalWork()” of ForkJoinPool to create a new thread to execute task.          final void pushTask(ForkJoinTask t) {    ForkJoinTask[] q; int s, m;    if ((q = queue) != null) {    // ignore if queue removed        long u = (((s = queueTop) &amp; (m = q.length - 1)) &lt;&lt; ASHIFT) + ABASE;        UNSAFE.putOrderedObject(q, u, t);        queueTop = s + 1;         // or use putOrderedInt        if ((s -= queueBase) &lt;= 2)            pool.signalWork();   else if (s == m)            growQueue();    }   }                       “join” method main functionality is blocking current thread and wait for resutls.            public final V join() {    if (doJoin() != NORMAL)        return reportResult();    else        return getRawResult();    }    private V reportResult() {            int s; Throwable ex;            if ((s = status) == CANCELLED)                throw new CancellationException();    if (s == EXCEPTIONAL &amp;&amp; (ex = getThrowableException()) != null)                UNSAFE.throwException(ex);            return getRawResult();    }                       When do call doJoin(), you can get status of curent thread. There are 4 status:                    NORMAL: completed           CANCELLED           SIGNAL           EXCEPTIONAL                       The method of doJoin()           private int doJoin() {   Thread t; ForkJoinWorkerThread w; int s; boolean completed;   if ((t = Thread.currentThread()) instanceof ForkJoinWorkerThread) {       if ((s = status) &lt; 0)           return s;       if ((w = (ForkJoinWorkerThread)t).unpushTask(this)) {           try {               completed = exec();           } catch (Throwable rex) {               return setExceptionalCompletion(rex);           }           if (completed)               return setCompletion(NORMAL);       }       return w.joinTask(this);   }   else       return externalAwaitDone();   }                           newTaskFor   If a SocketUsingTask is cancelled through its Future, the socket is closed and the   As of Java 6, ExecutorService implementations can override newTaskFor in AbstractExecutorService to control instantiation of the Future corresponding to a submitted Callable or Runnable. The default implementation just creates a new FutureTask, as shown in Listing 6.12.  protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Callable&lt;T&gt; task) {  return new FutureTask&lt;T&gt;(task);  }   Listing 6.12. Default implementation of newTaskFor in ThreadPoolExecutor.   Thread shutdown     Sensible encapsulation practices dictate that you should not manipulate a thread—interrupt it, modify its priority, etc.—unless you own it. The thread API has no formal concept of thread ownership: a thread is represented with a Thread object that can be freely shared like any other object. However, it makes sense to think of a thread as having an owner, and this is usually the class that created the thread. So a thread pool owns its worker threads, and if those threads need to be interrupted, the thread pool should take care of it.   As with any other encapsulated object, thread ownership is not transitive: the application may own the service and the service may own the worker threads, but the application doesn’t own the worker threads and therefore should not attempt to stop them directly. Instead, the service should provide lifecycle methods for shutting itself down that also shut down the owned threads; then the application can shut down the service, and the service can shut down the threads. Executor- Service provides the shutdown and shutdownNow methods; other thread-owning services should provide a similar shutdown mechanism.   Log service implemented by blocking queue     If you are logging multiple lines as part of a single log message, you may need to use additional client-side locking to prevent undesirable interleaving of output from multiple threads. If two threads logged multiline stack traces to the same stream with one println call per line, the results would be interleaved unpredictably, and could easily look like one large but meaningless stack trace.   public class LogWriter { private final BlockingQueue&lt;String&gt; queue; private final LoggerThread logger; public LogWriter(Writer writer) { this.queue = new LinkedBlockingQueue&lt;String&gt;(CAPACITY); this.logger = new LoggerThread(writer); } public void start() { logger.start(); } public void log(String msg) throws InterruptedException { queue.put(msg); } private class LoggerThread extends Thread { private final PrintWriter writer; ... public void run() {   try {     while (true) writer.println(queue.take()); } catch(InterruptedException ignored) { } finally {     writer.close(); } } } }   Stop logging      However, this approach has race conditions that make it unreliable. The implementation of log is a check-then-act sequence: producers could observe that the service has not yet been shut down but still queue messages after the shutdown, again with the risk that the producer might get blocked in log and never become unblocked. There are tricks that reduce the likelihood of this (like having the consumer wait several seconds before declaring the queue drained), but these do not change the fundamental problem, merely the likelihood that it will cause a failure.   public void log(String msg) throws InterruptedException {      if (!shutdownRequested)         queue.put(msg);     else   throw new IllegalStateException(\"logger is shut down\"); }     The way to provide reliable shutdown for LogWriter is to fix the race con- dition, which means making the submission of a new log message atomic. But we don’t want to hold a lock while trying to enqueue the message, since put could block. Instead, we can atomically check for shutdown and conditionally increment a counter to “reserve” the right to submit a message, as shown in Log- Service in Listing 7.15.   Delegate shutdown to high level service   public class LogService { private final ExecutorService exec = newSingleThreadExecutor(); ... public void start() { } public void stop() throws InterruptedException { try { exec.shutdown(); exec.awaitTermination(TIMEOUT, UNIT);         } finally {             writer.close(); } } public void log(String msg) { try {  } } exec.execute(new WriteTask(msg)); } catch (RejectedExecutionException ignored) { }       It can even delegate to one shot Executor, OneShotExecutionService.java ```java import java.util.Set; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.TimeUnit; import java.util.concurrent.atomic.AtomicBoolean;   /**     Created by todzhang on 2017/1/30.   If a method needs to process a batch of tasks and does not return   until all the tasks are finished, it can simplify service lifecycle management   by using a private Executor whose lifetime is bounded by that method.  *  *   The checkMail method in Listing checks for new mail in parallel   on a number of hosts. It creates a private executor and submits   a task for each host: it then shuts down the executor and waits        for termination, which occurs when all  */ public class OneShotExecutionService {       boolean checkMail(Set hosts, long timeout, TimeUnit unit) throws InterruptedException{      ExecutorService exec= Executors.newCachedThreadPool();      final AtomicBoolean hasNewMail=new AtomicBoolean(false);      try {          for (final String host : hosts                  ) {              exec.execute(new Runnable() {                  @Override                  public void run() {                      if (checkMail(host)) {                          hasNewMail.set(true);                      }                  }              });          }      }      finally{          exec.shutdown();          exec.awaitTermination(timeout,unit);      }      return hasNewMail.get();  }       boolean checkMail(String host){      return true;  } }        - When an ExecutorService is shut down abruptly with shutdownNow, it attempts to cancel the tasks currently in progress and returns a list of tasks that were sub- mitted but never started so that they can be logged or saved for later processing. Detailed logic can be found at [CancelledTaskTrackingExecutor.java](https://github.com/CloudsDocker/algo/blob/master/algoWS/src/main/java/com/todzhang/CancelledTaskTrackingExecutor.java) - The leading cause of premature thread death is RuntimeException.  # JVM shutdown - The JVM can shut down in either an _orderly_ or _abrupt_ manner. An orderly shut- down is initiated when the last “normal” (nondaemon) thread terminates, some- one calls System.exit, or by other platform-specific means (such as sending a SIGINT or hitting Ctrl-C). While this is the standard and preferred way for the JVM to shut down, it can also be shut down abruptly by calling **Runtime.halt or by killing the JVM process** through the operating system (such as sending a SIGKILL).  ## Shutdown hooks - In an orderly shutdown, the JVM first starts all registered shutdown hooks. Shutdown hooks are unstarted threads that are registered with **Runtime.addShutdownHook**. The JVM makes no guarantees on the order in which shutdown hooks are started. If any application threads (daemon or nondaemon) are still running at shutdown time, they continue to run concurrently with the shutdown process.  - When all shutdown hooks have completed, the JVM may choose **to run finalizers if runFinalizersOnExit is true**,  - and then halts.  - The JVM makes no attempt to stop or interrupt any application threads that are still running at shutdown time; they are abruptly terminated when the JVM eventually halts. If the shutdown hooks or finalizers don’t complete, then the orderly shutdown process “hangs” and the JVM must be shut down abruptly. In an abrupt shutdown, the JVM is not required to do anything other than halt the JVM; shutdown hooks will not run. - Shutdown **hooks should be thread-safe**: they must **use synchronization when accessing shared data** and should be careful to avoid deadlock, just like any other concurrent code. Further, they should not make assumptions about the state of the application (such as whether other services have shut down already or all normal threads have completed) or about why the JVM is shutting down, and **must therefore be coded extremely defensively**.  - Finally, they **should exit as quickly as possible**, since their existence delays JVM termination at a time when the user may be expecting the JVM to terminate quickly. - Shutdown hooks can be used for service or **application cleanu**p, such as deleting temporary files or cleaning up resources that are not automatically cleaned up by the OS. Listing 7.26 shows how LogService in Listing 7.16 could register a shutdown hook from its start method to ensure the log file is closed on exit. - Because shutdown hooks all run concurrently, closing the log file could cause trouble for other shutdown hooks who want to use the logger. To avoid this problem, shutdown hooks should not rely on services that can be shut down by the application or other shutdown hooks. **One way to accomplish this is to use a single shutdown hook for all services**, rather than one for each service, and have it call a series of shutdown actions. This ensures that shutdown actions execute sequentially in a single thread, thus avoiding the possibility of race conditions or deadlock between shutdown actions. This technique can be used whether or not you use shutdown hooks; **executing shutdown actions sequentially rather than concurrently** eliminates many potential sources of failure.  ```java public void start() {    Runtime.getRuntime().addShutdownHook(new Thread() {         public void run() {           try { LogService.this.stop(); }           catch (InterruptedException ignored) {} } }); }   Daemon thread     Threads are divided into two types: normal threads and daemon threads. When the JVM starts up, all the threads it creates (such as garbage collector and other housekeeping threads) are daemon threads, except the main thread. When a new thread is created, it inherits the daemon status of the thread that created it, so by default any threads created by the main thread are also normal threads.   Normal threads and daemon threads differ only in what happens when they exit. When a thread exits, the JVM performs an inventory of running threads, and if the only threads that are left are daemon threads, it initiates an orderly shutdown. When the JVM halts, any remaining daemon threads are abandoned— finally blocks are not executed, stacks are not unwound—the JVM just exits.   Daemon threads should be used sparingly—few processing activities can be safely abandoned at any time with no cleanup. In particular, it is dangerous to use daemon threads for tasks that might perform any sort of I/O. Daemon threads are best saved for “housekeeping” tasks, such as a background thread that periodically removes expired entries from an in-memory cache. Daemon threads are not a good substitute for properly managing the life- cycle of services within an application.   Finalizer     Finalizers offer no guarantees on when or even if they run, and they impose a significant performance cost on objects with nontrivial finalizers. They are also extremely difficult to write correctly.9 In most cases, the combination of finally blocks and explicit close methods does a better job of resource management than finalizers; the sole exception is when you need to manage objects that hold resources acquired by native methods.   Java does not provide a preemptive mechanism for cancelling activities or terminating threads. Instead, it provides a cooperative interruption mechanism that can be used to facilitate cancellation, but it is up to you to construct protocols for cancellation and use them consistently. Using FutureTask and the Executor framework simplifies building cancellable tasks and services.   Thread Pool     Thread pools work best when tasks are homogeneous and independent. Mix- ing long-running and short-running tasks risks “clogging” the pool unless it is very large; submitting tasks that depend on other tasks risks deadlock unless the pool is unbounded. Fortunately, requests in typical network-based server applications—web servers, mail servers, file servers—usually meet these guide- lines.   Some tasks have characteristics that require or preclude a specific exe- cution policy. Tasks that depend on other tasks require that the thread pool be large enough that tasks are never queued or rejected; tasks that exploit thread confinement require sequential execution. Document these requirements so that future maintainers do not undermine safety or live- ness by substituting an incompatible execution policy.   In a single-threaded executor, a task that submits another task to the same executor and waits for its result will always deadlock.   The same thing can happen in larger thread pools if all threads are executing tasks that are blocked waiting for other tasks still on the work queue. This is called thread starvation deadlock, and can occur whenever a pool task initiates an unbounded blocking wait for some resource or condition that can succeed only through the action of another pool task, such as waiting for the return value or side effect of another task, unless you can guarantee that the pool is large enough.      Whenever you submit to an Executor tasks that are not independent, be aware of the possibility of thread starvation deadlock, and document any pool sizing or configuration constraints in the code or configuration file where the Executor is configured.       Task that deadlocks in a single-threaded Executor. Don’t do this.     Future&lt;String&gt; header,footer;           header=exec.submit(new LoadFileTask(\"header.html\"));           footer=exec.submit(new LoadFileTask(\"footer.html\"));           String body=renderBody();           return header.get()+body+footer.get();           Long running tasks     Thread pools can have responsiveness problems if tasks can block for extended periods of time, even if deadlock is not a possibility. A thread pool can become clogged with long-running tasks, increasing the service time even for short tasks. If the pool size is too small relative to the expected steady-state number of long- running tasks, eventually all the pool threads will be running long-running tasks and responsiveness will suffer.   One technique that can mitigate the ill effects of long-running tasks is for tasks to use timed resource waits instead of unbounded waits. Most blocking methods in the plaform libraries come in both untimed and timed versions, such as Thread.join, BlockingQueue.put, CountDownLatch.await, and Selector.sel- ect. If the wait times out, you can mark the task as failed and abort it or requeue it for execution later. This guarantees that each task eventually makes progress towards either successful or failed completion, freeing up threads for tasks that might complete more quickly. If a thread pool is frequently full of blocked tasks, this may also be a sign that the pool is too small.   size the thread pool     The ideal size for a thread pool depends on the types of tasks that will be submitted and the characteristics of the deployment system. Thread pool sizes should rarely be hard-coded; instead pool sizes should be provided by a configuration mechanism or computed dynamically by consulting Runtime.availableProcessors.   If you have different categories of tasks with very different behaviors, consider using multiple thread pools so each can be tuned according to its workload.   The optimal pool size for keeping the processors at the desired utilization is: Nthreads=Ncpu∗Ucpu∗ (1+((W/C) Ncpu: Number of CPU Ucpu: target CPU utilization , 0&lt;Ucpu&lt;1 W/C: ratio of wait time to compute time   ThreadPoolExecutor     ThreadPoolExecutor provides the base implementation for the executors re- turned by the newCachedThreadPool, newFixedThreadPool, and newScheduled- ThreadExecutor factories in Executors.   Implementation of ThreadPoolExecutor     public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue,ThreadFactory threadFactory,RejectedExecutionHandler handler){...}                 corePoolSize is the target size, the implementation attempts to maintain the pool at this size when there are no tasks to execute. and will not create more threads than this unless the work queue is full. When a ThreadPoolExecutor is initially created, the core threads are not started immediately, but instead as tasks are submitted. Unless you call prestartAllCoreThreads       The maximum pool size is the upper bound on how many threads can be active at once.       A thread that has been idel for longer than the keep-alive time becomes a candidate for reaping and can be terminated if the current pool size exceed the core size.           By tuning the core pool size and keep-alive times, you can encourage the pool to reclaim resources used by otherwise idle threads, making them available for more useful work. (Like everything else, this is a tradeoff: reaping idle threads incurs additional latency due to thread creation if threads must later be created when demand increases.)   The newFixedThreadPool factory sets both the core pool size and the maxi- mum pool size to the requested pool size, creating the effect of infinite timeout;   the newCachedThreadPool factory sets the maximum pool size to Integer.MAX_VALUE and the core pool size to zero with a timeout of one minute, creating the effect of an infinitely expandable thread pool that will contract again when demand decreases.   Other combinations are possible using the explicit ThreadPool- Executor constructor.   ThreadPoolExecutor allows you to supply a BlockingQueue to hold tasks awaiting execution. There are three basic approaches to task queueing: un- bounded queue, bounded queue, and synchronous handoff. The choice of queue interacts with other configuration parameters such as pool size.   The default for newFixedThreadPool and newSingleThreadExecutor is to use an unbounded LinkedBlockingQueue. Tasks will queue up if all worker threads are busy, but the queue could grow without bound if the tasks keep arriving faster than they can be executed.   A more stable resource management strategy is to use a bounded queue, such as an ArrayBlockingQueue or a bounded LinkedBlockingQueue or Priority- BlockingQueue. Bounded queues help prevent resource exhaustion but introduce the question of what to do with new tasks when the queue is full. (There are a number of possible saturation policies for addressing this problem;   For very large or unbounded pools, you can also bypass queueing entirely and instead hand off tasks directly from producers to worker threads using a SynchronousQueue. A SynchronousQueue is not really a queue at all, but a mechanism for managing handoffs between threads. In order to put an element on a SynchronousQueue, another thread must already be waiting to accept the handoff. If no thread is waiting but the current pool size is less than the maximum, ThreadPoolExecutor creates a new thread; otherwise the task is rejected according to the saturation policy. Using a direct handoff is more efficient because the task can be handed right to the thread that will execute it, rather than first placing it on a queue and then having the worker thread fetch it from the queue. Synchron- ousQueue is a practical choice only if the pool is unbounded or if rejecting excess tasks is acceptable. The newCachedThreadPool factory uses a SynchronousQueue.   Using a FIFO queue like LinkedBlockingQueue or ArrayBlockingQueue causes tasks to be started in the order in which they arrived. For more con- trol over task execution order, you can use a PriorityBlockingQueue, which orders tasks according to priority. Priority can be defined by natural order (if tasks implement Comparable) or by a Comparator.   The newCachedThreadPool factory is a good default choice for an Executor, providing better queuing performance than a fixed thread pool.5 A fixed size thread pool is a good choice when you need to limit the number of concurrent tasks for resource-management purposes, as in a server application that accepts requests from network clients and would otherwise be vulnerable to overload.   ith tasks that depend on other tasks, bounded thread pools or queues can cause thread starvation deadlock; instead, use an unbounded pool configuration like newCachedThreadPool.   Saturation policies     When a bounded work queue fills up, the saturation policy comes into play. The saturation policy for a ThreadPoolExecutor can be modified by calling setRejectedExecutionHandler.   Several implementations of RejectedExecutionHandler are provided, each implementing a different saturation policy: AbortPolicy, CallerRunsPolicy, DiscardPolicy, and DiscardOldestPolicy.   The default policy, abort, causes execute to throw the unchecked Rejected- ExecutionException; the caller can catch this exception and implement its own overflow handling as it sees fit. The discard policy silently discards the newly submitted task if it cannot be queued for execution; the discard-oldest policy discards the task that would otherwise be executed next and tries to resubmit the new task. (If the work queue is a priority queue, this discards the highest-priority element, so the combination of a discard-oldest saturation policy and a priority queue is not a good one.)   The caller-runs policy implements a form of throttling that neither discards tasks nor throws an exception, but instead tries to slow down the flow of new tasks by pushing some of the work back to the caller. It executes the newly submitted task not in a pool thread, but in the thread that calls execute. If we modified our WebServer example to use a bounded queue and the caller-runs policy, after all the pool threads were occupied and the work queue filled up the next task would be executed in the main thread during the call to execute.   Thread Factory     Whenever a thread pool needs to create a thread, it does so through a thread factory (see Listing 8.5). The default thread factory creates a new, nondaemon thread with no special configuration. Specifying a thread factory allows you to customize the configuration of pool threads. ThreadFactory has a single method, newThread, that is called whenever a thread pool needs to create a new thread.   There are a number of reasons to use a custom thread factory. You might want to specify an UncaughtExceptionHandler for pool threads, or instantiate an instance of a custom Thread class, such as one that performs debug logging.     public interface ThreadFactory{   Thread newThread(Runnable r); }           BoundedExecutor.java is using semaphore and Executor for bounded executor service.   MyThreadFactory.java and MyAppThread.java are used to customize ThreadFactory, a customized Thread.   MyExtendedThreadPool.java implemented beforeExecute, afterExecute, etc method to add statistics, such as log and timing for each operations in the thread pool   Process sequential processing to parallel  void processSequentially(List&lt;Element&gt; elements) { for (Element e : elements) process(e); } void processInParallel(Executor exec, List&lt;Element&gt; elements) { for (final Element e : elements) exec.execute(new Runnable() { public void run() { process(e); } }); }     If you want to submit a set of tasks and wait for them all to complete, you can use ExecutorService.invokeAll; to retrieve the results as they become available, you can use a CompletionService.   Deadlocks     There is often a tension between safety and liveness. We use locking to ensure thread safety, but indiscriminate use of locking can cause lock-ordering deadlocks. Similarly, we use thread pools and semaphores to bound resource consumption, but failure to understand the activities being bounded can cause resource deadlocks. Java applications do not recover from deadlock, so it is worthwhile to ensure that your design precludes the conditions that could cause it.   When a thread holds a lock forever, other threads attempting to acquire that lock will block forever waiting. When thread A holds lock L and tries to acquire lock M, but at the same time thread B holds M and tries to acquire L, both threads will wait forever. This situation is the simplest case of deadlock (or deadly embrace),   Database systems are designed to detect and recover from deadlock. A trans- action may acquire many locks, and locks are held until the transaction commits. So it is quite possible, and in fact not uncommon, for two transactions to deadlock. Without intervention, they would wait forever (holding locks that are probably re- quired by other transactions as well). But the database server is not going to let this happen. When it detects that a set of transactions is deadlocked (which it does by searching the is-waiting-for graph for cycles), it picks a victim and aborts that transaction. This releases the locks held by the victim, allowing the other transactions to proceed. The application can then retry the aborted transaction, which may be able to complete now that any competing transactions have com- pleted.   A program will be free of lock-ordering deadlocks if all threads acquire the locks they need in a fixed global order.   To break deadlock by ensuring lock order     uses System.identityHashCode to induce a lock ordering. It involves a few extra lines of code, but eliminates the possibility of deadlock.      public static native int identityHashCode(Object x);           In the rare case that two objects have the same hash code, we must use an arbitrary means of ordering the lock acquisitions, and this reintroduces the pos- sibility of deadlock. To prevent inconsistent lock ordering in this case, a third “tie breaking” lock is used. By acquiring the tie-breaking lock before acquiring either Account lock, we ensure that only one thread at a time performs the risky task of acquiring two locks in an arbitrary order, eliminating the possibility of deadlock (so long as this mechanism is used consistently). If hash collisions were common, this technique might become a concurrency bottleneck (just as having a single, program-wide lock would), but because hash collisions with System.identity- HashCode are vanishingly infrequent, this technique provides that last bit of safety at little cost.   two locks are acquired by two threads in different orders, risking deadlock.   Calling a method with no locks held is called an open call [CPJ 2.4.1.3], and classes that rely on open calls are more well-behaved and composable than classes that make calls with locks held. Using open calls to avoid deadlock is analogous to using encapsulation to provide thread safety: while one can certainly construct a thread-safe program without any encapsulation, the thread safety analysis of a program that makes effective use of encapsulation is far easier than that of one that does not.   Avoiding and diagnosing deadlocks     A program that never acquires more than one lock at a time cannot experience lock-ordering deadlock. Of course, this is not always practical, but if you can get away with it, it’s a lot less work. If you must acquire multiple locks, lock ordering must be a part of your design: try to minimize the number of potential locking interactions, and follow and document a lock-ordering protocol for locks that may be acquired together.   In programs that use fine-grained locking, audit your code for deadlock free- dom using a two-part strategy: first, identify where multiple locks could be ac- quired (try to make this a small set), and then perform a global analysis of all such instances to ensure that lock ordering is consistent across your entire pro- gram. Using open calls wherever possible simplifies this analysis substantially. With no non-open calls, finding instances where multiple locks are acquired is fairly easy, either by code review or by automated bytecode or source code anal- ysis.   Timed lock attempts     Another technique for detecting and recovering from deadlocks is to use the timed tryLock feature of the explicit Lock classes (see Chapter 13) instead of intrinsic locking. Where intrinsic locks wait forever if they cannot acquire the lock, explicit locks let you specify a timeout after which tryLock returns failure.   JVM Thread dump including dead lock     There are two threads trying to accquire two locks in different orders   ```bash Java stack information for the threads listed above: “ApplicationServerThread “: at MumbleDBConnection.remove_statement   waiting to lock &lt;0x650f7f30&gt; (a MumbleDBConnection) at MumbleDBStatement.close   locked &lt;0x6024ffb0&gt; (a MumbleDBCallableStatement) … “ApplicationServerThread “: at MumbleDBCallableStatement.sendBatch   waiting to lock &lt;0x6024ffb0&gt; (a MumbleDBCallableStatement) at MumbleDBConnection.commit   locked &lt;0x650f7f30&gt; (a MumbleDBConnection) ```   Other liveness hazards     While deadlock is the most widely encountered liveness hazard, there are sev- eral other liveness hazards you may encounter in concurrent programs including starvation, missed signals, and livelock.   Reference     http://www.javamex.com/tutorials/threads/thread_scheduling.shtml   http://www.javamex.com/tutorials/threads/priority.shtml   http://www.javamex.com/tutorials/threads/how_threads_work.shtml   http://www.javamex.com/tutorials/threads/thread_scheduling_2.shtml   http://www.javamex.com/tutorials/threads/yield.shtml   http://javarevisited.blogspot.in/2012/07/countdownlatch-example-in-java.html   http://javarevisited.blogspot.sg/2012/05/how-to-use-threadlocal-in-java-benefits.html   http://javarevisited.blogspot.com/2012/03/simpledateformat-in-java-is-not-thread.html   http://javarevisited.blogspot.com/2012/05/counting-semaphore-example-in-java-5.html#ixzz4WRuTQFDF   http://javarevisited.blogspot.in/2012/02/what-is-race-condition-in.html   http://javarevisited.blogspot.in/2011/05/wait-notify-and-notifyall-in-java.html   http://javarevisited.blogspot.in/2015/07/how-to-use-wait-notify-and-notifyall-in.html   http://javarevisited.blogspot.in/2012/07/cyclicbarrier-example-java-5-concurrency-tutorial.html   http://javarevisited.blogspot.in/2011/09/fork-join-task-java7-tutorial.html   https://en.wikipedia.org/wiki/Dining_philosophers_problem   http://javarevisited.blogspot.com/2011/11/collection-interview-questions-answers.html#ixzz4WTN72QPa   http://javarevisited.blogspot.in/2011/11/collection-interview-questions-answers.html   http://www.oracle.com/technetwork/articles/java/fork-join-422606.html   http://tutorials.jenkov.com/java-util-concurrent/java-fork-and-join-forkjoinpool.html   http://coopsoft.com/ar/CalamityArticle.html   http://www.infoq.com/cn/articles/fork-join-introduction  ","categories": [],
        "tags": ["Java","Concurrent"],
        "url": "/2020/03/10/Concurrent-In-Java.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Java Concurrent Column 2",
        "excerpt":"This is the second half about Java Concurrent of my blog   non-blocking synchronization     Much of the recent research on concurrent algorithms has focused on nonblock- ing algorithms, which use low-level atomic machine instructions such as compare- and-swap instead of locks to ensure data integrity under concurrent access. Non- blocking algorithms are used extensively in operating systems and JVMs for thread and process scheduling, garbage collection, and to implement locks and other concurrent data structures.   Nonblocking algorithms are considerably more complicated to design and im- plement than lock-based alternatives, but they can offer significant scalability and liveness advantages. They coordinate at a finer level of granularity and can greatly reduce scheduling overhead because they don’t block when multiple threads contend for the same data. Further, they are immune to deadlock and other liveness problems. In lock-based algorithms, other threads cannot make progress if a thread goes to sleep or spins while holding a lock, whereas nonblocking algorithms are impervious to individual thread failures. As of Java 5.0, it is possible to build efficient nonblocking algorithms in Java using the atomic variable classes such as AtomicInteger and AtomicReference.   Atomic variables can also be used as “better volatile variables” even if you are not developing nonblocking algorithms. Atomic variables offer the same memory semantics as volatile variables, but with additional support for atomic updates— making them ideal for counters, sequence generators, and statistics gathering while offering better scalability than lock-based alternatives.   Coordinating access to shared state using a consistent locking protocol ensures that whichever thread holds the lock guarding a set of variables has exclusive access to those variables, and that any changes made to those variables are visible to other threads that subsequently acquire the lock.   Volatile variables are a lighter-weight synchronization mechanism than locking because they do not involve context switches or thread scheduling. However, volatile variables have some limitations compared to locking: while they provide similar visibility guarantees, they cannot be used to construct atomic compound actions. This means that volatile variables cannot be used when one variable de- pends on another, or when the new value of a variable depends on its old value. This limits when volatile variables are appropriate, since they cannot be used to reliably implement common tools such as counters or mutexes.   This can be a serious problem if the blocked thread is a high-priority thread but the thread holding the lock is a lower-priority thread—a performance hazard known as priority inversion. Even though the higher-priority thread should have precedence, it must wait until the lock is released, and this effectively down- grades its priority to that of the lower-priority thread. If a thread holding a lock is permanently blocked (due to an infinite loop, deadlock, livelock, or other liveness failure), any threads waiting for that lock can never make progress.   hardware     Exclusive locking is a pessimistic technique—it assumes the worst (if you don’t lock your door, gremlins will come in and rearrange your stuff) and doesn’t proceed until you can guarantee, by acquiring the appropriate locks, that other threads will not interfere.   For fine-grained operations, there is an alternate approach that is often more efficient—the optimistic approach, whereby you proceed with an update, hopeful that you can complete it without interference. This approach relies on collision detection to determine if there has been interference from other parties during the update, in which case the operation fails and can be retried (or not). The optimistic approach is like the old saying, “It is easier to obtain forgiveness than permission”, where “easier” here means “more efficient”.   Processors designed for multiprocessor operation provide special instructions for managing concurrent access to shared variables. Early processors had atomic test-and-set, fetch-and-increment, or swap instructions sufficient for implementing mutexes that could in turn be used to implement more sophisticated concurrent objects. Today, nearly every modern processor has some form of atomic read- modify-write instruction, such as compare-and-swap or load-linked/store-conditional. Operating systems and JVMs use these instructions to implement locks and con- current data structures, but until Java 5.0 they had not been available directly to Java classes.   Compare and swap     The approach taken by most processor architectures, including IA32 and Sparc, is to implement a compare-and-swap (CAS) instruction. (Other processors, such as PowerPC, implement the same functionality with a pair of instructions: load- linked and store-conditional.) CAS has three operands—a memory location V on which to operate, the expected old value A, and the new value B. CAS atomically updates V to the new value B, but only if the value in V matches the expected old value A; otherwise it does nothing. In either case, it returns the value currently in V. (The variant called compare-and-set instead returns whether the operation succeeded.) CAS means “I think V should have the value A; if it does, put B there, otherwise don’t change it but tell me I was wrong.” CAS is an optimistic technique—it proceeds with the update in the hope of success, and can detect failure if another thread has updated the variable since it was last examined. SimulatedCAS in Listing 15.1 illustrates the semantics (but not the implementation or performance) of CAS.   When multiple threads attempt to update the same variable simultaneously using CAS, one wins and updates the variable’s value, and the rest lose. But the losers are not punished by suspension, as they could be if they failed to acquire a lock; instead, they are told that they didn’t win the race this time but can try again. Because a thread that loses a CAS is not blocked, it can decide whether it wants to try again, take some other recovery action, or do nothing.3 This flexibility eliminates many of the liveness hazards associated with locking (though in unusual cases can introduce the risk of livelock—see Section 10.3.3).   CAS addresses the problem of implementing atomic read-modify-write sequences without locking, because it can detect interference from other threads.   counter implemented by CAS     At first glance, the CAS-based counter looks as if it should perform worse than a lock-based counter; it has more operations and a more complicated control flow, and depends on the seemingly complicated CAS operation. But in reality, CAS-based counters significantly outperform lock-based counters if there is even a small amount of contention, and often even if there is no contention. The fast path for uncontended lock acquisition typically requires at least one CAS plus other lock-related housekeeping, so more work is going on in the best case for a lock-based counter than in the normal case for the CAS-based counter. Since the CAS succeeds most of the time (assuming low to moderate contention), the hardware will correctly predict the branch implicit in the while loop, minimizing the overhead of the more complicated control logic.   The language syntax for locking may be compact, but the work done by the JVM and OS to manage locks is not. Locking entails traversing a relatively com- plicated code path in the JVM and may entail OS-level locking, thread suspension, and context switches. In the best case, locking requires at least one CAS, so using locks moves the CAS out of sight but doesn’t save any actual execution cost. On the other hand, executing a CAS from within the program involves no JVM code, system calls, or scheduling activity. What looks like a longer code path at the ap- plication level is in fact a much shorter code path when JVM and OS activity are taken into account. The primary disadvantage of CAS is that it forces the caller to deal with contention (by retrying, backing off, or giving up), whereas locks deal with contention automatically by blocking until the lock is available.   Competitive forces will likely result in continued CAS performance improvement over the next sev- eral years. A good rule of thumb is that the cost of the “fast path” for uncontended lock acquisition and release on most processors is approximately twice the cost of a CAS.   CAS support in JVM     So, how does Java code convince the processor to execute a CAS on its behalf? Prior to Java 5.0, there was no way to do this short of writing native code. In Java 5.0, low-level support was added to expose CAS operations on int, long, and object references, and the JVM compiles these into the most efficient means provided by the underlying hardware. On platforms supporting CAS, the run- time inlines them into the appropriate machine instruction(s); in the worst case, if a CAS-like instruction is not available the JVM uses a spin lock. This low-level JVM support is used by the atomic variable classes (AtomicXxx in java.util.con- current.atomic) to provide an efficient CAS operation on numeric and reference types; these atomic variable classes are used, directly or indirectly, to implement most of the classes in java.util.concurrent.   Other liveness hazards     While deadlock is the most widely encountered liveness hazard, there are sev- eral other liveness hazards you may encounter in concurrent programs including starvation, missed signals, and livelock.   Starvation     Starvation occurs when a thread is perpetually denied access to resources it needs in order to make progress; the most commonly starved resource is CPU cycles. Starvation in Java applications can be caused by inappropriate use of thread prior- ities. It can also be caused by executing nonterminating constructs (infinite loops or resource waits that do not terminate) with a lock held, since other threads that need that lock will never be able to acquire it.   The thread priorities defined in the Thread API are merely scheduling hints. The Thread API defines ten priority levels that the JVM can map to operating system scheduling priorities as it sees fit. This mapping is platform-specific, so two Java priorities can map to the same OS priority on one system and different OS priorities on another.   Avoid the temptation to use thread priorities, since they increase platform dependence and can cause liveness problems. Most concurrent applica- tions can use the default priority for all threads.   Poor responsiveness     One step removed from starvation is poor responsiveness, which is not uncom- mon in GUI applications using background threads.   If the work done by other threads are truly background tasks, lowering their priority can make the foreground tasks more responsive.   Livelock     Livelock is a form of liveness failure in which a thread, while not blocked, still cannot make progress because it keeps retrying an operation that will always fail.   Livelock often occurs in transactional messaging applications, where the messaging infrastructure rolls back a transaction if a message cannot be processed successfully, and puts it back at the head of the queue. If a bug in the message handler for a particular type of message causes it to fail, every time the message is dequeued and passed to the buggy handler, the transaction is rolled back. Since the message is now back at the head of the queue, the handler is called over and over with the same result. (This is sometimes called the poison message problem.) The message handling thread is not blocked, but it will never make progress either. This form of livelock often comes from overeager error-recovery code that mistakenly treats an unrecoverable error as a recoverable one.   This is similar to what happens when two overly polite people are walking in opposite directions in a hallway: each steps out of the other’s way, and now they are again in each other’s way. So they both step aside again, and again, and again. . .   Solutions     The solution for this variety of livelock is to introduce some randomness into the retry mechanism. For example, when two stations in an ethernet network try to send a packet on the shared carrier at the same time, the packets collide. The stations detect the collision, and each tries to send their packet again later. If they each retry exactly one second later, they collide over and over, and neither packet ever goes out, even if there is plenty of available bandwidth. To avoid this, we make each wait an amount of time that includes a random component. (The ethernet protocol also includes exponential backoff after repeated collisions, reducing both congestion and the risk of repeated failure with multiple colliding stations.) Retrying with random waits and backoffs can be equally effective for avoiding livelock in concurrent applications.   Summary     Liveness failures are a serious problem because there is no way to recover from them short of aborting the application. The most common form of liveness failure is lock-ordering deadlock. Avoiding lock ordering deadlock starts at design time: ensure that when threads acquire multiple locks, they do so in a consistent order. The best way to do this is by using open calls throughout your program. This greatly reduces the number of places where multiple locks are held at once, and makes it more obvious where those places are.     Reference       Performance     One of the primary reasons to use threads is to improve performance.   First make your program right, then make it fast—and then only if your performance requirements and measurements tell you it needs to be faster. In designing a con- current application, squeezing out the last bit of performance is often the least of your concerns.   When the performance of an activity is limited by availability of a par- ticular resource, we say it is bound by that resource: CPU-bound, database-bound, etc.   using multiple threads always introduces some performance costs compared to the single-threaded approach. These include the overhead associated with coordinating between threads (locking, signaling, and memory synchronization), increased context switching,thread creation and teardown, and scheduling overhead. When threading is employed effectively, these costs are more than made up for by greater throughput, responsiveness, or capacity. On the other hand, a poorly designed concurrent application can perform even worse than a comparable sequential one.   we want to keep the CPUs busy with useful work   Scalability     Scalability describes the ability to improve throughput or capacity when additional computing resources (such as additional CPUs, memory, stor- age, or I/O bandwidth) are added.   Nearly all engineering decisions involve some form of tradeoff.   This is one of the reasons why most optimizations are premature: they are often undertaken before a clear set of requirements is available.   Avoid premature optimization. First make it right, then make it fast—if it is not already fast enough.   Measure, don’t guess.   Amdahl’s law     the theoretical speedup is always limited by the part of the task that cannot benefit from the improvement.   If F is the fraction of the calculation that must be executed serially, then Amdahl’s law says that on a machine with N processors, we can achieve a speedup of at most: Speedup ≤ 1 / (F + (1 − F)/N)   As N approaches infinity, the maximum speedup converges to 1/F, meaning that a program in which fifty percent of the processing must be executed serially can be sped up only by a factor of two, regardless of how many processors are available, and a program in which ten percent must be executed serially can be sped up by at most a factor of ten.   Amdahl’s law also quantifies the efficiency cost of serialization. With ten processors, a program with 10% serialization can achieve at most a speedup of 5.3 (at 53% utilization), and with 100 processors it can achieve at most a speedup of 9.2 (at 9% utilization). It takes a lot of inefficiently utilized CPUs to never get to that factor of ten.   It is clear that as processor counts increase, even a small percentage of serialized execution limits how much throughput can be increased with additional computing resources.   All concurrent applications have some sources of serialization; if you think yours does not, look again.   Amdahl’s law tells us that the scalability of an application is driven by the proportion of code that must be executed serially. Since the primary source of serialization in Java programs is the exclusive resource lock, scalability can often be improved by spending less time holding locks, either by reducing lock granu- larity, reducing the duration for which locks are held, or replacing exclusive locks with nonexclusive or nonblocking alternatives.   Costs introduced by threads  Context switching     Context switches are not free; thread scheduling requires manipulating shared data structures in the OS and JVM. The OS and JVM use the same CPUs your pro- gram does; more CPU time spent in JVM and OS code means less is available for your program.   When a new thread is switched in, the data it needs is unlikely to be in the local processor cache, so a context switch causes a flurry of cache misses, and thus threads run a little more slowly when they are first scheduled.   The actual cost of context switching varies across platforms, but a good rule of thumb is that a context switch costs the equivalent of 5,000 to 10,000 clock cycles, or several microseconds on most current processors.   memory synchronization     The performance cost of synchronization comes from several sources. The visibility guarantees provided by synchronized and volatile may entail using special instructions called memory barriers that can flush or invalidate caches, flush hard- ware write buffers, and stall execution pipelines. Memory barriers may also have indirect performance consequences because they inhibit other compiler optimizations; most operations cannot be reordered with memory barriers.   When assessing the performance impact of synchronization, it is important to distinguish between contended and uncontended synchronization. The synchronized mechanism is optimized for the uncontended case (volatile is always uncontended), and at this writing, the performance cost of a “fast-path” uncontended synchronization ranges from 20 to 250 clock cycles for most systems. While this is certainly not zero, the effect of needed, uncontended synchronization is rarely significant in overall application performance, and the alternative involves compromising safety and potentially signing yourself (or your succes- sor) up for some very painful bug hunting later.   Modern JVMs can reduce the cost of incidental synchronization by optimizing away locking that can be proven never to contend. If a lock object is accessible only to the current thread, the JVM is permitted to optimize away a lock acquisi- tion because there is no way another thread could synchronize on the same lock. For example, the lock acquisition in following Listing can always be eliminated by the JVM.   Following synchronization has no effect  synchronized (new Object()) {     // do something }      More sophisticated JVMs can use escape analysis to identify when a local object reference is never published to the heap and is therefore thread-local. As below sample:     public String getStoogeNames() { List&lt;String&gt; stooges = new Vector&lt;String&gt;(); stooges.add(\"Moe\"); stooges.add(\"Larry\"); stooges.add(\"Curly\"); return stooges.toString(); }           the only reference to the List is the local variable stooges, and stack-confined variables are automatically thread-local. A naive execution of getStoogeNames would acquire and release the lock on the Vector four times, once for each call to add or toString. However, a smart runtime compiler can inline these calls and then see that stooges and its internal state never escape, and therefore that all four lock acquisitions can be eliminated.   Even without escape analysis, compilers can also perform lock coarsening, the merging of adjacent synchronized blocks using the same lock. For getStooge- Names, a JVM that performs lock coarsening might combine the three calls to add and the call to toString into a single lock acquisition and release, using heuristics on the relative cost of synchronization versus the instructions inside the synch- ronized block.5 Not only does this reduce the synchronization overhead, but it also gives the optimizer a much larger block to work with, likely enabling other optimizations.      Don’t worry excessively about the cost of uncontended synchronization. The basic mechanism is already quite fast, and JVMs can perform addi- tional optimizations that further reduce or eliminate the cost. Instead, focus optimization efforts on areas where lock contention actually occurs.       Synchronization by one thread can also affect the performance of other threads. Synchronization creates traffic on the shared memory bus; this bus has a limited bandwidth and is shared across all processors. If threads must compete for synchronization bandwidth, all threads using synchronization will suffer.   Blocking     Uncontended synchronization can be handled entirely within the JVM (Bacon et al., 1998); contended synchronization may require OS activity, which adds to the cost. When locking is contended, the losing thread(s) must block. The JVM can implement blocking either via spin-waiting (repeatedly trying to acquire the lock until it succeeds) or by suspending the blocked thread through the operating system. Which is more efficient depends on the relationship between context switch overhead and the time until the lock becomes available; spin-waiting is preferable for short waits and suspension is preferable for long waits. Some JVMs choose between the two adaptively based on profiling data of past wait times, but most just suspend threads waiting for a lock.   Reducing lock contention     We’ve seen that serialization hurts scalability and that context switches hurt performance. Contended locking causes both, so reducing lock contention can improve both performance and scalability. Access to resources guarded by an exclusive lock is serialized—only one thread at a time may access it. Of course, we use locks for good reasons, such as preventing data corruption, but this safety comes at a price. Persistent contention for a lock limits scalability.   The principal threat to scalability in concurrent applications is the exclu- sive resource lock.   Two factors influence the likelihood of contention for a lock: how often that lock is requested and how long it is held once acquired.7 If the product of these factors is sufficiently small, then most attempts to acquire the lock will be uncon- tended, and lock contention will not pose a significant scalability impediment.   There are three ways to reduce lock contention:     Reduce the duration for which locks are held;   Reduce the frequency with which locks are requested; or   Replace exclusive locks with coordination mechanisms that permit greater concurrency.   Narrowing lock scope     An effective way to reduce the likelihood of contention is to hold locks as briefly as possible. This can be done by moving code that doesn’t require the lock out of synchronized blocks, especially for expensive operations and potentially block- ing operations such as I/O.   It is easy to see how holding a “hot” lock for too long can limit scalability   Reducing the scope of the lock in userLocationMatches substantially reduces the number of instructions that are executed with the lock held. By Amdahl’s law, this removes an impediment to scalability because the amount of serialized code is reduced.   Because AttributeStore has only one state variable, attributes, we can im- prove it further by the technique of delegating thread safety (Section 4.3). By replacing attributes with a thread-safe Map (a Hashtable, synchronizedMap, or Con- currentHashMap), AttributeStore can delegate all its thread safety obligations to the underlying thread-safe collection.   Reducing lock granularity     The other way to reduce the fraction of time that a lock is held (and therefore the likelihood that it will be contended) is to have threads ask for it less often. This can be accomplished by lock splitting and lock striping, which involve using separate locks to guard multiple independent state variables previously guarded by a single lock. These techniques reduce the granularity at which locking occurs, potentially allowing greater scalability—but using more locks also increases the risk of deadlock.   If a lock guards more than one independent state variable, you may be able to improve scalability by splitting it into multiple locks that each guard different variables. This results in each lock being requested less often.   After splitting the lock, each new finer-grained lock will see less locking traffic than the original coarser lock would have.   Lock stripping     Splitting a heavily contended lock into two is likely to result in two heavily contended locks.   Lock splitting can sometimes be extended to partition locking on a variable- sized set of independent objects, in which case it is called lock striping. For exam- ple, the implementation of ConcurrentHashMap uses an array of 16 locks, each of which guards 1/16 of the hash buckets; bucket N is guarded by lock N mod 16.   One of the downsides of lock striping is that locking the collection for ex- clusive access is more difficult and costly than with a single lock. Usually an operation can be performed by acquiring at most one lock, but occasionally you need to lock the entire collection, as when ConcurrentHashMap needs to expand the map and rehash the values into a larger set of buckets. This is typically done by acquiring all of the locks in the stripe set   common optimizations such as caching frequently computed values can introduce “hot fields” that limit scalability.   A common optimization is to update a separate counter as entries are added or removed; this slightly increases the cost of a put or remove operation to keep the counter up-to-date, but reduces the cost of the size operation from O(n) to O(1).   In this case, the counter is called a hot field because every mutative operation needs to access it.   ConcurrentHashMap avoids this problem by having size enumerate the stripes and add up the number of elements in each stripe, instead of maintaining a global count. To avoid enumerating every element, ConcurrentHashMap maintains a separate count field for each stripe, also guarded by the stripe lock.   Alternative to exclusive lock     A third technique for mitigating the effect of lock contention is to forego the use of exclusive locks in favor of a more concurrency-friendly means of managing shared state. These include using the concurrent collections, read-write locks, immutable objects and atomic variables.   ReadWriteLock     enforces a multiple-reader, single-writer locking discipline: more than one reader can access the shared resource concurrently so long as none of them wants to modify it, but writers must acquire the lock excusively. For read-mostly data structures, ReadWriteLock can offer greater concurrency than exclusive locking; for read-only data structures, immutability can eliminate the need for locking entirely.   Atomic variables (see Chapter 15) offer a means of reducing the cost of updat- ing “hot fields” such as statistics counters, sequence generators, or the reference   If size is called frequently compared to mutative operations, striped data structures can optimize for this by caching the collection size in a volatile whenever size is called and invalidating the cache (setting it to -1) whenever the collection is modified. If the cached value is nonnegative on entry to size, it is accurate and can be returned; otherwise it is recomputed.   The atomic variable classes pro- vide very fine-grained (and therefore more scalable) atomic operations on integers or object references, and are implemented using low-level concurrency primitives (such as compare-and-swap) provided by most modern processors. If your class has a small number of hot fields that do not participate in invariants with other variables, replacing them with atomic variables may improve scalability.   Comparing Map     The single-threaded performance of ConcurrentHashMap is slightly better than that of a synchronized HashMap, but it is in concurrent use that it really shines. The implementation of ConcurrentHashMap assumes the most common operation is retrieving a value that already exists, and is therefore optimized to provide highest performance and concurrency for successful get operations.   The major scalability impediment for the synchronized Map implementations is that there is a single lock for the entire map, so only one thread can access the map at a time. On the other hand, ConcurrentHashMap does no locking for most successful read operations, and uses lock striping for write operations and those few read operations that do require locking. As a result, multiple threads can access the Map concurrently without blocking.   The numbers for the synchronized collections are not as encouraging. Perfor- mance for the one-thread case is comparable to ConcurrentHashMap, but once the load transitions from mostly uncontended to mostly contended—which happens here at two threads—the synchronized collections suffer badly. This is common behavior for code whose scalability is limited by lock contention. So long as contention is low, time per operation is dominated by the time to actually do the work and throughput may improve as threads are added. Once contention becomes significant, time per operation is dominated by context switch and scheduling delays, and adding more threads has little effect on throughput.   Building a asynchronous log     Building a logger that moves the I/O to another thread may improve performance, but it also introduces a number of design complications, such as interruption (what happens if a thread blocked in a logging operation is interrupted?), service guarantees (does the logger guarantee that a success- fully queued log message will be logged prior to service shutdown?), saturation policy (what happens when the producers log messages faster than the logger thread can handle them?), and service lifecycle (how do we shut down the logger, and how do we communicate the service state to producers?).   Reducing context switching     The “get in, get out” principle of Section 11.4.1 tells us that we should hold locks as briefly as possible, because the longer a lock is held, the more likely that lock will be contended. If a thread blocks waiting for I/O while holding a lock, another thread is more likely to want the lock while the first thread is holding it. Concurrent systems perform much better when most lock acquisitions are uncontended, because contended lock acquisition means more context switches. A coding style that encourages more context switches thus yields lower overall throughput.   Testing concurrency     we defined safety as “nothing bad ever happens” and liveness as “something good eventually happens”.   when interrupted, it throws InterruptedException. This is one of the few cases in which it is appropriate to subclass Thread explicitly instead of using a Runnable in a pool: in order to test proper termination with join.   The result of Thread.getState should not be used for concurrency control, and is of limited usefulness for testing—its primary utility is as a source of debugging information.   a common error in implementing semaphore-controlled buffers is to forget that the code actually doing the insertion and extraction requires mutual exclu- sion (using synchronized or ReentrantLock). A sample run of PutTakeTest with a version of BoundedBuffer that omits making doInsert and doExtract synch- ronized fails fairly quickly.   Tests should be run on multiprocessor systems to increase the diversity of potential interleavings. However, having more than a few CPUs does not necessarily make tests more effective. To maximize the chance of detecting timing-sensitive data races, there should be more active threads than CPUs, so that at any given time some threads are running and some are switched out, thus reducing the predicatability of interactions between threads.   Tests like PutTakeTest tend to be good at finding safety violations. For exam- ple, a common error in implementing semaphore-controlled buffers is to forget that the code actually doing the insertion and extraction requires mutual exclu- sion (using synchronized or ReentrantLock). A sample run of PutTakeTest with a version of BoundedBuffer that omits making doInsert and doExtract synch- ronized fails fairly quickly. Running PutTakeTest with a few dozen threads iterating a few million times on buffers of various capacity on various systems increases our confidence about the lack of data corruption in put and take.   The source code PutTakeTest.java demonstreated aforesaid logic.   Test resource management     The tests so far have been concerned with a class’s adherence to its specifica- tion—that it does what it is supposed to do. A secondary aspect to test is that it does not do things it is not supposed to do, such as leak resources. Any object that holds or manages other objects should not continue to maintain references to those objects longer than necessary. Such storage leaks prevent garbage collectors from reclaiming memory (or threads, file handles, sockets, database connections, or other limited resources) and can lead to resource exhaustion and application failure.  ","categories": [],
        "tags": ["java","concurrent"],
        "url": "/2020/03/15/Concurrent-In-Java-col-2.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "JVM warm up by Escape Analysis",
        "excerpt":"Why JVM need warm up  I don’t know how and why you get to this blog. But I know the key words in your mind are “warm” for JVM. As the name “warm up” suggested, and like what you normally do before work out. Warm up means get your body/system ready and tune it to perfect state As for systems running under JVM, if they are time critical and latency sensitive. The best approach to make it run in good state is good preparation.   One feasible strategy for low-latency applications, we need to cache all classes beforehand – so that they’re available instantly when accessed at runtime.   This process of tuning the JVM is known as warming up.   There are multiple tools &amp; approach to implement warm up, while I’m going to dig into one of most important strategy to warm up your application in JVM. That’s EA: Escape Analysis.   Escape Analysis   Java objects are created and stored in the heap. The programming language does not offer the possibility to let the programmer decide if an object should be generated in the stack. But in certain cases it would be desirable to allocate an object on the stack, as the memory allocation on the stack is cheaper than the memory allocation in the heap, deallocation on the stack is free and the stack is efficiently managed by the runtime.   The JVM uses therefore internally escape analysis to check if an object is used only with a thread or method. If the JVM identify this it may decide to create the object on the stack, increasing performance of the Java program.   Escape analysis is a technique by which the Java Hotspot Server Compiler can analyze the scope of a new object’s uses and decide whether to allocate it on the Java heap.   Why EA help to boost application performance in JVM?   As you know, JVM will allocate memory space in different sections. They performs significanlty different, normally heap is slower than stack, which is even slower than CPU registers. So to improve system performance, we’d put our data step away from heap.  When your java source code being compiled, JIT will leverage Escape Analysis to allocate space on the method stack or even in CPU registers instead of Java heap space. This is a very important performance optimization because stack allocation and de-allocation are much faster than heap space allocation.   How escape analysis work   Based on escape analysis during JIT, an object’s escape state might be one of the following:      GlobalEscape – An object escapes the method and thread. For example, an object stored in a static field, or, stored in a field of an escaped object, or, returned as the result of the current method.   ArgEscape – An object passed as an argument or referenced by an argument but does not globally escape during a call. This state is determined by analyzing the bytecode of called method.   NoEscape – A scalar replaceable object, meaning its allocation could be removed from generated code.   After escape analysis, the server compiler eliminates scalar replaceable object allocations and associated locks from generated code. The server compiler also eliminates locks for all non-globally escaping objects.   Please beadvised it does not replace a heap allocation with a stack allocation for non-globally escaping objects.   JIT inline optimisation   The JIT aggressively inlines methods, removing the overhead of method calls. Methods that can be inlined include static, private or final methods but also public methods if it can be determined that they are not overridden.   Because of this, subsequent class loading can invalidate the previously generated code. Because inlining every method everywhere would take time and would generate an unreasonably big binary, the JIT compiler inlines the hot methods first until it reaches a threshold.  To determine which methods are hot, the JVM keeps counters to see how many times a method is called and how many loop iterations it has executed. This means that inlining happens only after a steady state has been reached, so you need to repeat the operations a certain number of times before there is enough profiling information available for the JIT compiler to do its job.   Rather than trying to guess what the JIT is doing, you can take a peek at what’s happening by turning on java command line flags: -XX:+PrintCompilation -XX:+UnlockDiagnosticVMOptions -XX:+PrintInlining   Here is what they do:   -XX:+PrintCompilation: logs when JIT compilation happens -XX:+UnlockDiagnosticVMOptions: enables other flags like -XX:+PrintInlining   Improve performance by removing locking  As you konw, Lock would significantely slow down or even freeze your application running in JVM.   During Escape analysis. GlobalEscape and ArgEscape objects must be allocated on the heap, but for ArgEscape objects it is possible to remove some locking and memory synchronization overhead because these objects are only visible from the calling thread.   The NoEscape objects may be allocated freely, for example on the stack instead of on the heap. In fact, under some circumstances, it is not even necessary to construct an object at all, but instead only the object’s scalar values, such as an int for the object Integer. Synchronization may be removed too, because we know that only this thread will use the objects. For example, if we were to use the somewhat ancient StringBuffer (which as opposed to StringBuilder has synchronized methods), then these synchronizations could safely be removed.   EA is currently only available under the C2 HotSpot Compiler so we have to make sure that we run in -server mode.   Why It Matters  In theory, NoEscape objects objects can be allocated on the stack or even in CPU registers using EA,  giving very fast execution.   When we allocate objects on the heap, we start to drain our CPU caches because objects are placed on different addresses on the heap possibly far away from each other. This way we will quickly deplete our L1 CPU cache and performance will decrease. With EA and stack allocation on the other hand, we are using memory that (most likely) is already in the L1 cache anyhow.  So, EA and stack allocation will improve our localization of data. This is good from a performance standpoint.   Obviously, the garbage collects needs to run much less frequently when we are using EA with stack allocation. This is perhaps the biggest performance advantage. Recall that each time the JVM runs a complete heap scan, we take performance out of our CPUs and the CPU caches will quickly deplete. Not to mention if we have virtual memory paged out on our server, whereby GC is devastating for performance.   The most important advantage of EA is not performance though. EA allows us to use local abstractions like Lambdas, Functions, Streams, Iterators etc. without any significant performance penalty so that we can write better and more readable code. Code that describes what we are doing rather than how it is done.   The GC cleans up the heap and not the stack. The stack is cleaned up automatically when methods return to their caller whereby the stack pointer is reset to its former value. So GC will clean up objects that ended up on the stack before EA/C2 compilation could be performed. The actual instances (or rather their corresponding representations) live on the stack, there are no referenced objects on the stack in the context of EA optimizations.   JIT optimization   Some JIT Compilation Techniques   One of the most common JIT compilation techniques used by Java HotSpot VM is inlining, which is the practice of substituting the body of a method into the places where that method is called. Inlining saves the cost of calling the method; no new stack frames need to be created. By default, Java HotSpot VM will try to inline methods that contain less than 35 bytes of JVM bytecode.   Another common optimization that Java HotSpot VM makes is monomorphic dispatch, which relies on the observed fact that, usually, there aren’t paths through a method that cause an object reference to be of one type most of the time but of another type at other times.   You might think that having different types via different code paths would be ruled out by Java’s static typing, but remember that an instance of a subtype is always a valid instance of a supertype (this principle is known as the Liskov substitution principle, after Barbara Liskov). This situation means that there could be two paths into a method—for example, one that passes an instance of a supertype and one that passes an instance of a subtype—which would be legal by the rules of Java’s static typing (and does occur in practice).   In the usual case (the monomorphic case), however, having different, path-dependent types does not happen. So we know the exact method definitions that will be called when methods are called on the passed object, because we don’t need to check which override is actually being used. This means we can eliminate the overhead of doing virtual method lookup, so the JIT compiler can emit optimized machine code that is often faster than an equivalent C++ call (because in the C++ case, the virtual lookup cannot easily be eliminated). The two Java HotSpot VM compiler modes use different techniques for JIT compilation, and they can output very different machine code for the same Java method. Modern Java applications, however, can usually make use of both compilation modes.   Java HotSpot VM uses many other techniques to optimize the code that JIT compilation produces. Loop optimization, type sharpening, dead-code elimination, and intrinsics are just some of the other ways that Java HotSpot VM tries to optimize code as much as it can. Techniques are frequently layered one on top of another, so that once one optimization has been applied, the compiler might be able to see more optimizations that can be performed.   Compilation Modes   Inside Java HotSpot VM, there are actually two separate JIT compiler modes, which are known as C1 and C2. C1 is used for applications where quick startup and rock-solid optimization are required; GUI applications are often good candidates for this compiler. C2, on the other hand, was originally intended for long-running, predominantly server-side applications. Prior to some of the later Java SE 7 releases, these two modes were available using the -client and -server switches, respectively.   The two compiler modes use different techniques for JIT compilation, and they can output very different machine code for the same Java method. Modern Java applications, however, can usually make use of both compilation modes. To take advantage of this fact, starting with some of the later Java SE 7 releases, a new feature called tiered compilation became available. This feature uses the C1 compiler mode at the start to provide better startup performance. Once the application is properly warmed up, the C2 compiler mode takes over to provide more-aggressive optimizations and, usually, better performance. With the arrival of Java SE 8, tiered compilation is now the default behavior.   Java memory monitoring tools    pemi$ jps | grep Main 50903 Main pemi$ jmap -histo 50903 | head  num     #instances         #bytes  class name  ----------------------------------------------    1:            95       42952184  [I    2:          1079         101120  [C    3:           485          55272  java.lang.Class    4:           526          25936  [Ljava.lang.Object;    5:            13          25664  [B    6:          1057          25368  java.lang.String    7:            74           5328  java.lang.reflect.Field   jmap - Memory Map   Tool or Option  Description and Usage   Java Mission Control   Java Mission Control (JMC) is a new JDK profiling and diagnostic tools platform for HotSpot JVM. It s a tool suite basic monitoring, managing, and production time profiling and diagnostics with high performance. Java Mission Control minimizes the performance overhead that’s usually an issue with profiling tools. See Java Mission Control.   jcmd utility   The jcmd utility is used to send diagnostic command requests to the JVM, where these requests are useful for controlling Java Flight Recordings. The JFRs are used to troubleshoot and diagnose JVM and Java Applications with flight recording events. See The jcmd Utility.   Java VisualVM   This utility provides a visual interface for viewing detailed information about Java applications while they are running on a Java Virtual Machine. This information can be used in troubleshooting local and remote applications, as well as for profiling local applications. See Java VisualVM.   JConsole utility   This utility is a monitoring tool that is based on Java Management Extensions (JMX). The tool uses the built-in JMX instrumentation in the Java Virtual Machine to provide information about performance and resource consumption of running applications. See JConsole.   jmap utility   This utility can obtain memory map information, including a heap histogram, from a Java process, a core file, or a remote debug server. See The jmap Utility.   jps utility   This utility lists the instrumented Java HotSpot VMs on the target system. The utility is very useful in environments where the VM is embedded, that is, it is started using the JNI Invocation API rather than the java launcher. See The jps Utility.   jstack utility   This utility can obtain Java and native stack information from a Java process. On Oracle Solaris and Linux operating systems the utility can alos get the information from a core file or a remote debug server. See The jstack Utility.   jstat utility   This utility uses the built-in instrumentation in Java to provide information about performance and resource consumption of running applications. The tool can be used when diagnosing performance issues, especially those related to heap sizing and garbage collection. See The jstat Utility.   jstatd daemon   This tool is a Remote Method Invocation (RMI) server application that monitors the creation and termination of instrumented Java Virtual Machines and provides an interface to allow remote monitoring tools to attach to VMs running on the local host. See The jstatd Daemon.   visualgc utility   This utility provides a graphical view of the garbage collection system. As with jstat, it uses the built-in instrumentation of Java HotSpot VM. See The visualgc Tool.   Native tools   Each operating system has native tools and utilities that can be useful for monitoring purposes. For example, the dynamic tracing (DTrace) capability introduced in Oracle Solaris 10 operating system performs advanced monitoring. See Native Operating System Tools.   $ jps 16217 MyApplication 16342 jps  The utility lists the virtual machines for which the user has access rights. This is determined by access-control mechanisms specific to the operating system. On Oracle Solaris operating system, for example, if a non-root user executes the jps utility, then the output is a list of the virtual machines that were started with that user's uid.  In addition to listing the PID, the utility provides options to output the arguments passed to the application's main method, the complete list of VM arguments, and the full package name of the application's main class. The jps utility can also list processes on a remote system if the remote system is running the jstatd daemon.    GC-less Java   Java Development without GC All products developed by Coral Blocks have the very important feature of leaving ZERO garbage behind. Because the latency imposed by the Java Garbage Collector (i.e. GC) is unacceptable for high-performance systems and because it is impossible to turn off the GC, the best option for real-time systems in Java is to not produce any garbage at all so that the GC never kicks in. Imagine a high-performance matching engine operating in the microsecond level, sending and receiving hundreds of thousands messages per second. If at any given time the GC decides to kick in with its 1+ millisecond latencies, the disruption in the system will be huge. Therefore, if you want to develop real-time systems in Java with minimal variance and latency, the best option is to do it right without creating any garbage for the GC.   Warming up, Checking the GC and Sampling  The key to make sure your system is not creating any garbage is to warm up your critical path from start to finish a couple of million times and then check for memory allocation another couple of million times. If it is allocating memory linearly as the number of iterations increases, it is most likely creating garbage and you should use the stack trace  ","categories": [],
        "tags": ["JVM warm up","Java","Low latency"],
        "url": "/2020/03/28/JVM-Warm-up.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Java Deep Notes",
        "excerpt":"Java Deep Notes   Is string concatenation a devil?   In fact, a string concatenation is going to be just fine, as the javac compiler will optimize the string concatenation as a series of append operations on a StringBuilder anyway. Here’s a part of the disassembly of the bytecode from the for loop from the above program:   ","categories": [],
        "tags": ["Java"],
        "url": "/2020/05/10/Java-Deep-Notes.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Java Pesistence API Notes",
        "excerpt":"JPA   As you create Session from a SessionFactory, it’s not hard to understand that you use EntityManagerFactory to create an instance of EntityManager. However, because JPA is a standard applicable to Enterprise and Standalone applications, there are a couple of modes for obtaining or creating the EntityManagerFactory itself—one that will be created in a managed environment such as Application Servers or Web containers while the other in a standalone application.   Once you have the EntityManager (obtained from EntityManagerFactory), the next step is to declare a persistence unit. A persistence unit is a logical group of persistent classes (called entities in JPA lingo), database settings, and relational mappings.   The JPA specification classifies two types of entity managers: one that runs in a con- tainer-managed environment, and another that runs in a standalone JVM. The former one is typically a Java Enterprise Edition (JEE) container such as an application server or a web container, while the latter is a Java Standard Edition standalone program.   The EntityManager itself is no different in both types but the EntityManagerFactory that creates the EntityManager is a bit unique in how it is configured and created.   In a standalone environment, you should create the EntityManager as shown here:   private EntityManagerFactory factory = null;  private EntityManager entityManager = null; .. private void init() { factory = Persistence.createEntityManagerFactory(\"trade-mysql-pu\"); entityManager = factory.createEntityManager(); }   You should pass in the name of the previously defined persistence unit (from persistence.xml) to the createEntityManagerFactory() method. We then obtain the EntityManager by calling createEntityManager on the factory.  ","categories": [],
        "tags": ["Java","JPA"],
        "url": "/2020/06/21/JPA-Notes.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Awesome SSL certificates and HTTPS",
        "excerpt":"What’s TLS  TLS (Transport Layer Security) and its predecessor, SSL (Secure Sockets Layer), are security protocols designed to secure the communication between a server and a client, for example, a web server and a browser. Both protocols are frequently referred to as SSL.   A TLS/SSL certificate (simply called SSL certificate) is required to enable SSL/TLS on your site and serve your website using the secure HTTPS protocol.   We offer different types of domain-validated SSL certificates signed by globally recognized certificate authorities.   CA  A Certificate Authority (CA) (or Certification Authority) is an entity that issues digital certificates.   The digital certificate certifies the ownership of a public key by the named subject of the certificate. This allows others (relying parties) to rely upon signatures or assertions made by the private key that corresponds to the public key that is certified.   Root certificate  In the SSL ecosystem, anyone can generate a signing key and sign a new certificate with that signature. However, that certificate is not considered valid unless it has been directly or indirectly signed by a trusted CA.   A trusted certificate authority is an entity that has been entitled to verify that someone is effectively who it declares to be. In order for this model to work, all the participants on the game must agree on a set of CA which they trust. All operating systems and most of web browsers ship with a set of trusted CAs.   The SSL ecosystem is based on a * model of trust relationship*, also called ** “chain of trust” **. When a device validates a certificate, it compares the certificate issuer with the list of trusted CAs. If a match is not found, the client will then check to see if the certificate of the issuing CA was issued by a trusted CA, and so on until the end of the certificate chain. The top of the chain, the root certificate, must be issued by a trusted Certificate Authority.   Tips  The root certificate is generally embedded in your connected device. In the case of web browsers, root certificates are packaged with the browser software.   To  install the Intermediate SSL certificates?   The procedure to install the Intermediate SSL certificates depends on the web server and the environment where you install the certificate.   For instance, Apache requires you to bundle the intermediate SSL certificates and assign the location of the bundle to the SSLCertificateChainFile configuration. Conversely, NGINX requires you to package the intermediate SSL certificates in a single bundle with the end-user certificate.   SSL certificate chain  There are two types of certificate authorities (CAs): root CAs and intermediate CAs. In order for an SSL certificate to be trusted, that certificate must have been issued by a CA that is included in the trusted store of the device that is connecting.   In this model of trust relationships, a CA is a trusted third party that is trusted by both the subject (owner) of the certificate and the party relying upon the certificate.   In the context of a website, when we use the term digital certificate we often refer to SSL certificates. The CA is the authority responsible for issuing SSL certificates publicly trusted by web browsers.   Anyone can issue SSL certificates, but those certificates would not be trusted automatically by web browsers. Certificates such as these are called self-signed. The CA has the responsibility to validate the entity behind an SSL certificate request and, upon successful validation, the ability to issue publicly trusted SSL certificates that will be accepted by web browsers. Essentially, the browser vendors rely on CAs to validate the entity behind a web site.   How SSL work in browser  There are 3 essential elements at work in the process described above: a protocol for communications (SSL), credentials for establishing identity (the SSL certificate), and a third party that vouches for the credentials (the certificate authority).   Computers use protocols to allow different systems to work together. Web servers and web browsers rely on the Secure Sockets Layer (SSL) protocol to enable encrypted communications. The browser’s request that the server identify itself is a function of the SSL protocol. Credentials for establishing identity are common to our everyday lives: a driver’s license, a passport, a company badge. An SSL certificate is a type of digital certificate that serves as a credential in the online world. Each SSL certificate uniquely identifies a specific domain (such as thawte.com) and a web server. Our trust of a credential depends on our confidence in the organization that issued it. Certificate authorities have a variety of methods to verify information provided by individuals or organizations. Established certificate authorities, such as Thawte, are well known and trusted by browser vendors. Browsers extend that trust to digital certificates that are verified by the certificate authority.   PKI  You are correct that SSL uses an asymmetric key pair. One public and one private key is generated which also known as public key infrastructure (PKI). The public key is what is distributed to the world, and is used to encrypt the data. Only the private key can actually decrypt the data though.      Say we both go to walmart.com and buy stuff. Each of us get a copy of Walmart’s public key to sign our transaction with. Once the transaction is signed by Walmart’s public key, only Walmart’s private key can decrypt the transaction. If I use my copy of Walmart’s public key, it will not decrypt your transaction. Walmart must keep their private key very private and secure, else anyone who gets it can decrypt transactions to Walmart. This is why the DigiNotar breach was such a big deal    A sample of how browser get SSL certificate   If I get an SSL certificate from a well-known provider, what does that prove about my site and how?   Here’s what I know:   Assume Alice and Bob both have public and private keys If Alice encrypts something with Bob's public key, she ensures that only Bob can decrypt it (using his private key) If Alice encrypts something with her own private key, anyone can decrypt it (using her public key), but they will know that it was encrypted by her Therefore, if Alice encrypts a message first with her own private key, then with Bob's public key, she will ensure that only Bob can decrypt it and that Bob will know the message is from her.   Regarding certificates, here’s what I think happens (updated):   I generate a request for a certificate. In that request, I put my public key and a bunch of information about myself. The certificate issuer (in theory) checks me out to make sure it knows who I am: talks to me in person, sees my driver's license, retina scan, or whatever. If they're satisfied, the certificate issuer then encrypts my request with their private key. Anyone who decrypts it with their public key knows that they vouch for the information it contains: they agree that the public key is mine and that the information stated is true about me. This encrypted endorsement is the certificate that they issue to me. When you connect to my site via https, I send you the certificate. Your browser already knows the issuer's public key because your browser came installed with that information. Your browser uses the issuer's public key to decrypt what I sent you. The fact that the issuer's public key works to decrypt it proves that the issuer's private key was used to encrypt it, and therefore, that the issuer really did create this certificate. Inside the decrypted information is my public key, which you now know has been vouched for. You use that to encrypt some data to send to me.   Your key theory: basically right, but authentication is usually done by encrypting a cryptographically secure hash of the data rather than the data itself.   A CA’s signature on an SSL certificate should indicate that the CA has done a certain amount of diligence to ensure that the credentials on the certificate match the owner. That diligence varies, but the ultimate point is that they’re saying that the certificate they signed belongs to the entity named on it.   See http://en.wikipedia.org/wiki/Digital_signature#Definition   A public key certificate is the signed combination between a public key, identifiers, and possibly other attributes. Those who sign this document effectively assert the authenticity of the binding between the public key and the identifier and these attributes, in the same way as a passport issuing authority asserts the binding between the picture and the name in a passport, as various other pieces of information (nationality, date of birth, …).   The private key is used for signing and deciphering/decrypting. The public key is used for verifying signatures and enciphering/encrypting.   public key cryptography: A class of cryptographic techniques employing two-key ciphers. Messages encrypted with the public key can only be decrypted with the associated private key. Conversely, messages signed with the private key can be verified with the public key.   It should be pointed out, along with all the other answers, that your private key is not always just one key that is used for both decrypting and signing messages. These should be two separate keys. This would create 4 keys for each person:   Public Encryption Key - Used to encrypt data to send to me.   Private Decryption Key - Used to decrypt messages that were encrypted using my Public Encryption Key.   Private Signing Key - Used to sign messages that I send to other people.   Public Verify Key - Used to verify that a message was, in fact, signed by me.   https://en.wikipedia.org/wiki/Savvis   Savvis - Wikipedia   Savvis, formerly SVVS on Nasdaq and formerly known as Savvis Communications Corporation, and, later, Savvis Inc., is a subsidiary of CenturyLink, a company headquartered in Monroe, Louisiana.[1] The company sells managed hosting and colocation services with more than 50 data centers[2] (over 2 million square feet) in North America, Europe, and Asia, automated management and provisioning systems, and information technology consulting. Savvis has approximately 2,500 unique business and government customers.[3][4]   The file extensions .CRT and .CER are interchangeable.  If your server requires that you use the .CER file extension, you can change the extension by following the steps below:   Double-click on the yourwebsite.crt file to open it into the certificate display. Select the Details tab, then select the Copy to file button. Hit Next on the Certificate Wizard. Select Base-64 encoded X.509 (.CER), then Next. Select Browse (to locate a destination) and type in the filename yourwebsite. Hit Save. You now have the file yourwebsite.cer   File extensions for cryptographic certificates aren't really as standardized as you'd expect. Windows by default treats double-clicking a .crt file as a request to import the certificate into the Windows Root Certificate store, but treats a .cer file as a request just to view the certificate. So, they're different in that sense, at least, that Windows has some inherent different meaning for what happens when you double click each type of file.   But the way that Windows handles them when you double-click them is about the only difference between the two. Both extensions just represent that it contains a public certificate. You can rename a file or use one in place of the other in any system or configuration file that I’ve seen. And on non-Windows platforms (and even on Windows), people aren’t particularly careful about which extension they use, and treat them both interchangeably, as there’s no difference between them as long as the contents of the file are correct.   *.pem, *.crt, *.ca-bundle, *.cer, *.p7b, *.p7s files contain one or more X.509 digital certificate files that use base64 (ASCII) encoding.   .DER = The DER extension is used for binary DER encoded certificates. These files may also bear the CER or the CRT extension.   Proper English usage would be “I have a DER encoded certificate” not “I have a DER certificate”.   .PEM = The PEM extension is used for different types of X.509v3 files which contain ASCII (Base64) armored data prefixed with a “—– BEGIN …” line.   .CRT = The CRT extension is used for certificates. The certificates may be encoded as binary DER or as ASCII PEM. The CER and CRT extensions are nearly synonymous.  Most common among *nix systems   CER = alternate form of .crt (Microsoft Convention) You can use MS to convert .crt to .cer (.both DER encoded .cer, or base64[PEM] encoded .cer)  The .cer file extension is also recognized by IE as a command to run a MS cryptoAPI command (specifically rundll32.exe cryptext.dll,CryptExtOpenCER) which displays a dialogue for importing and/or viewing certificate contents.   .KEY = The KEY extension is used both for public and private PKCS#8 keys. The keys may be encoded as binary DER or as ASCII PEM.   The only time CRT and CER can safely be interchanged is when the encoding type can be identical.  (ie  PEM encoded CRT = PEM encoded CER)   What is the SSL Certificate Chain?   There are two types of certificate authorities (CAs): root CAs and intermediate CAs. In order for an SSL certificate to be trusted, that certificate must have been issued by a CA that is included in the trusted store of the device that is connecting.   Good. I see you want to access this particular page.  I need to send the page to you in a secure way. If I encrypt it using my public key, you won’t be able to decrypt it because you don’t have my private key. And since you don’t have any public key of your own that I can use to encrypt the page for you here’s what I propose Since you can send me encrypted messages that only me can read (you have my public key), send me an encrypted message with an encryption key in it. Just make up a random encryption key that we’ll both use to encrypt and decrypt the messages between us during this session .   A simple symmetric key is enought. We’ll use the same key to encrypt and decrypt the messages.      So there’s no way that anybody with your public key can trick others to believe that he is you ?   Nope. That’s the beauty of the assymetric encryption.   When you send the public key to the victim’s contain your public key + a certificate that this public key belongs to you. If you are a website, then the certificate will contain the domain name of the website. Basically, a certificate says something like:  the following public key “XYZ123” belongs to example.com.   that’s why we have “Certificate Authorities” like Verisign, Digicert or even Symantec. It is believed that these companies have the necessary trustworthiness to deliver certificates to different •entities. Think of a CA like a registrar for public keys. Just like registrars assert that a domain name belongs to a certain person or company, CAS assert that a public key belongs to a certain domain name (or IP address) .   The certificate will contain the CA that delivered it, but you don’t even have to check with them because the certificate is signed by them. That signature alone is enough proof that the certificate comes from them.   A signature is simply a small message that is encrypted with their private key. Since private keys are asymetric, that means that only the associated public key can decrypt it.   Asymmetric encryption works in both way. public -&gt; private and private -&gt; public. What the public key encrypts only the private key can decrypt, and what the private key can encrypt only the public key can decrypt.   for PKI, we’re not looking for secrecy here, we only want to prove that we’ re the real authors of the message. Suppose I send you the message “HELLO WORLD”, encrypted with my private key. The encrypted message would be, for example, “XYZ1234”. So you receive “XYZ1234” . If I give you my public key, you would be able to decrypt “XYZ1234” into “HELLO WORLD” . And by doing so, you would have proof that that message was sent by me, because the public key you used decrypts messages that were encrypted by my private key only. And since I am the only person in the universe who has that private key, that proves that I am the author of that message.   Really nice. So I don’t have to contact the CA to check the validity of the certificate, all I have to do is use their public key to decrypt the signature that’s in it. If it’s the same as err, wait, what should I compare the decrypted signature to again ?   You have to find the same hash as the one you have calculated. They are sending a small hash of the whole certificate. So what you have to do is to calculate the hash of the certificate yourself, then compare it to the hash you get when you decrypt the signature. If the two are the same that means two things     The CA’s public key worked, so the signature was encrypted by the associated private key, which means the certificate was really issued by the CA.   Since the hash is the same, it also means that you are seeing the exact same certificate that the CA delivered to the website you are visiting. The information contained inside hasn’t been tampered with.   That’s really good. So, let me recap one more time .     I contact you for an HTTPS page.   You send me an SSL certificate that contains your public key and a signature from the CA that delivered   I make sure the certificate is valid by using the CA’s public key to decrypt the signature. In parallel, I also calculate the hash of the certificate.   If my hash and the one I got from decrypting the signature are equal, that means that the certificate was really issued by the CA and that I can be sure that the public key you sent me is really yours.   Because you implicitly trust the CA.   Let’s continue:     I generate a random key that we’ll both use as a symmetric key to encrypt and decrypt the messages we’ll be sending each other.   I encrypt this symmetric key with your public key and send it to you.   You decrypt my message with your private key and find my secret key.   Every request or response between us will be encrypted with this shared secret symmetric key.   CN  The Common Name (AKA CN) represents the server name protected by the SSL certificate.   The certificate is valid only if the request hostname matches the certificate common name.   To check the status, such as  sudo openssl x509 -noout -in xxx.com.cer -text  Subject: C=UK, ST=London, L=London, O=AAA Bank, OU=Product and Markets, CN=*.xxxtest.com         Subject Public Key Info:   commonName format   The common name is not a URL. It doesn’t include any protocol (e.g. http:// or https://), port number, or pathname. For instance, https://example.com or example.com/path are incorrect. In both cases, the common name should be example.com   Common Name vs Subject Alternative Name   The common name can only contain up to one entry: either a wildcard or non-wildcard name. It’s not possible to specify a list of names covered by an SSL certificate in the common name field.   The Subject Alternative Name extension (also called Subject Alternate Name or SAN) was introduced to solve this limitation. The SAN allows issuance of multi-name SSL certificates.   SHA-2 SSL Certificates  Almost all certificates are currently signed with the SHA-2 hash algorithm.   This article provides a simple overview of the SHA-1 to SHA-2 transition plans, as well additional informations on the SHA-2 hash algorithm and SSL certificates purchased with DNSimple previous than September 2014.   The SHA family of hashing algorithms were developed by the National Institute of Standards and Technology (NIST) and are used by certificate authorities (CAs) when digitally signing issued certificates.   Reference     https://support.dnsimple.com/articles/what-is-ssl-certificate-chain/   https://www.thawte.com/resources/getting-started/how-ssl-works/  ","categories": [],
        "tags": ["SSL","HTTPS"],
        "url": "/2020/06/26/TLS-SSL-HTTPS.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "A Facial Recognition utility in a dozen of LOC",
        "excerpt":"A Facial Recognition utility in a dozen of python LOC (Lines Of Code)   CV (Computer Vision)  I have been soak myself in open sourced libraries, such as OpenCV. I gradually came to discern concepts such as Machine Learning , Deep Learning are not academic standing water. As a matter of fact, those elusive topics and certain pragmatic use cases could coalesce in a amount of interesting products. For instance, in past couple of months, there were a hype of guess-ages-by-photo, below screenshot depicts such.      What a seductive one! Initially been attracted by such funky features, after second thoughts, I found at the heart of it is two cohesive parts, the first one is how to locate human faces from background and whole picture, consequently to have a ballpark age guess for the recognized the faces. You may guess how difficult to codify a program to implement the 1st feature. Actually no need chunks of code, at here purely a dozen of lines of code are necessitated (actually only 10 lines of code, excluding space line and comments). I’d like to piggyback on such tiny utility to elaborate advanced topics of Computer Visions.   Faces recognition  Actually face recognition is not new to us, this feature prevailing in so-called auto focus in DC (Digital Camera) and many main stream smart phone built-in cameras. Just like below photo. You can get a sense of how commonplace of face recognition , which is becoming a widely used technology around us.      Theoretically speaking, face recognition is also called face detection, it’s a type of technology/program to electronically identify human frontal faces in digital images, such as photos, camera or surveillance. Further more, face detection is kind of objects detection in computer vision area. Which will locate object (e.g. human face) and get the size.   My ‘10 LOC program’  First of all, let’s have some visual and concrete feeling of this program, below screenshot is the source code.      The whole program source code can be found at  this github repository https://github.com/CloudsDocker/pyFacialRecognition . Please feel free to fork , check it out and have a try. I’ll walk through this program one line by one line at this blog.   “You serious? This is all the problem, just aforementioned 10 lines?” Let’s first take a look at the actual run output.   Here is the origional image     Below is the result of execution of this tiny utility  Please be advised the red rectangle around faces.    Souce Code  Prerequite  First of first, as you know, this program is composed by python,therefore, make sure you work station or laptop equiped with python, vesrion is irrelavant for this program.   In addition, this utility is built upon OpenCV (http://opencv.org/downloads.html), therefore please install this component as well. Just as its name suggested, it is an open source framework focus on computer vision related deep learning, surfaced decades ago. This is one Intel lab built by Rusian, which is a very active community.   Particulary, if you are Mac users, it’s recommended to use brew to setup OpenCV. Below is sample commands(The 1st line of following command may raise some errors, in that case please contact me via the link at the rear of this blog):  brew tap homebrew/science brew install opencv   Upon completion of preceding scripts, you can execute following scripts to verify whether it’s installed success or not, e.g. it means all fine if no exception/errors raised  &gt;&gt;&gt; import cv2   Souce Code Dissection  Let’s dissect file recognizeFace_loose_en.py as one example   import cv2,sys     To import library of OpenCV and python built-in system library, which is used to parse input arguments.   inputImageFile=sys.argv[1]     To read the 1st argument, which to be the file name of the image to be parsed, e.g. test.jpg   faceClassifier=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')      To load HAAR Casscade Classifier, the human face recognition cascade categorizer which shipped with OpenCV. Which will do the actual computation, logic to recognize and size human faces from any given images.   Expansion of computer vision knowledge  We stop here not reading further code, avoiding perplex you, I’ll walk through certain CV topics pertaining to this blog. As for more deep concepts, please feel free to contact me or goole by yourself.   Classifier  In arena of computer vision and machine learning, a variaty of classifiers been and being built, to assemle special domain knowledge to recognize corresponding objects. For example, there are particular classifier to recognize cars, there are plane classifier, and classifiers to recognize smile, eyes, etc. For our case, we need a specific classifier help us to detect and locate human faces.   Conceps of objects recognize  Generally speaking，, to recognize one object (such as human faces) means finding and identifying objects in an image or video sequence. However, it’s neccessitate tons of sample/specimen to train machine to learn, for instance, it’s likely thousands of hundreds of digital images/video will be prepared as learning material, while all of specimen should be categorized to two mutax type,  positive or negative. e.g. phots containss human face and ones without human face. When machine read one photo, it was told this is either a positive one or negative one, then machine could gradually analysys and induce some common facets and persist to files for future usages, e.g. when given a new photo, the machine can classify it whether it’s a positive or negative. That’s why it’s called classifier.   Cascade  Your feeling is right, just as it’s name suggrested, cascade implies propagating something. In this case, it’s specifically means Cascade classifier. Intuitively the next question is why cascade is required? Let me try to articulate the underlying logic, as you know, at the heart of digital images, which is the raw material of computer vision, are pixel。For one CV process, it need to scan each pixel per pixel, while in contemporary world, size of image tend to incresing more than we expected, e.g. normall one photo taken by smart phone tend to contains millions of pixels. At the meanwhile, to fine tune and get a more accuate result of one object recognition, it tend to lots of classifiers to work from different point of views of the underlying photo. Therefore these two factors interwhirled together, the final number would be astronomical. Therefore, one innovative solution is cascade, in a nutshell, all classifiers will be splited to multiple layers, one photo will be examined by classifiers on 1st layer at the very begining, if failed, the whole CV can retain negative immediately, with fewest efforts and time cost, while majority of other classifiers won’t be executed in actual. This should significantely accelerate the whole process of CV. This is similar to FF(Fail Fast) in other areas,severed for sake of running efficiency.   objImage=cv2.imread(inputImageFile)     To create one OpenCV image object by loading the input digital file via OpenCV   cvtImage=cv2.cvtColor(objImage,cv2.COLOR_BGR2GRAY)     Firstly, convert the digital colorful image to grayscale one, which easy the task to scan and analyse the image. Actually this is quite common in image analys area. e.g. this could eliminate those noisy pixel from the picture.   foundFaces=faceClassifier.detectMultiScale(cvtImage,scaleFactor=1.3,minNeighbors=9,minSize=(50,50),flags = cv2.cv.CV_HAAR_SCALE_IMAGE)     Call method detectMultiScale to recongnize object, i.e. human face in this case. The parameters overview as below:   scaleFactor: For a photo, particualy from selpie, some faces are shows bigger than rest of others, due to the distance between each faces and lens. Therefore this parameter is used to config the factor, please be advised this double should greater than 1.0   minNeighbors: Because it need to gradually scan the photo by a certain window, i.e. a rectangle. So this parameter is telling how many other object in the vacinity to be detected, before making final decision that it’s positive or negative.   minSize：For aforementioend window, this parameter is setting the size of this rectangle.   print(\" Found {} human faces in this image\".format(len(foundFaces)))     To print how many faces detected, be reminded returned value is a list, each item is the actual position of every faces. Therefore, using  len  to print total number of ojects found.   for (x,y,w,h) in foundFaces:     cv2.rectangle(objImage,(x,y),(x+w,y+h),(0,0,255),2)     Traverese all faces detected, please be noted returning object is consist of 4 parts, i.e. the horizontal and vertial position, width and height.   Consequently to draw a rectangle by an off-the-shelf method from OpenCV. Be advised (0,0,255) represents color of the rectangel. It use R/G/B mode, e.g. black is (0,0,0)，white is (255,255,255)，etc. Well versed web programmer should be familiar with it.   cv2.imshow('Detected human faces highlighted. Press any key to exit. ', objImage) cv2.waitKey(0)     To display this image via opencv provided method imshow, together with the rectangles we draw previously   The last one is one user hint, remind you can quit the applicaiton by press any key on the image display window   In summary  We’ve skimmed source codes and pertaining knowledge. This is just scratched the surface of this framework, hope this can open the door to more advanced topics and insights, such as hack of CAPTCHA, newly open sourced project form Yahoo, NSFW, Not Suitable for Work (NSFW)，to detect images with pornagraphy, etc.   Finally，please be reminded all related source are open sourced at github repository https://github.com/CloudsDocker/pyFacialRecognition ，please fork and sync to your local disk, check it out and paly it.  git clone https://github.com/CloudsDocker/pyFacialRecognition.git cd pyFacialRecognition ./run.sh   You can access my blog. Any comments/suggestions, feel free to contact me.   Contact me：     phray.zhang@gmail.com (email，whatsapp, linkedin)   helloworld_2000 (wechat)   weibo: cloudsdocker   github   jianshu   wechat：vibex   Reference     Object recognition   OpenCV   HAAR features   Face Detection using Haar Cascades   NSFW  ","categories": [],
        "tags": ["DeepLearning","MachineLearning","FacialRecognition","Python","Face Recongnition"],
        "url": "/2020/06/28/Facial-Recognition_en.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Concurrency in Java",
        "excerpt":"How to make thread-safe   You can create a new object for each method call   Sample code as SpelExpressionParser in Spring   public class SpelExpressionParser extends TemplateAwareExpressionParser { \t//.... \t// Be noticed it will new a object each time, so it's thread safe \t@Override \tprotected SpelExpression doParseExpression(String expressionString, @Nullable ParserContext context) throws ParseException { \t\treturn new InternalSpelExpressionParser(this.configuration).doParseExpression(expressionString, context); \t}  }   :books:Concepts  Threads are sometimes called lightweight processes, and most modern operating systems treat threads, not processes, as the basic units of scheduling.   Liveness   While safety means \"nothing bad ever happens\", liveness concerns the complementary goal that \"something good eventually happens\". A liveness failure occurs when an activity gets into a state such that it is permanently unable to make forward progress.   thread-safety   Whether an object needs to be thread-safe depends on whether it will be accessed from multiple threads. This is a property of how the object is used in a program, not what it does. Making an object thread-safe requires using synchronization to coordinate access to its mutable state; failing to do so could result in data corruption and other undesirable consequences.   Whenever more than one thread accesses a given state variable, and one of them might write to it, they all must coordinate their access to it using synchronization. The primary mechanism for synchronization in Java is the synchronized keyword, which provides exclusive locking, but the term “synchronization” also includes the use of volatile variables, explicit locks, and atomic variables.   How to fix borken multithreading   If multiple threads access the same mutable state variable without appropriate synchronization, your program is broken. There are three ways to fix it:     Don’t share the state variable across threads;   Make the state variable immutable;   Use synchronization whenever accessing the state variable.   This “code confidence” is about as close as many of us get to correctness, so let’s just assume that single-threaded correctness is something that “we know it when we see it”.   A class is thread-safe when it continues to behave correctly when accessed from multiple threads.      A class is thread-safe if it behaves correctly when accessed from multiple threads, regardless of the scheduling or interleaving of the execution of those threads by the runtime environment, and with no additional synchronization or other coordination on the part of the calling code.    race condition  A race condition occurs when getting the right answer relies on lucky timing.   The most common type of race condition is check-then-act, where a potentially stale observation is used to make a decision on what to do next.   check-then-act  It is this invalidation of observations that characterizes most race conditions— using a potentially stale observation to make a decision or perform a computation. This type of race condition is called check-then-act: you observe something to be true (file X doesn’t exist) and then take action based on that observation (create X); but in fact the observation could have become invalid between the time you observed it and the time you acted on it (someone else created X in the meantime), causing a problem (unexpected exception, overwritten data, file corruption).   A common idiom that uses check-then-act is lazy initialization.   To ensure thread safety, check-then-act operations (like lazy initialization) and read-modify-write operations (like increment) must always be atomic.   Sample   this. @NotThreadSafe public class LazyInitRace {           private ExpensiveObject instance = null;           public ExpensiveObject getInstance() {                    if (instance == null)                        instance = new ExpensiveObject();                    return instance;           } }    consequences  Like most concurrency errors, race conditions don’t always result in failure: some unlucky timing is also required. But race conditions can cause serious problems.   Intrinsic lock   Every Java object can implicitly act as a lock for purposes of synchronization; these built-in locks are called intrinsic locks or monitor locks. The lock is automatically acquired by the executing thread before entering a synchronized block and automatically released when control exits the synchronized block, whether by the normal control path or by throwing an exception out of the block.   Intrinsic locks in Java act as mutexes (or mutual exclusion locks), which means that at most one thread may own the lock. When thread A attempts to acquire a lock held by thread B, A must wait, or block, until B releases it. If B never releases the lock, A waits forever.   Reentrancy lock  Reentrancy means that locks are acquired on a per-thread rather than per-invocation basis.[7] Reentrancy is implemented by associating with each lock an acquisition count and an owning thread. When the count is zero, the lock is considered unheld.   When a thread acquires a previously unheld lock, the JVM records the owner and sets the acquisition count to one. If that same thread acquires the lock again, the count is incremented, and when the owning thread exits the synchronized block, the count is decremented. When the count reaches zero, the lock is released.   Guarding State with Locks  Because locks enable serialized access to the code paths they guard, we can use them to construct protocols for guaranteeing exclusive access to shared state. Following   However, just wrapping the compound action with a synchronized block is not sufficient; if synchronization is used to coordinate access to a variable, it is needed everywhere that variable is accessed. Further, when using locks to coordinate access to a variable, the same lock must be used wherever that variable is accessed.      It is a common mistake to assume that synchronization needs to be used only when writing to shared variables; this is simply not true. For each mutable state variable that may be accessed by more than one thread, all accesses to that variable must be performed with the same lock held. In this case, we say that the variable is guarded by that lock.    The fact that every object has a built-in lock is just a convenience so that you needn’t explicitly create lock objects.[9]   Every shared, mutable variable should be guarded by exactly one lock. Make it clear to maintainers which lock that is.  ","categories": [],
        "tags": [],
        "url": "/2020/06/29/Java-Concurrency.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Awesome solutions for algorithm questions",
        "excerpt":"你就会发现只要涉及递归的问题，都是 树的问题。   Different Trees     Full Binary Tree : For every node, it can either has no children or two children   Complete Binary Tree: Leaf nodes are aligned leftwards (it must be left child if there is only one child)   Perfect Binary Tree: every node has two children   但是必须说明的是，不管怎么优化，都符合回溯框架，而且时间复杂度都不 可能低于 O(N!)，因为穷举整棵决策树是无法避免的。这也是回溯算法的一 个特点，不像动态规划存在重叠子问题可以优化，回溯算法就是纯暴力穷 举，复杂度一般都很高。   vector&lt;vector&lt;string&gt;&gt; res; /* 输入棋盘边⻓ n，返回所有合法的放置 */ vector&lt;vector&lt;string&gt;&gt; solveNQueens(int n) { // '.' 表示空，'Q' 表示皇后，初始化空棋盘。 vector&lt;string&gt; board(n, string(n, '.')); backtrack(board, 0); return res; } // 路径:board 中小于 row 的那些行都已经成功放置了皇后 // 选择列表:第 row 行的所有列都是放置皇后的选择 // 结束条件:row 超过 board 的最后一行 void backtrack(vector&lt;string&gt;&amp; board, int row) { // 触发结束条件 if (row == board.size()) { res.push_back(board); return; } int n = board[row].size(); for (int col = 0; col &lt; n; col++) { // 排除不合法选择 if (!isValid(board, row, col)) continue; // 做选择 board[row][col] = 'Q'; // 进入下一行决策 backtrack(board, row + 1); // 撤销选择 board[row][col] = '.';  } } /* 是否可以在 board[row][col] 放置皇后? */ bool isValid(vector&lt;string&gt;&amp; board, int row, int col) { int n = board.size(); // 检查列是否有皇后互相冲突 for (int i = 0; i &lt; n; i++) {     if (board[i][col] == 'Q')         return false; } // 检查右上方是否有皇后互相冲突 for (int i = row - 1, j = col i &gt;= 0 &amp;&amp; j &lt; n; i--, + 1; j++) {     if (board[i][j] == 'Q')         return false; } // 检查左上方是否有皇后互相冲突 for (int i = row - 1, j = col - 1; i &gt;= 0 &amp;&amp; j &gt;= 0; i--, j--) {         if (board[i][j] == 'Q')             return false; }     return true; }   Suduko   有的时候，我们并不想得到所有合法的答案，只想要一个答案，怎么办呢? 比如解数独的算法，找所有解法复杂度太高，只要找到一种解法就可以。 其实特别简单，只要稍微修改一下回溯算法的代码即可:   Backtrack summary  写 backtrack 函数时，需要维护走过的「路径」和当前可以做的「选择列 表」，当触发「结束条件」时，将「路径」记入结果集。 其实想想看，回溯算法和动态规划是不是有点像呢?我们在动态规划系列文 章中多次强调，动态规划的三个需要明确的点就是「状态」「选择」和 「base case」，是不是就对应着走过的「路径」，当前的「选择列表」和 「结束条件」?   Binary Search  分析二分查找的一个技巧是:不要出现 else，而是把所有情况用 else if 写清 楚，这样可以清楚地展现所有细节。本文都会使用 else if，旨在讲清楚，读 者理解后可自行简化。   寻找左侧边界的二分搜索  以下是最常⻅的代码形式，其中的标记是需要注意的细节:  int left_bound(int[] nums, int target) {   if (nums.length == 0) return -1;   int left = 0;   int right = nums.length; // 注意   while (left &lt; right) { // 注意     int mid = (left + right) / 2;     if (nums[mid] == target) {             right = mid;     } else if (nums[mid] &lt; target) {             left = mid + 1;     } else if (nums[mid] &gt; target) {       right = mid; // 注意 }     }   }     return left; }   Left left_bound  int left_bound(int[] nums, int target) { int left = 0, right = nums.length - 1; // 搜索区间为 [left, right] while (left &lt;= right) { int mid = left + (right - left) / 2; if (nums[mid] &lt; target) { // 搜索区间变为 [mid+1, right]             left = mid + 1;         } else if (nums[mid] &gt; target) { // 搜索区间变为 [left, mid-1] right = mid - 1; } else if (nums[mid] == target) { // 收缩右侧边界 right = mid - 1; } } // 检查出界情况 if (left &gt;= nums.length || nums[left] != target) return -1;     return left; }   这样就和第一种二分搜索算法统一了，都是两端都闭的「搜索区间」，而且 最后返回的也是 left 变量的值。只要把住二分搜索的逻辑，两种形式大 家看自己喜欢哪种记哪种吧。   Right bound   寻找右侧边界的二分查找 类似寻找左侧边界的算法，这里也会提供两种写法，还是先写常⻅的左闭右 开的写法，只有两处和搜索左侧边界不同，已标注:  int right_bound(int[] nums, int target) { if (nums.length == 0) return -1; int left = 0, right = nums.length;     while (left &lt; right) {         int mid = (left + right) / 2;         if (nums[mid] == target) { left = mid + 1; // 注意 } else if (nums[mid] &lt; target) {             left = mid + 1;         } else if (nums[mid] &gt; target) { right = mid; } } return left - 1; // 注意 }   为什么这个算法能够找到右侧边界? 答:类似地，关键点还是这里:   if (nums[mid] == target) {     left = mid + 1;     当 nums[mid] == target 时，不要立即返回，而是增大「搜索区间」的下界 left ，使得区间不断向右收缩，达到锁定右侧边界的目的。   是否也可以把这个算法的「搜索区间」也统一成两端都闭的形式呢?这 样这三个写法就完全统一了，以后就可以闭着眼睛写出来了。 答:当然可以，类似搜索左侧边界的统一写法，其实只要改两个地方就行 了:   int right_bound(int[] nums, int target) { int left = 0, right = nums.length - 1; while (left &lt;= right) {     int mid = left + (right - left) / 2; if (nums[mid] &lt; target) {                 left = mid + 1;             } else if (nums[mid] &gt; target) {     right = mid - 1;     } else if (nums[mid] == target) {     // 这里改成收缩左侧边界即可                 left = mid + 1;             }     }     // 这里改为检查 right 越界的情况，⻅下图     if (right &lt; 0 || nums[right] != target)     return -1; return right;     }   Code tips   map.put(key, map.getOrDefault(key, 0) + 1)   Sliding window concepts   滑动窗口算法的思路是这样: 1、我们在字符串 S 中使用双指针中的左右指针技巧，初始化 left = right = 0 ，把索引左闭右开区间 [left, right) 称为一个「窗口」。 2、我们先不断地增加 right 指针扩大窗口 [left, right) ，直到窗口中 的字符串符合要求(包含了 T 中的所有字符)。 3、此时，我们停止增加 right ，转而不断增加 left 指针缩小窗口 [left, right) ，直到窗口中的字符串不再符合要求(不包含 T 中的所有 字符了)。同时，每次增加 left ，我们都要更新一轮结果。 4、重复第 2 和第 3 步，直到 right 到达字符串 S 的尽头。   这个思路其实也不难，第 2 步相当于在寻找一个「可行解」，然后第 3 步在 优化这个「可行解」，最终找到最优解，也就是最短的覆盖子串。左右指针 轮流前进，窗口大小增增减减，窗口不断向右滑动，这就是「滑动窗口」这 个名字的来历。   KPM  Be advised shadow pointer will only work for one case, that is the pattern with repeated char sets same as position zero   // base case,  state will changed \"0\" -&gt; \"1\" for given char at 0   dp[0][pattern.charAt(0)]=1;   int shaldow=0;   for (int i = 1; i &lt; n; i++) {       // to traver each char       for (int c = 0; c &lt; 256; c++) {           if(pattern.charAt(i)==c){               dp[i][c]=i+1;    //[!!!!111] Not = dp[i][c]+1;, should be i+1           }else{               dp[i][c]=dp[shaldow][c];           }       }       shaldow=dp[shaldow][pattern.charAt(i)];  // ONLY start from \"0\" to check with prefix and save time   }   max profit buy sell stock   V2   Buy and sell stocks without any limitations  public int maxProfit(int[] prices) {     if(prices==null || prices.length&lt;0)         return 0;     int n=prices.length;     int profitNoholding=0,profitHolding=Integer.MIN_VALUE; // initially no transactions, the holding position will only be invalid     for(int i=0;i&lt;n;i++){         int temp = profitNoholding;         profitNoholding=Math.max(profitNoholding, profitHolding+prices[i]);         profitHolding = Math.max(profitHolding, temp-prices[i]);     }      return profitNoholding; }   public int maxProfit_best(int[] prices) {     int total=0;     // the philosophy is : add to total if current price is greater than previous one     for(int i=1;i&lt;prices.length;i++){         if(prices[i]&gt;prices[i-1]){             total+=prices[i]-prices[i-1];         }     }     return total; } }   Next Greater Element   class Solution {     public int[] nextGreaterElements(int[] nums) {         if(nums==null) return nums;         // for next greater , monotonic stack         Stack&lt;Integer&gt; stack = new Stack();         int n=nums.length;         int[] rtn=new int[n];         // looped, so use \"%n\" + 2n for loop processing         for(int i=2*n-1;i&gt;=0;i--){             // to strip off unqualified element, aka. to keep monotonic stak             while(!stack.isEmpty() &amp;&amp; stack.peek()&lt;=nums[i%n]){ //[!!!!] be careful of bug, it should be \"&lt;=\", rather than \"&lt;\", becaust the question is \"greater than\", and it will add current element to stack, so if \"=\", it should be populate out as well                 stack.pop();             }             rtn[i%n]=stack.isEmpty()?-1:stack.peek();             stack.push(nums[i%n]);         }     return rtn;     } }    递归反转整个链表   ListNode reverse(ListNode head) { if (head.next == null)      return head;  ListNode last = reverse(head.next);  head.next.next = head; head.next = null; return last; }   对于递归算法，最重要的就是明确递归函数的定义。具体来说，我们的 reverse 函数定义是这样的: 输入一个节点 head ，将「以 head 为起点」的链表反转，并返回反转之 后的头结点。   ","categories": [],
        "tags": [],
        "url": "/2020/06/29/Awesome-Algorithm-Solutions.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Pigeons in holes principle",
        "excerpt":"# Pigeonhole principle   In mathematics, the pigeonhole principle states that if {isplaystyle n}n items are put into {isplaystyle m}m containers, with {isplaystyle n&gt;m}n&gt;m, then at least one container must contain more than one item.[1] In layman’s terms, if you have more “objects” than you have “holes,” at least one hole must have multiple objects in it. A real-life example could be, “if you have three gloves, then you have at least two right-hand gloves, or at least two left-hand gloves,” because you have 3 objects, but only two categories to put them into (right or left). This seemingly obvious statement, a type of counting argument, can be used to demonstrate possibly unexpected results. For example, if you know that the population of London is greater (by at least two people) than the maximum number of hairs that can be present on a human’s head, then the pigeonhole principle requires that there must be (at least) two people in London who have the same number of hairs on their heads.   Examples  Sock-picking Assume a drawer contains a mixture of black socks and blue socks, each of which can be worn on either foot, and that you are pulling a number of socks from the drawer without looking. What is the minimum number of pulled socks required to guarantee a pair of the same color? Using the pigeonhole principle, to have at least one pair of the same color (m = 2 holes, one per color) using one pigeonhole per color, you need to pull only three socks from the drawer (n = 3 items). Either you have three of one color, or you have two of one color and one of the other.   Hand-shaking If there are n people who can shake hands with one another (where n &gt; 1), the pigeonhole principle shows that there is always a pair of people who will shake hands with the same number of people. In this application of the principle, the ‘hole’ to which a person is assigned is the number of hands shaken by that person. Since each person shakes hands with some number of people from 0 to n − 1, there are n possible holes. On the other hand, either the ‘0’ hole or the ‘n − 1’ hole or both must be empty, for it is impossible (if n &gt; 1) for some person to shake hands with everybody else while some person shakes hands with nobody. This leaves n people to be placed into at most n − 1 non-empty holes, so that the principle applies.   The birthday problem The birthday problem asks, for a set of n randomly chosen people, what is the probability that some pair of them will have the same birthday? By the pigeonhole principle, if there are 367 people in the room, we know that there is at least one pair who share the same birthday, as there are only 366 possible birthdays to choose from (including February 29, if present). The birthday “paradox” refers to the result that even if the group is as small as 23 individuals, the probability that there is a pair of people with the same birthday is still above 50%. While at first glance this may seem surprising, it intuitively makes sense when considering that a comparison will actually be made between every possible pair of people rather than fixing one individual and comparing them solely to the rest of the group.  ","categories": [],
        "tags": [],
        "url": "/2020/07/02/pigeon-hole-principle.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Tips about algorithm resolving from Leetcode",
        "excerpt":"Here are some tips and notes about how to resolve algorithm issues listed in LeetCode  Rotation problem   Rotate array   Here is one sample from Leetcode  189. Rotate Array Given an array, rotate the array to the right by k steps, where k is non-negative.  Follow up:  Try to come up as many solutions as you can, there are at least 3 different ways to solve this problem. Could you do it in-place with O(1) extra space?    Example 1:  Input: nums = [1,2,3,4,5,6,7], k = 3 Output: [5,6,7,1,2,3,4] Explanation: rotate 1 steps to the right: [7,1,2,3,4,5,6] rotate 2 steps to the right: [6,7,1,2,3,4,5] rotate 3 steps to the right: [5,6,7,1,2,3,4]  e.g. if k is bigger then array’s length    Tip 1: To find out the smallest rotate count   class Solution {     public void rotate(int[] nums, int k) {     \t// sample input      \t// [1,2]     \t// 3         k=k%nums.length; //[!!!!!] for 'rotation problem', firstly make sure to get smallest iterate count         if(nums==null || k&gt;nums.length) return ; //[!!! k&gt;nums.length]         int[] ori=Arrays.copyOf(nums,nums.length);         for(int i=0;i&lt;nums.length-k;i++){             nums[i+k]=ori[i];         }         for(int i=0;i&lt;k;i++){             nums[i]=ori[nums.length-k+i];         }     } }   Here is the runtime result      Find next greater element in Rotate array   首先，计算机的内存都是线性的，没有真正意义上的环形数组，但是我们可以模拟出环形数组的效果，一般是通过 \\%% 运算符求模（余数），获得环形特效：   回到 Next Greater Number 的问题，增加了环形属性后，问题的难点在于：这个 Next 的意义不仅仅是当前元素的右边了，有可能出现在当前元素的左边（如上例）。   明确问题，问题就已经解决了一半了。我们可以考虑这样的思路：将原始数组 “翻倍”，就是在后面再接一个原始数组，这样的话，按照之前“比身高”的流程，每个元素不仅可以比较自己右边的元素，而且也可以和左边的元素比较了。   Index Stack vs value Stack  Q: Can anyone please tell me the difference between stack and index stack? I mean which one to use on which problems? A: When you need to use index, use the index. When you only need the value, both works.   So the rule of thumb is, if you only need value, both will work. However, if you need the index, for example, to compute the distance, e.g., how many days before getting a wammer day, or how many days the stock prices get higher, you will need to use the index. In such case, you can store index. Plus, you can store multiple pieces of information, such as a tuple (A[i], i, count)…, just an example.   Binary Search  只要数组有序，就应该想到双指针技巧。   BFS &amp; DFS   BFS 的核心思想应该不难理解的，就是把一些问题抽象成图，从一个点开 始，向四周开始扩散。一般来说，我们写 BFS 算法都是用「队列」这种数 据结构，每次将一个节点周围的所有节点加入队列。 BFS 相对 DFS 的最主要的区别是: BFS 找到的路径一定是最短的，但代价 就是空间复杂度比 DFS 大很多，   Use case for BFS   我们先举例一下 BFS 出现的常⻅场景好吧，问题的本质就 是让你在一幅「图」中找到从起点 start 到终点 target 的最近距离，这 个例子听起来很枯燥，但是 BFS 算法问题其实都是在干这个事儿   这个广义的描述可以有各种变体，比如走迷宫，有的格子是围墙不能走，从起点到终点的最短距离是多少?如果这个迷宫带「传送门」可以瞬间传送呢?   再比如说两个单词，要求你通过某些替换，把其中一个变成另一个，每次只能替换一个字符，最少要替换几次?   BFS template  // 计算从起点 start 到终点 target 的最近距离  int BFS(Node start, Node target) { \tQueue&lt;Node&gt; q; // 核心数据结构 Set&lt;Node&gt; visited; // 避免走回头路 \tq.offer(start); // 将起点加入队列 visited.add(start); \tint step = 0; // 记录扩散的步数 \twhile (q not empty) { \t\tint sz = q.size(); \t\t/* 将当前队列中的所有节点向四周扩散 */  \t\tfor (int i = 0; i &lt; sz; i++) { \t\t\tNode cur = q.poll(); \t\t\t/* 划重点:这里判断是否到达终点 */ \t\t\tif (cur is target) \t\t\t    return step; \t\t\t/* 将 cur 的相邻节点加入队列 */  \t\t\tfor (Node x : cur.adj()) \t\t\t\tif (x not in visited) {  \t\t\t\t\tq.offer(x); \t\t\t\t\tvisited.add(x);  \t\t\t\t} \t\t\t/* 划重点:更新步数在这里 */ \t\t\tstep++;  \t\t} }   怎么套到 BFS 的框架里呢?首先明确一下起点 start 和终点 target 是什 么，怎么判断到达了终点? 显然起点就是 root 根节点，终点就是最靠近根节点的那个「叶子节点」 嘛，叶子节点就是两个子节点都是 null 的节点:     int minDepth(TreeNode root) {   \tif (root == null) return 0;    \tQueue&lt;TreeNode&gt; q = new LinkedList&lt;&gt;();    \tq.offer(root); // root 本身就是一层，depth 初始化为 1 int depth = 1; while (!q.isEmpty()) { int sz = q.size(); /* 将当前队列中的所有节点向四周扩散 */  for (int i = 0; i &lt; sz; i++) { TreeNode cur = q.poll(); /* 判断是否到达终点 */ if (cur.left == null &amp;&amp; cur.right == null) return depth; /* 将 cur 的相邻节点加入队列 */ if (cur.left != null) q.offer(cur.left); if (cur.right != null) q.offer(cur.right); } /* 这里增加步数 */ depth++; }     return depth; }   什么 BFS 可以找到最短距离，DFS 不行吗? 首先，你看 BFS 的逻辑， depth 每增加一次，队列中的所有节点都向前迈一步，这保证了第一次到达终点的时候，走的步数是最少的。   形象点说，DFS 是线，BFS 是面;DFS 是单打独斗，BFS 是集体行动。这个应该比较容易理解吧。   既然 BFS 那么好，为啥 DFS 还要存在? BFS 可以找到最短距离，但是空间复杂度高，而 DFS 的空间复杂度较低。 还是拿刚才我们处理二叉树问题的例子，假设给你的这个二叉树是满二叉 树，节点数为 N ，对于DFS算法来说，空间复杂度无非就是递归堆栈，最 坏情况下顶多就是树的高度，也就是 O(logN) 。 但是你想想 BFS 算法，队列中每次都会储存着二叉树一层的节点，这样的 话最坏情况下空间复杂度应该是树的最底层节点的数量，也就是 N/2 ，用 BigO表示的话也就是 O(N) 。 由此观之，BFS 还是有代价的，一般来说在找最短路径的时候使用 BFS， 其他时候还是 DFS 使用得多一些(主要是递归代码好写)。   Dynamic Porgramming   动态规划的一般流程就是三步:暴力的递归解法 -&gt; 带备忘录的 递归解法 -&gt; 迭代的动态规划解法。 就思考流程来说，就分为一下几步:找到状态和选择 -&gt; 明确 dp 数组/函数 的定义 -&gt; 寻找状态之间的关系。   Backpack  遇到背包问题可以说是手到擒来好吧。 无非就是状态 + 选择，也没啥特别之处嘛。   Super Egg Drop  思路分析  对动态规划问题，直接套我们以前多次强调的框架即可:这个问题有什么「状态」，有什么「选择」，然后穷举。   「状态」很明显，就是当前拥有的鸡蛋数 K 和需要测试的楼层数 N 。随 着测试的进行，鸡蛋个数可能减少，楼层的搜索范围会减小，这就是状态的 变化。 「选择」其实就是去选择哪层楼扔鸡蛋。回顾刚才的线性扫描和二分思路， 二分查找每次选择到楼层区间的中间去扔鸡蛋，而线性扫描选择一层层向上 测试。不同的选择会造成状态的转移。 现在明确了「状态」和「选择」，动态规划的基本思路就形成了:肯定是个 二维的 dp 数组或者带有两个状态参数的 dp 函数来表示状态转移;外加 一个 for 循环来遍历所有选择，择最优的选择更新状态:   我们选择在第 i 层楼扔了鸡蛋之后，可能出现两种情况:鸡蛋碎了，鸡蛋 没碎。注意，这时候状态转移就来了: 如果鸡蛋碎了，那么鸡蛋的个数 K 应该减一，搜索的楼层区间应该从 [1..N] 变为 [1..i-1] 共 i-1 层楼; 如果鸡蛋没碎，那么鸡蛋的个数 K 不变，搜索的楼层区间应该从 [1..N] 变为 [i+1..N] 共 N-i 层楼。   Reference     https://labuladong.gitbook.io/algo/  ","categories": [],
        "tags": [],
        "url": "/2020/07/04/Leetcode-tips.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Awesome Shortcuts",
        "excerpt":"Shortcuts &amp; tips   Intellij Notes      CMD+3: Open actions (Eclipse)   CMD+F12 (Mac) or Option+F12: Show terminal   Shift+ESC: close auxiliary windows, such as project explorer, terminal window, etc.   Cmd+O: File structure, list all methods   Shift+Cmd+F: reformat code   Sublime      Ctrl+-:  Go to last edit:   Notion Notes   Cmd+: show left bar   Cmd+P: global search   Mac tips      Cmd + ← or → : move cursor to begin or end of line   Shift + Cmd + →: select whole line (move cursor to line front first)   Option + Shift + → / ←: select whole word   ^ + Down: to show notiifcation center   Ctrl+F2: to access menu bar   Linux comands      List only folders:   ls -dl conflu*   sudo lsof -i tcp:3000   lsof is a command line utility for all Unix and Linux like operating systems to check “list of open files”      Redirect   using &gt; /dev/null 2&gt;&amp;1 will redirect all your command output (both stdout and stderr) to /dev/null, meaning no outputs are printed to the terminal.   By default:   stdin  ==&gt; fd 0 stdout ==&gt; fd 1 stderr ==&gt; fd 2    In the script, you use &gt; /dev/null causing:   stdin  ==&gt; fd 0 stdout ==&gt; /dev/null stderr ==&gt; fd 2    And then 2&gt;&amp;1 causing:   stdin  ==&gt; fd 0 stdout ==&gt; /dev/null stderr ==&gt; stdout   Docker   commands      docker network ls   list all docker networkds      docker rmi:   remove and un-tags one or more images from the host node   Junit   No tests found for given includes: [   import org.junit.Test rather than import org.junit.jupiter.api.Test   Kotlin   Errors   lateinit property sut has not been initialized kotlin.UninitializedPropertyAccessException: lateinit property sut has not been initialized  ","categories": [],
        "tags": [],
        "url": "/2020/07/07/Awesome-shortcuts.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Micrometer notes",
        "excerpt":"As a general rule it should be possible to use the name as a pivot. Dimensions allow a particular named metric to be sliced to drill down and reason about the data.  ","categories": [],
        "tags": [],
        "url": "/2020/07/07/micrometer-notes.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "AOP",
        "excerpt":"The concept of join points as matched by pointcut expressions is central to AOP, and Spring uses the AspectJ pointcut expression language by default.  ","categories": [],
        "tags": [],
        "url": "/2020/07/24/AOP.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Curl",
        "excerpt":"Linux Curl command   arguments   -L, –location   (HTTP) If the server reports that the requested page has moved to a different location (indicated with a Location: header and a 3XX response code), this option will make curl redo the request on the new place. If used together with -i, –include or -I, –head, headers from all requested pages will be shown. When authentication is used, curl only sends its credentials to the initial host. If a redirect takes curl to a different host, it won’t be able to intercept the user+password. See also –location-trusted on how to change this. You can limit the amount of redirects to follow by using the –max-redirs option.   references     https://curl.haxx.se/docs/manpage.html  ","categories": [],
        "tags": ["Linux"],
        "url": "/2020/07/27/Curl.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "How to decode path parameters in All REST WebServices calls",
        "excerpt":"How to decode path parameters in All REST WebServices calls      TL:DR If there are multiple places requires decode a parameter during WebServices RESTful calls, you can use UrlPathHelper from springframework to do this in central place    Image there is one param1 in RESTful URL which may contains some Non URL friendly characters, such as forward slash “/”. You definitely can call URLCodec().decode(param1) on every calls. But that’s cumbersome and error-prone.   Thanks to springframework’s support of URL intercepor, you can extend UrlPathHelper and overwrite decodePathVariables to do this in one central place.   Below is one kotlin and java samples just FYI:   Kotlin   import org.apache.commons.codec.net.URLCodec import org.springframework.web.util.UrlPathHelper import javax.servlet.http.HttpServletRequest  class MyPathHelper : UrlPathHelper() {     override fun decodePathVariables(request: HttpServletRequest, vars: MutableMap&lt;String, String&gt;): MutableMap&lt;String, String&gt; {         val map = super.decodePathVariables(request, vars)         map.computeIfPresent(\"param1\") { _, v -&gt; URLCodec().decode(v) }         return map     } }  Java   import org.apache.commons.codec.DecoderException; import org.apache.commons.codec.net.URLCodec; import org.springframework.web.util.UrlPathHelper;  import java.util.Map;  public class  MyPathHelper extends UrlPathHelper {     @Override     public Map&lt;String, String&gt; decodePathVariables(javax.servlet.http.HttpServletRequest request, Map&lt;String, String&gt; vars) {         Map&lt;String, String&gt; map = super.decodePathVariables(request, vars);         map.computeIfPresent(\"param1\", (k,v) -&gt; {             try {                 return new URLCodec().decode(v);             } catch (DecoderException e) {                 e.printStackTrace();             }             return v;         });         return map;     } }   Hope this help!   –End–  ","categories": [],
        "tags": ["RESTful","WebServices"],
        "url": "/2020/08/03/How-to-decode-path-parameters-in-All-REST-calls.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Docker",
        "excerpt":"Dockers Concepts   Official Definition      Docker is an open platform for developers and sysadmins to build,ship and run distributed applications       Docker’s philosophy is “build-&gt;ship-&gt;run”.    In contemporary IT industry, there are two major usage of Docker.      Focus on Build &amp; Ship, to leverage Docker to setup a platform of “CI/CD”, for develop, test enviornment.   Make use of Docker as light weight VM (virtual machine), focus on Run, apply it in large scale production environment.   ACL   the access control in docker is rely on iptables, the firewall software shipped in almost all Linux release.   Sample Commands  List all images  docker images   sudo docker version   remove all exited containers  docker rm $(docker ps -a -f status=exited -q)   Run  docker run IMAGE_NAME [COMMAND] # run a command in new container docker run -t -i f2d8ce9fa988 /bin/bash # run bash in console and interactive mode  View docker details, e.g. start up script, working dir  docker inspect containerid   Start bash to view files inside docker  docker -t -i imageFile /bin/bash   Map hosts between host and contains  docker -P xxx docker ps -l  You’ll see  0.0.0.0:32768-&gt;5000/tcp     It means host port 32768 map to port 5000 in contains   To list docker containers including histories  docker ps -a # show all containers instead of only running as default docker ps -l # show latest created container   removes containers once return from one run  docker run --rm -name myApp1  -link  db:db training/webapp env   to ping other containers  Be advised there is no built-in ping for containers, therefore it’s requried manually install one. As following sample:  apt-get install -yqq inetutils-ping   Links     GitBook Docker —— 从入门到实践   知乎 docker   Yelp Docker -中文版   Yelp Docker -英文版   Amazon Docker books  ","categories": [],
        "tags": ["Docker","Container"],
        "url": "/2020/08/13/Docker.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Mock in kotlin",
        "excerpt":"Argument Matching &amp; Answers  For example, you have mocked DOC with call(arg: Int): Intfunction. You want to return 1 if argument is greater than 5 and -1 if it is less or equal to 5. This can be achieved by following construct: every { mock.call(more(5)) } returns 1 every { mock.call(or(less(5), eq(5))) } returns -1   returnsMany specify a number of values that are used one by one i.e. first matched call returns first element, second — second element, e.t.c. every { mock1.call(5) } returnsMany listOf(1, 2, 3) You can achieve the same using andThen construct: every { mock1.call(5) } returns 1 andThen 2 andThen 3   Capture   CapturingSlot allows to capture only one value, so it is simpler to use.   val slot = slot&lt;Int&gt;() val mock = mockk&lt;Divider&gt;() every { mock.divide(capture(slot), any()) } returns 22   you can use a slot in an answer lambda: every {    mock.divide(capture(slot), any())  } answers {   slot.captured * 11 } So mock.divide(5, 2) returns 55.   Capture list  That is basically it. Working with MutableList is the same, just instead of a slot in capture function MutableList should be used. val list = mutableList() val mock = mockk() every { mock.divide(capture(list), any()) } returns 22   Relaxed mocks   to skip specifying expected behavior and replies with some basic value alike null or 0. You can achieve in MockK by declaring relaxed mock.   val mock = mockk(relaxed = true)   Then you can use it right away:   mock.divide(5, 2) // returns 0   Spies  Spies give the possibility to set expected behavior and do behavior verification while still executing original methods of an object.   class Adder {     fun magnify(a: Int) = a   fun add(a: Int, b: Int) = a + magnify(b) } We want to test the behavior of add function while mocking behavior of magnify function. Let’s first create a spy: val spy = spyk(Adder())   Here we create object Adder() and build a spy on top of it. Building a spy actually means creating a special empty object of the same type and copying all the fields. Now we can use it, as if it was regular Adder() object. assertEquals(9, spy.add(4, 5)) This checks that original method is called. Besides that, we can define spy behavior by putting it to every block: every { spy.magnify(any()) } answers { firstArg() * 2 }   After that, behavior of add has changed because it was dependent on magnify: assertEquals(14, spy.add(4, 5))   Annotations  The library supports annotations @MockK, @SpyK and @RelxedMockK, which can be used as a simpler way to create mocks, spies, and relaxed mocks correspondingly.  class Test {     @MockK     lateinit var doc1: Dependency1      @RelaxedMockK     lateinit var doc2: Dependency2      @SpyK     val doc3 = Dependency3()      @Before     fun setUp() = MockKAnnotations.init(this)      @Test     fun calculateAddsValues1() {         every { doc1.call().add(any()) } returns 5         every { doc2.value2 } returns \"6\"         every { doc3.sub(any()) } returns 7          val sut = SystemUnderTest(doc1)          assertEquals(11, sut.calculate())     } }  The important part here is MockKAnnotations.init(this) call which is executed at @Before phase. When it is executed all annotated properties are substituted with corresponding objects: mocks, spies and relaxed mocks.   Mockito for Java   Mockito provides several methods to create mock objects:   Using the static mock() method.   Using the @Mock annotation.   If you use the @Mock annotation, you must trigger the initialization of the annotated fields. The MockitoRule does this by calling the static method MockitoAnnotations.initMocks(this).   Hope this help!   –End–  ","categories": [],
        "tags": ["Kotlin","Mock"],
        "url": "/2020/08/13/Mock-In-Kotlin.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Swift for iOS",
        "excerpt":"#   This book is intended to accompany and precede Programming iOS 13, which picks up where this book leaves off. If writing an iOS program is like building a house of bricks, this book teaches you what a brick is and how to handle it, while Program‐ ming iOS 13 shows you some actual bricks and tells you how to assemble them.   –End–  ","categories": [],
        "tags": ["Swift","Mac","iOS"],
        "url": "/2020/08/16/Swift-For-iOS.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Awesome Swift for iOS",
        "excerpt":"Frame in Swift   Forgetting to assign a view a frame when creating it in code, and then wondering why it isn’t appearing when added to a superview, is a common beginner mistake. If a view has a standard size that you want it to adopt, especially in relation to its contents (like a UIButton in relation to its title), an alternative is to call its sizeToFit method.   Matt Neuburg. Programming iOS 13 (Kindle Locations 555-557). O’Reilly Media. Kindle Edition.   Configu a view  Initializers, modifiers and inheritance Overall, there are three different ways to configure a SwiftUI view — by passing arguments to its initializer, using modifiers, and through its surrounding environment.   SF Symbols  SF Symbols provides a set of over 2,400 consistent, highly configurable symbols you can use in your app. Apple designed SF Symbols to integrate seamlessly with the San Francisco system font, so the symbols automatically ensure optical vertical alignment with text in all weights and sizes.   You can use SF symbols to represent tasks and types of content in a variety of UI elements, such as navigation bars, toolbars, tab bars, context menus, and widgets. T   To show Avatar  // avatar AvatarView(image: post.user.avatar, size: 70)   Tricks   Create a red calendar  Since SwiftUI views are responsible for determining their own size, we need to tell our image to resize itself to occupy all available space, rather than sticking to its default size. To make that happen, we simply have to apply the .resizable() modifier to it           Image(systemName: \"calendar\")         .resizable()             .frame(width: 50, height: 50)             .background(Color.red)             .padding()  Again, the result of the above might not be what we were expecting, as we’ve essentially given our calendar icon outer padding — additional whitespace that doesn’t include its background color. If we think about it, this is the exact same behavior as we encountered before when applying our .frame() modifier — calling .padding() doesn’t actually mutate our earlier views and modifiers, it simply adds whitespace around the result of the preceding expressions.   A rounded calendar view   struct CalendarView: View {     var body: some View {         Image(systemName: \"calendar\")             .resizable()             .frame(width: 50, height: 50)             .padding()             .background(Color.red)             .cornerRadius(10)             .foregroundColor(.white)     } }   Add a badge   extension View {     func addVerifiedBadge(_ isVerified: Bool) -&gt; some View {         ZStack(alignment: .topTrailing) {             self              if isVerified {                 Image(systemName: \"checkmark.circle.fill\")                     .offset(x: 3, y: -3)             }         }     } }   struct CalendarView: View {     var eventIsVerified = true      var body: some View {         Image(systemName: \"calendar\")             .resizable()             .frame(width: 50, height: 50)             .padding()             .background(Color.red)             .cornerRadius(10)             .foregroundColor(.white)             .addVerifiedBadge(eventIsVerified)     } }   View modifiers often wrap the current view within yet another view, which is why we can get completely different layout results depending on which order that we call our modifiers in.   Split multiple text block evenly  To make that happen, let’s give the Text within our EventInfoBadge an infinite max width — which will make the layout system scale it as much as possible on the horizontal axis before splitting it up into multiple lines:  struct EventInfoBadge: View {     var iconName: String     var text: String      var body: some View {         VStack {             Image(systemName: iconName)                 .resizable()                 .aspectRatio(contentMode: .fit)                 .frame(width: 25, height: 25)             Text(text)                 .frame(maxWidth: .infinity)                 .multilineTextAlignment(.center)         }         .padding(.vertical, 10)         .padding(.horizontal, 5)         .background(Color.secondary)         .cornerRadius(10)     } }  ","categories": [],
        "tags": ["Swift","iOS","Mac"],
        "url": "/2020/08/20/Awesome-swift.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Awesome Reactive programming",
        "excerpt":"Key points of Reactive Programming      Reactive is used to broadly define event-driven systems   Move imperative to            Asynchronous       non-blocking       functaion-style code           Allows stable,scalable access to external sytems  ","categories": [],
        "tags": ["Reactive","Java","Microservice"],
        "url": "/2020/08/21/Reactor-Reactive.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Gradle build stuck",
        "excerpt":"Gradle build stuck, keep on running but never ending   Here are some key error in console logs      Waiting to acquire shared lock on daemon addresses registry   Daemon stuck   Gradle build stuck at “Waiting to acquire shared lock on daemon addresses registry.”   Here are sample logs from Gradle console    22:18:32.549 [DEBUG] [org.gradle.cache.internal.DefaultFileLockManager] Waiting to acquire shared lock on daemon addresses registry. 22:18:32.549 [DEBUG] [org.gradle.cache.internal.DefaultFileLockManager] Lock acquired on daemon addresses registry. 22:18:32.549 [DEBUG] [org.gradle.cache.internal.DefaultFileLockManager] Releasing lock on daemon addresses registry. 22:18:32.550 [DEBUG] [org.gradle.cache.internal.DefaultFileLockManager] Waiting to acquire shared lock on daemon addresses registry. 22:18:32.550 [DEBUG] [org.gradle.cache.internal.DefaultFileLockManager] Lock acquired on daemon addresses registry. 22:18:32.550 [DEBUG] [org.gradle.cache.internal.DefaultFileLockManager] Releasing lock on daemon addresses registry. 22:18:33.386 [DEBUG] [org.gradle.process.internal.health.memory.MemoryManager] Emitting OS memory status event {Total: 34359738368, Free: 14353453056} 22:18:33.386 [DEBUG] [org.gradle.launcher.daemon.server.health.LowMemoryDaemonExpirationStrategy] Received memory status update: {Total: 34359738368, Free: 14353453056} 22:18:33.386 [DEBUG] [org.gradle.process.internal.health.memory.MemoryManager] Emitting JVM memory status event {Maximum: 1908932608, Committed: 1227358208}  Investigation   List process  To see what happending behind the scene, firstly run jps and get process IDs for Gradle , e.g. output as   ~|⇒  jps 8304  72803 KotlinCompileDaemon 72501 GradleDaemon 73700 GradleDaemon 74823 GradleWrapperMain 75366 Jps 74603 KotlinCompileDaemon 636  71455 GradleDaemon 74846 ApplicationKt   Show details of Gradle Daemon   To know more about stack trace for Gralde, use following command  jstack 72501  There are lots of waiting process, which indicate they are waiting for further signal to go ahead.   Solution  Then it turn out there is gradle argument debug in grade command list beow:   ./gradlew bootRun --debug-jvm --stacktrace   So you should open debug to attach the process. i.e. go to Run and click Attach to process and select your assigned point, then Gradle process would resume and continue to run.   –end–  ","categories": [],
        "tags": ["Gradle","Java"],
        "url": "/2020/09/03/Neverl-ending-Gralde-run.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Awesome tips and shortcuts for Slack",
        "excerpt":"Shortcuts for Slack      Cmd + T : Jump to  (channels, people,files)   Cmd + G : Search globally in Slack   –end–  ","categories": [],
        "tags": ["Slack","Shortcuts"],
        "url": "/2020/09/07/Awesome-shortcuts-Slack.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "JVM热身",
        "excerpt":"   此文是作者英文原文的翻译文章，英文原文在：http://todzhang.com/posts/2018-06-10-jvm-warm-up/    JVM预热  “低延迟”的Java虚拟机应用在一些应用场景下是对系统至关重要对一个指标。比如说在外汇交易系统等金融业核心系统中。此文就是探讨一下如何运用Java虚拟机预热来提高系统效率。   优化几调整JVM中每个进程过程称为预热 请记住，对于低延迟的应用程序，我们需要预先缓存所有类-以便在运行时访问它们时立即可用。   逃逸分析   逃逸分析是Java Hotspot Server编译器可以用来分析新对象使用范围并决定是否在Java堆上分配它的一种技术。 根据逃逸分析，对象的转义状态可能是以下之一：          GlobalEscape –对象转义方法和线程。例如，对象存储在静态字段中，或者存储在转义对象的字段中，或者作为当前方法的结果返回。            ArgEscape –作为参数传递或由参数引用但在调用过程中不会全局转义的对象。通过分析被调用方法的字节码确定此状态。            NoEscape –标量可替换对象，意味着可以从生成的代码中删除其分配。       经过逃逸分析后，服务器编译器从生成的代码中消除了标量可替换对象分配和关联的锁。服务器编译器还消除了对所有非全局转义对象的锁定。它不会用非全局转义对象的堆栈分配替换堆分配。   JIT积极地内联方法，从而消除了方法调用的开销。可以内联的方法包括静态，私有或最终方法，但如果可以确定它们没有被覆盖，则还可以包括公共方法。因此，后续的类加载可能会使先前生成的代码无效。因为在每个地方对每个方法进行内联都会花费时间，并且会生成不合理的大二进制文件，所以JIT编译器首先对热方法进行内联，直到达到阈值为止。为了确定哪些方法是热门的，JVM会保留计数器以查看方法被调用了多少次以及已执行了多少次循环迭代。这意味着内联仅在达到稳定状态后才发生，因此您需要重复进行一定次数的操作，然后才能有足够的概要分析信息供JIT编译器执行其工作。   您可以尝试打开Java命令行标志来窥视正在发生的事情，而不必试图猜测JIT在做什么：-XX：+ PrintCompilation -XX：+ UnlockDiagnosticVMOptions -XX：+ PrintInlining 这是他们的工作：     -XX：+ PrintCompilation：JIT编译发生时记录    -XX：+ UnlockDiagnosticVMOptions：启用其他标志，例如-XX：+ PrintInlining   GlobalEscape和ArgEscape对象必须在堆上分配，但是对于ArgEscape对象，可以删除一些锁定和内存同步开销，因为这些对象仅在调用线程中可见。   NoEscape对象可以自由分配，例如在堆栈上而不是堆上。实际上，在某些情况下，甚至根本不需要构造一个对象，而是仅构造该对象的标量值，例如对象Integer的int。同步也可能会被删除，因为我们知道只有这个线程会使用对象。例如，如果我们使用有些古老的StringBuffer（与StringBuilder相反，它具有同步方法），则可以安全地删除这些同步。   EA当前仅在C2 HotSpot编译器下可用，因此我们必须确保以-server模式运行。   为什么重要  从理论上讲，可以使用EA将NoEscape对象对象分配在堆栈上，甚至可以分配给CPU寄存器，从而可以非常快速地执行。   当我们在堆上分配对象时，我们开始消耗CPU缓存，因为对象被放置在堆上的不同地址上，彼此之间可能相距很远。这样，我们将快速耗尽L1 CPU缓存，并且性能会下降。另一方面，通过EA和堆栈分配，我们正在使用（很可能）已经在L1高速缓存中的内存。因此，EA和堆栈分配将改善我们的数据本地化。从性能的角度来看，这是很好的。   显然，当我们使用带有堆栈分配的EA时，垃圾收集的运行频率要低得多。这也许是最大的性能优势。回想一下，每次JVM运行一次完整的堆扫描时，我们都会从我们的CPU中获取性能，并且CPU高速缓存将很快耗尽。更不用说我们是否在服务器上调出了虚拟内存，从而导致GC破坏了性能。   EA的最重要优势不是性能。 EA允许我们使用Lambda，函数，流，迭代器等局部抽象，而不会造成任何明显的性能损失，从而使我们可以编写更好，更易读的代码。描述我们在做什么而不是如何完成的代码。   GC清理堆，而不是栈。当方法返回其调用方时，堆栈会自动清理，从而将堆栈指针重置为其以前的值。因此，GC将在执行EA / C2编译之前清理最终在堆栈中的对象。实际实例（或更确切地说，它们的相应表示形式）位于堆栈上，在EA优化的上下文中，堆栈上没有引用的对象。   JIT优化   一些JIT编译技术  内联是Java HotSpot VM使用的最常见的JIT编译技术之一，它是将方法的主体替换为调用该方法的位置的实践。内联节省了调用方法的成本；无需创建新的堆栈框架。默认情况下，Java HotSpot VM将尝试内联包含少于35个字节JVM字节码的方法。   Java HotSpot VM进行的另一种常见优化是单态调度，它依赖于观察到的事实，即通常情况下，没有一种方法会导致对象引用在大多数情况下属于一种类型，而在另一种类型中却存在另一种类型的路径。次。   您可能会认为Java的静态类型会排除通过不同代码路径具有不同类型的情况，但是请记住，子类型的实例始终是超类型的有效实例（在Barbara Liskov之后，该原理称为Liskov替换原理）。这种情况意味着方法可能有两条路径（例如，一条路径传递一个超类型的实例，而一条路径传递一个子类型的实例），这在Java的静态类型定义中是合法的（并且确实会在实践）。   但是，在通常情况下（单态情况），不会发生具有不同的依赖于路径的类型。因此，我们知道在传递的对象上调用方法时将调用的确切方法定义，因为我们无需检查实际上使用了哪个替代。这意味着我们可以消除执行虚拟方法查找的开销，因此JIT编译器可以发出优化的机器代码，通常比等效的C ++调用要快（因为在C ++情况下，无法轻松消除虚拟查找）。   两种Java HotSpot VM编译器模式使用不同的JIT编译技术，并且对于相同的Java方法，它们可以输出非常不同的机器代码。但是，现代Java应用程序通常可以同时使用两种编译模式。   Java HotSpot VM使用许多其他技术来优化JIT编译生成的代码。循环优化，类型锐化，死代码消除和内在函数只是Java HotSpot VM尝试尽可能多地优化代码的其他方式。技术通常是一层一层地放在另一层之上，因此一旦应用了一种优化，编译器便可能能够看到更多可以执行的优化。   编译模式   在Java HotSpot VM内部，实际上有两个单独的JIT编译器模式，分别称为C1和C2。 C1用于需要快速启动和坚如磐石的优化的应用； GUI应用程序通常是此编译器的理想选择。另一方面，C2最初是用于长时间运行的（主要是服务器端）应用程序。在某些Java SE 7更高版本之前，分别使用-client和-server开关可以使用这两种模式。   两种编译器模式使用不同的技术进行JIT编译，并且对于相同的Java方法，它们可以输出非常不同的机器代码。但是，现代Java应用程序通常可以同时使用两种编译模式。为了利用这一事实，从某些Java SE 7更高版本开始，提供了称为分层编译的新功能。此功能在开始时使用C1编译器模式以提供更好的启动性能。一旦对应用程序进行了适当的预热，C2编译器模式将接管其工作，以提供更具攻击性的优化，并且通常提供更好的性能。随着Java SE 8的到来，分层编译现已成为默认行为。   Java内存监视工具   pemi$ jps | grep Main 50903 Main pemi$ jmap -histo 50903 | head  num     #instances         #bytes  class name  ----------------------------------------------    1:            95       42952184  [I    2:          1079         101120  [C    3:           485          55272  java.lang.Class    4:           526          25936  [Ljava.lang.Object;    5:            13          25664  [B    6:          1057          25368  java.lang.String    7:            74           5328  java.lang.reflect.Field   jmap-内存映射   工具或选项的说明和用法 Java任务控制   Java Mission Control（JMC）  是用于HotSpot JVM的新JDK分析和诊断工具平台。它是一个具有高性能的工具套件，用于基本的监视，管理和生产时间分析以及诊断。 Java Mission Control最大限度地减少了性能分析开销，而性能开销通常是性能分析工具遇到的问题。请参阅Java Mission Control。  jcmd实用程序   jcmd实用程序用于将诊断命令请求发送到JVM，这些请求对于控制Java Flight Records很有用。 JFR用于通过飞行记录事件对JVM和Java应用程序进行故障排除和诊断。请参见jcmd实用程序。  Java VisualVM   该实用程序提供了一个可视界面，用于在Java应用程序在Java虚拟机上运行时查看有关Java应用程序的详细信息。此信息可用于对本地和远程应用程序进行故障排除，以及对本地应用程序进行性能分析。请参阅Java VisualVM。 JConsole实用程序   该实用程序是基于Java管理扩展（JMX）的监视工具。该工具使用Java虚拟机中的内置JMX工具来提供有关正在运行的应用程序的性能和资源消耗的信息。请参见JConsole。 jmap实用程序   该实用程序可以从Java进程，核心文件或远程调试服务器获取内存映射信息，包括堆直方图。请参阅jmap实用程序。 jps实用程序   该实用程序列出了目标系统上已检测到的Java HotSpot VM。该实用程序在嵌入式VM的环境中非常有用，也就是说，它是使用JNI Invocation API而不是Java启动器启动的。请参阅jps实用程序。 jstack实用程序   该实用程序可以从Java进程获取Java和本机堆栈信息。在Oracle Solaris和Linux操作系统上，该实用程序可以从核心文件或远程调试服务器中获取信息。请参阅jstack实用程序。 jstat实用程序   该实用程序使用Java中的内置工具来提供有关正在运行的应用程序的性能和资源消耗的信息。诊断性能问题（尤其是与堆大小和垃圾回收相关的问题）时可以使用该工具。请参见jstat实用程序。 jstatd守护程序   该工具是一个远程方法调用（RMI）服务器应用程序，它监视已检测Java虚拟机的创建和终止，并提供一个接口，以允许远程监视工具连接到在本地主机上运行的VM。请参见jstatd守护程序。 visualgc实用程序   该实用程序提供了垃圾收集系统的图形视图。与jstat一样，它使用Java HotSpot VM的内置工具。请参阅visualgc工具。 本机工具   $ jps 16217 MyApplication 16342 jps  The utility lists the virtual machines for which the user has access rights. This is determined by access-control mechanisms specific to the operating system. On Oracle Solaris operating system, for example, if a non-root user executes the jps utility, then the output is a list of the virtual machines that were started with that user's uid.  In addition to listing the PID, the utility provides options to output the arguments passed to the application's main method, the complete list of VM arguments, and the full package name of the application's main class. The jps utility can also list processes on a remote system if the remote system is running the jstatd daemon.    无GC的Java  没有GC的Java开发 Coral Blocks开发的所有产品都具有非常重要的功能，可以将零垃圾抛在后面。由于Java垃圾收集器（即GC）施加的延迟对于高性能系统是不可接受的，并且由于无法关闭GC，因此Java实时系统的最佳选择是根本不产生任何垃圾。想象一下，GC永远不会启动。想象一下一个高性能匹配引擎，其运行时间为微秒级，每秒发送和接收数十万条消息。如果GC在任何给定时间决定以1毫秒以上的延迟开始运行，则系统中的中断将是巨大的。因此，如果要使用Java开发具有最小方差和延迟的实时系统，最好的选择是正确执行此操作，而不会为GC创建任何垃圾。   热身，检查GC和采样  确保您的系统不产生任何垃圾的关键是从开始到完成数百万次预热您的关键路径，然后再检查数百万次内存分配。如果它随着迭代次数的增加而线性地分配内存，则很可能会产生垃圾，您应该使用堆栈跟踪   –end–  ","categories": ["chn"],
        "tags": ["JVM预热","Java","低延迟"],
        "url": "/chn/2020/09/15/JVM-Warm-up-CHN.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Awesome tips for Chrome",
        "excerpt":"Shortcuts &amp; tips   To quickly search and locate one opened tab  To install extension:  https://chrome.google.com/webstore/detail/quick-tabs/jnjfeinjfmenlddahdjdmgpbokiacbbb/related   Then you can run command Cmd+E to open a quick window allow your enterring keyword to search a tab   No extentions  If you have no access to Chrome extensions (due to some corporation restricts, etc.), you can manuallly run following command in Chrome address bar:   chrome://inspect/#pages  ","categories": [],
        "tags": ["Chrome"],
        "url": "/2020/09/17/Awesome-Chrome-tips.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Awesome Kotlin",
        "excerpt":"Difference with Scala  Kotlin takes the best of Java and Scala, the response times are similar as working with Java natively, which is a considerable advantage over Scala.   Programming tips   What’s differences between isNullOrBlank and isNullOrEmpty           isNummOrEpty: Returns true if this nullable char sequence is either null or empty (Length is zero).            isNullOrBlank: Returns true if this nullable char sequence is either null or empty or consists solely of whitespace characters.      ","categories": [],
        "tags": ["Kotlin","Java","JVM"],
        "url": "/2020/09/18/Awesome-Kotlin.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Debug Stuck IntelliJ",
        "excerpt":"What happened to a debug job hanging in IntelliJ (IDEAS) IDE?  You may find when you try to debug a class in Intellij but it stuck there and never proceed, e.g. the status is Instantiating tests ... and never returns   How to fix  There are numeber or reasonse, here are some commonly ones.   1. Too many breakpoints!  Solution is straightforwrd, firstly disable (or delete) ALL breakpoints. Then add only small number of breakpoints to retry.   To do so, please go to menu run -&gt; View breakponts to Untick ALL breakpoints. Rather than mute all breakpoints in Debugger view.   Alternatively, you can click menu “hellp” -&gt; “Find Actions…” . Then chose “remove all breakpoitns”, As below screenshot.      2. Change Intelij to use JUnit to run test  Another possible reason is how you start a debug test in your IDE.   Different Intellij veions go with different approach to run a test or debug.  For my case is Intellij try to debug a class by gradle runner, which cause debug hang forever. When I try to switch it to JUnit runner, all working fine.   Here are steps to switch to Junit      Go to menu Run -&gt; Edit Configurations, then click “-“ to remove your test configuration   Go to IntelliJ preference, search “Gradle”, then change value in two drop down to “IntelliJ” , rather than Gradle(default)   –End–  ","categories": [],
        "tags": ["IntelliJ","Java"],
        "url": "/2020/09/24/Debug-Stuck-IntelliJ.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "How to process data from S3 download URL",
        "excerpt":"S3 download URL  As you know, AWS S3 object can be downloaded/processed by S3 download URL.  I’m showing you two examples on how to process S3 Object by NIO from Java.   How to use S3 download URL   Below are Java samples to download S3 Object.  Save as a local file  The frist one is save S3 file to a file in local disk     private void readS3FileToFile(String url, Path filePath) throws IOException {         logger.info(\"Going to save file to be downloaded from S3\");         try (ReadableByteChannel srcChannel = Channels.newChannel(new URL(url).openStream());              FileOutputStream outputStream = new FileOutputStream(filePath.toFile());              FileChannel fileChannel = outputStream.getChannel()) {             fileChannel.transferFrom(srcChannel, 0, Integer.MAX_VALUE);             logger.info(\" File downloaded and saved to: \" + filePath.toString());         }     }   Output file content to String       private void readS3FileToString(String url) throws IOException {         logger.info(\"Going to read file content directly from S3\");         try (ReadableByteChannel channel = Channels.newChannel(new URL(url).openStream());              ByteArrayOutputStream outputStream = new ByteArrayOutputStream();              WritableByteChannel writeChannel = Channels.newChannel(outputStream)) {               ByteBuffer buffer = ByteBuffer.allocate(1_024);             boolean readon = true;             while (readon) {                 int len = channel.read(buffer);                 if (len &lt;= 0) {                     readon = false;                 }                 buffer.flip();                 writeChannel.write(buffer);                 buffer.clear();             }             String fileContent = outputStream.toString();             logger.info(\" File content is : \" + fileContent);         }     }  –End–  ","categories": [],
        "tags": ["AWS","S3","NIO"],
        "url": "/2020/10/20/How-to-use-S3-download-URL.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Debts in a nutshell",
        "excerpt":"A debt security represents a debt owed by the issuer to an investor. Here, the investor acts as a lender to the issuer which may be a government, organisation or company. However, unlike other types of debt such as bank loans, debt securities are generally tradable – that is, can be bought or sold between parties in the market prior to maturity.   –End–  ","categories": [],
        "tags": ["Financial","Debts"],
        "url": "/2021/01/04/Debts-In-a-nutshell.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "谷歌地图里面照片的评论显示不出来",
        "excerpt":"简介   我发现我的Oppo 安卓手机的应用“ Google Maps”出现了一个奇怪的问题。那就是当您在Google地图中搜索某个地点（例如“中央公园”）时，理想情况下，此应用应该向您显示该公园的照片和评论列表。例如，某人可能发布了该公园的草坪或河流的照片，并添加了一些评论，例如位置便利，易于停车等。但是，我的Google Apps中没有任何内容。   故障排除和解决方案  为了解决这个问题，我用谷歌搜索了可能的解决方案，这是找到最符合的结果。 https://support.google.com/googleplay/answer/7431675?hl=zh_CN   简而言之，您应该在Google Play中更改“国家/地区”，因为某些国家/地区存在内容限制。因此，您可以将其更改为某些限制最少的国家/地区，例如我更改为“澳大利亚”。   不幸的是，此更改仍然无法正常工作。   终极解决方案   通过进一步深入Google Map设置，并检查日志，不幸的是还是没有解决。到了最后，我通过“卸载并从Google Play重新下载”来解决了这个问题。事实证明，我安装的Google地图来自非官方的Google Play，但来自Opoo应用商店。也就是说这是个阉割版的Goolge Maps.   –End–  ","categories": [],
        "tags": ["Google","Maps"],
        "url": "/2021/01/19/Google-maps-no-photos-reviews-CHN.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Google maps no photos reviews",
        "excerpt":"Summary  I found a weird problem of the app Google Maps of my Oppo Android phone. That’s when you search a place in Google map, say “Central Park”, ideally this app should show you list of photos and reviews for this park. For example, someone may post photo of the lawn or rivers of this park, and add some reviews such as good location and easy to park etc. However, there is nothing in my Google Apps.   Troubleshooting &amp; Solutions  To solve this problem, I googled google for possible solution, here is the findings. https://support.google.com/googleplay/answer/7431675?hl=en   In short, you should change your country in Google Play, because there are content restrictions in certain countries. So you’d change it to some countries with least restrictions, such as I changed to Australia.   Unfortunately this change still not work.   Ultimate solution   By further dig into Google Map settings, and checking logs. It still in vain.  At last I solved this problem by unstall it and re-download from Google play. It turn out to be the Google map I installed is come from unofficial Google play but from Opoo app store.   –End–  ","categories": [],
        "tags": ["Google","Maps"],
        "url": "/2021/01/19/Google-maps-no-photos-reviews.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Whitelabel Error Page",
        "excerpt":"Summary  Whitelabel Error Page is the default error page in Spring Boot web app. It provide a more user-friently error page whenever there are any issues when user trying to access a apge.      Generally this is good to end users but we may watnt to see more error details during develop/debug stage. To show error stack trace, you can add following annotation in front of your SpringApplicaiton main class.   @SpringBootApplication(exclude = {ErrorMvcAutoConfiguration.class}) public class YourWebApplication {   After this change and restart your application, you’d see your familiar error page. :-)      –End–  ","categories": [],
        "tags": ["Financial","Debts"],
        "url": "/2021/09/13/Whitelabel-Error-Page.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "How to get CPU name, core, 64bit and speed in command line",
        "excerpt":"Summary  In windows operation system, if you want to get your CPU name, core, 64bit and speed in command line. Just follow below actions:      Press Win+R and type cmd   Enter following command to get your CPU details.   wmic cpu get caption, deviceid,name, numberofcores,maxclockspeed,status  You’ll get output simliar to below:      Caption                               DeviceID  MaxClockSpeed  Name                                NumberOfCores  Status  Intel64 Family 6 Model 85 Stepping 7  CPU0      3912           Intel(R) Xeon(R) W-2245 CPU @ 3.90GHz  8              OK   Addtionally reading, this wmic is an utility shipped by Windows OS. It’s full name is WMI command-line (WMIC) , one utility provides a command-line interface for Windows Management Instrumentation (WMI).   –End–  ","categories": [],
        "tags": ["Windnows","WMI"],
        "url": "/2021/09/14/Get-CPU-info-core-in-windows.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Deep dive into Kubernetes Client API",
        "excerpt":"Summary  To talk to K8s for getting data, there are few approaches. While K8s’ official Java library is the most widely used one. This blog will look into this client library.   ClientAPI  This class is essentially a wraper of library OKHttp, which will call K8s Restful API to get data back.   To have a new client API, you can either using username/password, CA token or in-cluster client API.   In-clust client API  This is means your appilcatin is runing inside Kubernetes, so that it will read various configrations from running environment. This will save lots time to setup tokens, login details.   Here is one sample FYI. https://github.com/kubernetes-client/java/blob/master/examples/examples-release-12/src/main/java/io/kubernetes/client/examples/InClusterClientExample.java   Firstly, call follwoing command ClientBuilder.cluster   Creates a builder which is pre-configured from the cluster configuration. Following details will be loaded from running envrionemnt :     KUBERNETES_SERVICE_HOST   KUBERNETES_SERVICE_PORT   SERVICEACCOUNT_CA_PATH (/var/run/secrets/kubernetes.io/serviceaccount/ca.crt)   SERVICEACCOUNT_TOKEN_PATH (/var/run/secrets/kubernetes.io/serviceaccount/token)   For accesstoken,   builder.setAuthentication(new TokenFileAuthentication(SERVICEACCOUNT_TOKEN_PATH)); //cluster vs builder.setAuthentication(new AccessTokenAuthentication(token)); // oldCluster   In windows operation system, if you want to get your CPU name, core, 64bit and speed in command line. Just follow below actions:      Press Win+R and type cmd   Enter following command to get your CPU details.   wmic cpu get caption, deviceid,name, numberofcores,maxclockspeed,status  You’ll get output simliar to below:      Caption                               DeviceID  MaxClockSpeed  Name                                NumberOfCores  Status  Intel64 Family 6 Model 85 Stepping 7  CPU0      3912           Intel(R) Xeon(R) W-2245 CPU @ 3.90GHz  8              OK   Addtionally reading, this wmic is an utility shipped by Windows OS. It’s full name is WMI command-line (WMIC) , one utility provides a command-line interface for Windows Management Instrumentation (WMI).   –End–  ","categories": [],
        "tags": ["Kubernetes","K8s"],
        "url": "/2021/09/16/Kubernetes-Client-API-DeepDive.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "2021-09-22-IT-Solutions-For-Remote-Learning",
        "excerpt":"IT-Solutions-For-Remote-Learning.md  ","categories": ["blog_business","business"],
        "tags": [],
        "url": "/blog_business/business/2021/09/22/IT-Solutions-For-Remote-Learning.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "2021-09-22-IT-Solutions-For-Remote-Learning",
        "excerpt":"IT-Solutions-For-Remote-Learning.md  ","categories": ["blog_chn","business"],
        "tags": [],
        "url": "/blog_chn/business/2021/09/22/IT-Solutions-For-Remote-Learning.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Deep dive into ApplicationEvent in SpringBoot",
        "excerpt":"Summary  As you know,  there are various event will be sent (multicast) when a specific story taken place.   This simple blog will walk through the list of key ApplicationEvent when you kick start a Spring Boot applicaiton. So that you’ll get an insight into how SpringBoot application bootstraped and started to serving requests.   Key application events   Holistic diagram view      ApplicationStartingEvent      Event published as early as conceivably possible as soon as a SpringApplication has been started - before the Environment or ApplicationContext is available, but after the ApplicationListeners have been registered.    This is called in SpringApplication.java ‘s run method.   listeners.starting(bootstrapContext, this.mainApplicationClass);   ApplicationEnvironmentPreparedEvent     Event published when a SpringApplication is starting up and the Environment is first available for inspection and modification.    This is triggered in SpringApplication’s prepareEnvrionment method, which indica envrionment configured.   ApplicationContextInitializedEvent     Event published when a SpringApplication is starting up and the ApplicationContext is prepared and ApplicationContextInitializers have been called but before any bean definitions are loaded.    This is triggered in SpringApplication’s prepareContext method, which indicate context initialized and context inititializers are called.   ApplicationPreparedEvent      Event published as when a SpringApplication is starting up and the ApplicationContext is fully prepared but not refreshed. The bean definitions will be loaded and the Environment is ready for use at this event.    This is called at last step of SpringApplication’s prepareContext method, which indicate ApplicationContext is loaded   ObjectMapperConfigured (optional)     This is used for MvcObjectMapper    This is come from WebMvcObjectMapperConfigurer, it’s actually means HttpMessageConverter beans configured. Those used in Mvc’s RequestMappingHandlerAdapter   ServletWebServerInitializedEvent (optional)     Event to be published after the WebServer is ready. Useful for obtaining the local port of a running server. Normally it will have been started, but listeners are free to inspect the server and stop and start it if they want to.    This is triggered in WebServerStartStopLifecycle.java#start() method, after calling underlying webserver’s start method and set “running” flag to true.   Because WebServerStartStopLifecycle implemented SmartLifecycle, and it’s autoStartup flag is true, so AbastractApplicationContext’s finsihRefresh will call all lifecycleProcessoer.onRefresh(), which will invoke lifeCycle’s start() method.   Ultimately this is called when SpringApplicatin to refresh(context)   ContextRefreshedEvent     Event raised when an ApplicationContext gets initialized or refreshed. This is thrown in AbstractApplicationContext#fresh()    publishEvent(new ContextRefreshedEvent(this));   ApplicationStartedEvent     Event published once the application context has been refreshed but before any application and command line runners have been called.    It then will call following one to further publish availability change event   AvailabilityChangeEvent.publish(context, LivenessState.CORRECT);   AvailabilityChangeEvent  It’s payload is “CORRECT”   public enum LivenessState implements AvailabilityState { “Liveness” state of the application. An application is considered live when it’s running with a correct internal state. “Liveness” failure means that the internal state of the application is broken and we cannot recover from it. As a result, the platform should restart the application.   After this event, SpringApplication’s “run” method will be invoked, normally you’ll call start method of your service/runner   ApplicationReadyEvent     Event published as late as conceivably possible to indicate that the application is ready to service requests.    Then it will invoke another event for Accepting_traffic   AvailabilityChangeEvent   ApplicationEvent sent when the AvailabilityState of the application changes.   When starting Spring Application, this state will be “ACCEPTING_TRAFFIC”   “Tada!” when you get this event, it means your SpringApplication started and happy to accept requests from users.   –End–  ","categories": ["tech"],
        "tags": ["SpringBoot","ApplicationEvent"],
        "url": "/tech/2021/10/07/Spring-Boot-ApplicationEvents-DeepDive.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "How to user fire extinguisher",
        "excerpt":"Summary  As you know, staff and your safety is paramount. So what if emergency take place, such as fire in office, how to help yourself and your colleagues by knowing how to use fire extinguisher correctly. Here is a short introduction.   Rule No.1 Keep calm and don’t panic  Have a breath and don’t try to do silly things, e.g. jump out of window or rush out of office which surrounded by fire already.   How to use Fire Extinguisher      When using a fire extinguisher, remember one word PASS, that’s accronym of following 4 steps:      Pull the pin. Test in small short bursts away from fire   Aim at the base of the fire   Squeeze the trigger   Sweep across the base of the fire      You may need to use a fire extinguisher to clear an egress path.    Lastly and most importantly rule:   Only use a fire extinguisher if you are confident or have been trained to do so. The general rule is, don’t try to extinguish anything larger than an office chair.   –End–  ","categories": ["business"],
        "tags": ["Emergency","OfficeSafety"],
        "url": "/business/2021/10/08/How-to-user-fire-extinguisher.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Day-Day-Up-Java",
        "excerpt":"More developer friendly Threa Sleep   Besides calling Thread.sleep(30000) indicate sleep 30 seconds, but is it too confusing to count how many zeros there?  Alternatively you can use TimeUnit To increase readability, I believe you’ll gree following one is easier to understand for non obvious durations :  TimeUnit.SECONDS.sleep(30); // sleep 30 seconds   –End–  ","categories": ["tech"],
        "tags": ["Java","Tips"],
        "url": "/tech/2021/10/11/Day-Day-Up-Java.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Failed to talk to github.com from corporation network",
        "excerpt":"Background  It’s typical to get various network connection issues when you run commands within corporation network. For example, you’ll find diversed issues when you trying to fetch/push about your repository host in github.com.   here is a short-and-sweet page to illustrate on how to sort it out by yourself.   Errors   Could not resolve host: github.com   Symptom  git push fatal: unable to access 'https://github.com/your_repo/repo1.git/': Could not resolve host: github.com  This is emblematic network proxy error.   Solution  Depends on your running command line tool (e.g. windows prompt, gitbash, cmder, etc.) you can run following command prior to your git command  export http_proxy=http://your-company-proxy.com:8080/;  or   set http_proxy=http://your-company-proxy.com:8080/;   Authentication failed for: https://github.com/your_repo  Symptom  git push remote: Invalid username or password. fatal: Authentication failed for 'https://github.com/your_repo/repo1.git/'   Solution  Summary  This is related to your personal API token. So got to https://github.com/settings/tokens check your token status, whether it’s expired.         If it’s expried, go to genearate a new one via https://github.com/settings/tokens/new      If this is for your personal usage, you can chose “No expiration” in dropdown in new token page.       Copy your newly geneated personal access token, then rerun your command in command console   You’ll get a pop up window to ask for your new token. Paste it here as below screenshot         Check running status in your command console, it shoud working fine now.   Grab a coffee and enjoy it. :coffee: :joy:   Reference     https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token   –End–  ","categories": ["tech"],
        "tags": ["KsqlDB","Kafka"],
        "url": "/tech/2021/10/11/Could-not-resolve-github-host.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "How to watch specific kubenetes deployment by labels",
        "excerpt":"How to watch specific kubenetes deployment by labels   Background  You can use Kubenetes Java client library to watch any changes in Kubenets, so that you can wire up your hook logic to call business logic upon any changes in K8s. But normally it’s waste of resource and time to wathc all changes, so youc an apply a filter on specific resouce change by fitlering on Kube labels. Here is a mini blog to show how to do so.   Check which lable to filter on   Here is screnshot from Kubenetes LENS, I’ll use label run=dummy-service as sample. As highlight in below screenshot.      Sample Java logic   If the label of your deployment in Kubenetse is run=dummy-service, you can use following code logic    public void run() {         Watch&lt;V1Deployment&gt; watch = null;         try {             watch = Watch.createWatch(                     client,                     appsApi.listNamespacedDeploymentCall(\"YOU_NAME_SPACE\", null, null, null, null, \"run=dummy-service\", null,                             null, null, null, true, null),                     new TypeToken&lt;Watch.Response&lt;V1Deployment&gt;&gt;() {                     }.getType());         } catch (ApiException e) {             LOGGER.error(\"Error occurred in DeploymentWatcher,\", e);                     }         assert watch != null;         watch.forEach(this::setMetadata);     }   So above code will return all Deployments with label “run=dummy-service”   Reference     https://appdoc.app/artifact/io.kubernetes/client-java-api/0.1/io/kubernetes/client/apis/AppsV1beta1Api.html#listNamespacedDeploymentCall-java.lang.String-java.lang.String-java.lang.String-java.lang.String-java.lang.String-java.lang.Integer-java.lang.Boolean-io.kubernetes.client.ProgressResponseBody.ProgressListener-io.kubernetes.client.ProgressRequestBody.ProgressRequestListener-   –End–  ","categories": ["tech"],
        "tags": ["Kubenetes","K8s"],
        "url": "/tech/2021/11/01/How-to-watch-kubenetes-deployment-by-label-selector.html",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "Debug of SpringBoot run not working in IntelliJ",
        "excerpt":"Normal approach to debug maven   Normaly if you want to debug a Java application, you can use following procedures     Running application with mvnDebug, such as mvnDebug spring-boot:run   This will open a port such as 5005   Then open IntelliJ, create a remote JVM debug , attach it to port 5005 for debug      However, sometimes you’ll find your breakpoints nevet got hit.   Better/Alternative way to solve this issue.           Open a new terminal within Intellij (rather than external command line window).            Navigate to your project folder where pom.xml saved            Run following command      mvn spring-boot:run  -D\"spring-boot.run.jvmArguments\"=\"-Dimport.dataset.list=importpb -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5005\"          After a while, you’ll see following message indicate progress is listenring at designed port       [INFO] Attaching agents: [] Listening for transport dt_socket at address: 5005      Then start a new JVM debug window on port 5005, as below screenshot:              Start debug with aforesaid debug configuration             Then you’ll see breakpoint you set will be hit. As per below screenshot        –End–  ","categories": [],
        "tags": ["IntelliJ","Java","IntelliJ"],
        "url": "/blogs/tech/en/debug_not_hit_intelliJ",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "How to copy files from resources folder in jar and save to a file",
        "excerpt":"Why to extract resources from jar to local disk   How to save resource to local  disk   Firslty let’s define a class to represen the file extracted from resoruce folder   import lombok.Builder; import lombok.Data;  import java.util.Date;  @Data @Builder class FileInfo {     String fileInfo;     String fileSize;     Date lastModified; }    Then create a helper class to extract file out of resource folder and save to local disk.     The dirty job done by ResourceLoader which is one out-of-box helper class from Springframwork     import ch.qos.logback.core.util.FileUtil; import lombok.*; import lombok.extern.slf4j.Slf4j; import org.apache.commons.io.FileUtils; import org.springframework.core.io.ResourceLoader; import org.springframework.stereotype.Component; import org.springframework.util.FileCopyUtils;  import java.io.File; import java.io.FileOutputStream; import java.nio.file.Files; import java.nio.file.Paths; import java.util.Date;  @Component @RequiredArgsConstructor @Slf4j public class KeyTabFilesHelper {       /**      * This final ResourceLoader + @RequiredArgsConstructor will make this loader automatically wired from Spring      */     private final ResourceLoader resourceLoader;       /**      * Extract file content from jar file's Resource folder, then save it to local disk      * @param resourceFolder relative folder under project \"resource\" folder      * @param resourceName resource name      * @param destPath path of local disk the resource to be saved      * @return A data class provide a summary of the file saved      */     @SneakyThrows     public FileInfo saveAsFile(String resourceFolder, String resourceName, String destPath) {         final val resource = resourceLoader.getResource(\"classpath:\" + resourceFolder + File.separator + resourceName);         final val path = Paths.get(destPath, resourceName);           if (!Files.exists(path)) {             log.info(\"File NOT exist, create it and it's missing parent folder :{}\", path);             File file = new File(path.toString());             FileUtil.createMissingParentDirectories(file);         }          FileCopyUtils.copy(resource.getInputStream(), new FileOutputStream(path.toFile()));          final val fileInfo = FileInfo.builder()                 .fileInfo(path.toFile().toString())                 .fileSize(FileUtils.byteCountToDisplaySize(Files.size(path)))                 .lastModified(new Date(path.toFile().lastModified()))                 .build();          log.info(\"saved file to {}, file is:{}\", path, fileInfo);         return fileInfo;     }  }   Normal approach to debug maven   Normaly if you want to debug a Java application, you can use following procedures     Running application with mvnDebug, such as mvnDebug spring-boot:run   This will open a port such as 5005   Then open IntelliJ, create a remote JVM debug , attach it to port 5005 for debug      However, sometimes you’ll find your breakpoints nevet got hit.   Better/Alternative way to solve this issue.           Open a new terminal within Intellij (rather than external command line window).            Navigate to your project folder where pom.xml saved            Run following command      mvn spring-boot:run  -D\"spring-boot.run.jvmArguments\"=\"-Dimport.dataset.list=importpb -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5005\"          After a while, you’ll see following message indicate progress is listenring at designed port       [INFO] Attaching agents: [] Listening for transport dt_socket at address: 5005      Then start a new JVM debug window on port 5005, as below screenshot:              Start debug with aforesaid debug configuration             Then you’ll see breakpoint you set will be hit. As per below screenshot        –End–  ","categories": [],
        "tags": ["jar","Java","IntelliJ"],
        "url": "/blogs/tech/en/save_resources_to_files",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "SQLServer Error about This driver is not configured for integrated authentication",
        "excerpt":"Symptoms  When you are using integrated authentication (Kerberos connection) for MS SqlServer connection, there is one possible error :   This driver is not configured for integrated authentication. ClientConnectionId   and   no mssql-jdbc_auth-9.2.1.x64 in java.library.path:   Failed to load the sqljdbc_auth.dll cause : no sqljdbc_auth in java.library.path   Error stack trace  The error strack trace may look like following   com.microsoft.sqlserver.jdbc.SQLServerException: This driver is not configured for integrated authentication. ClientConnectionId:f965454c-897b-4707-a625-52506e450878     at com.microsoft.sqlserver.jdbc.SQLServerConnection.terminate(SQLServerConnection.java:3206) ~[mssql-jdbc-9.2.1.jre8.jar:na]     at com.microsoft.sqlserver.jdbc.AuthenticationJNI.&lt;init&gt;(AuthenticationJNI.java:72) ~[mssql-jdbc-9.2.1.jre8.jar:na]     at com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:4015) ~[mssql-jdbc-9.2.1.jre8.jar:na]    Caused by: java.lang.UnsatisfiedLinkError: no mssql-jdbc_auth-9.2.1.x64 in java.library.path: [C:\\soft\\java-11-openjdk-11.0.11.9-1.windows.redhat.x86_64\\bin, C:\\WINDOWS\\Sun\\Java\\bin, C:\\WINDOWS\\system32, C:\\WINDOWS, C:\\Python39\\Scripts\\, C:\\Python39\\, C:\\Program Files (x86)\\RSA SecurID Token Common, C:\\WINDOWS\\system32, C:\\WINDOWS, C:\\WINDOWS\\System32\\Wbem, C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\, C:\\WINDOWS\\System32\\OpenSSH\\, C:\\Program Files\\Git\\cmd, c:\\dev\\scripts\\, c:\\dev\\apache-maven-3.8.1\\\\bin, C:\\Minikube, C:\\Soft\\nodejs\\, C:\\ProgramData\\chocolatey\\bin, C:\\Program Files (x86)\\Microsoft SQL Server\\150\\DTS\\Binn\\, C:\\Program Files\\Azure Data Studio\\bin, C:\\Program Files\\Docker\\Docker\\resources\\bin, C:\\ProgramData\\DockerDesktop\\version-bin, C:\\Program Files\\dotnet\\, C:\\ProgramData\\Riverbed\\ProcessInjection\\rpictrlBin, C:\\soft\\java\\java-11-openjdk-11.0.11.9-1.windows.redhat.x86_64\\bin, C:\\Program Files (x86)\\sbt\\bin, C:\\Program Files (x86)\\scala\\bin, C:\\Program Files\\PowerShell\\7\\, C:\\WINDOWS\\system32, C:\\WINDOWS, C:\\WINDOWS\\System32\\Wbem, C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\, C:\\WINDOWS\\System32\\OpenSSH\\, C:\\Users\\XXX\\AppData\\Local\\Microsoft\\WindowsApps, C:\\soft\\JetBrains\\IntelliJ IDEA 2021.1.3\\bin, ., c:\\ProgramData\\chocolatey\\lib\\gradle\\tools\\gradle-7.2\\\\bin, C:\\soft\\Microsoft VS Code\\bin, C:\\Users\\XXX\\AppData\\Roaming\\npm, C:\\Program Files\\Azure Data Studio\\bin, C:\\soft\\java-11-openjdk-11.0.11.9-1.windows.redhat.x86_64\\bin, ., C:\\soft\\JetBrains\\IntelliJ IDEA Educational Edition 2021.2.3\\bin, ., C:\\soft\\JetBrains\\IntelliJ IDEA Community Edition 2021.2.3\\bin, ., .]     at java.base/java.lang.ClassLoader.loadLibrary(ClassLoader.java:2670) ~[na:na]     at java.base/java.lang.Runtime.loadLibrary0(Runtime.java:830) ~[na:na]     at java.base/java.lang.System.loadLibrary(System.java:1873) ~[na:na]     at com.microsoft.sqlserver.jdbc.AuthenticationJNI.&lt;clinit&gt;(AuthenticationJNI.java:51) ~[mssql-jdbc-9.2.1.jre8.jar:na]     at com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:4014) ~[mssql-jdbc-9.2.1.jre8.jar:na]     ... 46 common frames omitted      2021-11-24 16:11:58.244  WARN 38084 --- [           main] c.m.s.jdbc.internals.AuthenticationJNI   : Failed to load the sqljdbc_auth.dll cause : no sqljdbc_auth in java.library.path: [C:\\soft\\java-11-openjdk-11.0.11.9-1.windows.redhat.x86_64\\bin, C:\\WINDOWS\\Sun\\Java\\bin, C:\\WINDOWS\\system32, C:\\WINDOWS, C:\\Python39\\Scripts\\, C:\\Python39\\, C:\\Program Files (x86)\\RSA SecurID Token Common, C:\\WINDOWS\\system32, C:\\WINDOWS, C:\\WINDOWS\\System32\\Wbem, C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\, C:\\WINDOWS\\System32\\OpenSSH\\, C:\\Program Files\\Git\\cmd, c:\\dev\\scripts\\, c:\\dev\\apache-maven-3.8.1\\\\bin, C:\\Minikube, C:\\Soft\\nodejs\\, C:\\ProgramData\\chocolatey\\bin, C:\\Program Files (x86)\\Microsoft SQL Server\\150\\DTS\\Binn\\, C:\\Program Files\\Azure Data Studio\\bin, C:\\Program Files\\Docker\\Docker\\resources\\bin, C:\\ProgramData\\DockerDesktop\\version-bin, C:\\Program Files\\dotnet\\, C:\\ProgramData\\Riverbed\\ProcessInjection\\rpictrlBin, C:\\soft\\java\\java-11-openjdk-11.0.11.9-1.windows.redhat.x86_64\\bin, C:\\Program Files (x86)\\sbt\\bin, C:\\Program Files (x86)\\scala\\bin, C:\\Program Files\\PowerShell\\7\\, C:\\WINDOWS\\system32, C:\\WINDOWS, C:\\WINDOWS\\System32\\Wbem, C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\, C:\\WINDOWS\\System32\\OpenSSH\\, C:\\Users\\XXX\\AppData\\Local\\Microsoft\\WindowsApps, C:\\soft\\JetBrains\\IntelliJ IDEA 2021.1.3\\bin, ., c:\\ProgramData\\chocolatey\\lib\\gradle\\tools\\gradle-7.2\\\\bin, C:\\soft\\Microsoft VS Code\\bin, C:\\Users\\XXX\\AppData\\Roaming\\npm, C:\\Program Files\\Azure Data Studio\\bin, C:\\soft\\java-11-openjdk-11.0.11.9-1.windows.redhat.x86_64\\bin, ., C:\\soft\\JetBrains\\IntelliJ IDEA Educational Edition 2021.2.3\\bin, ., C:\\soft\\JetBrains\\IntelliJ IDEA Community Edition 2021.2.3\\bin, ., .]    Root cause  This is a  hardcode logic in Microsoft JDBC library authentication module.  Downlaoded auth file will be named as mssql-jdbc_auth-9.4.0.x64.dll, it has to renamed to sqljdbc_auth.dll and save to %JAVA_HOME%\\bin folder   Solution     Firstly download file from Microsoft folder as below   https://docs.microsoft.com/en-us/sql/connect/jdbc/release-notes-for-the-jdbc-driver?view=sql-server-ver15           Then unzip and copy the file under folder “auth”            Lastly, copy your platform dill file (e.g. mssql-jdbc_auth-9.4.0.x64.dll) and rename to sqljdbc_auth.dll          –End–   ","categories": [],
        "tags": ["SQL Server","Java","Errors&Solutions"],
        "url": "/blogs/tech/en/sqlserver_driver_not_configured_integrated_authenciation",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      },{
        "title": "How logging system Bootstrapped in Spring Boot Application",
        "excerpt":"Summary  Following diagram demonstrated the process to bootstrap and use Logback for loggings in Spring Boot applciation.   Video version of this explanation  If you’d perfer a more intuitive and vision version, check it out below.     Workflow diagram      Key steps explained     The first section for logic in SpringBoot Application domain       When a SpringBoot application start to execute SpringApplicaiton.run , internally it will execute prepareEnvironment , which will create or load application run environment , such as environment variables.   SpringBoot use Observer Pattern and  SpringApplicationEvent to keep tack of each key steps in Spring Boot Application life cycle.  SpringBoot will pass loaded environments as argument to list of registered listeners.   listeners.environmentPrepared(bootstrapContext, environment);     SpringBoot’s Event publisher (EventPublishingRunListener) will publish an event called ApplicationEnvironmentPreparedEvent via it’s built-in multicast publisher.   LoggingApplicationListener is one of 6 components in Spring subscribed to ApplicationEnvironmentPreparedEvent. It will configure Spring’s LoggingSystem. If the environment contains a logging.config property, it will be used to bootstrap the logging system, otherwise a default configuration is used.   SpringBoot will leverage Classloader to try to load any LoggingSystem or its descendants.   If you added sl4j into class path, LoggingSystemFactory will produce a new LogbackLoggingSystem, which extended LoggingSystem ultimately.   return new LogbackLoggingSystem(classLoader);     Now we move to domain of logging system in slf4j &amp; logback.       LogbackLoggingSystem will invoke doConfig  of SpringBootJoranConfigurator to setup logging sub system. This Configurator is backbone of Joran framework.      Joran: a mature, flexible and powerful configuration framework. On which logback’s configuration logic is based. Details check it out at Joran Configuration framework       Interpreter will parse configuration details in logback.xml , internally logback would convert elements of aforesaid xml file as a list of Event , then Interpreter has one EventPlayer to process those events one by one.e   Interpreter is Joran’s main driving class. It extends SAX DefaultHandler which invokes various actions according to predefined patterns (configured as RuleStore ). It leverage Constructor to start a new instance of LogstashEncoder in memory.   Joran defines StartEvent  and EndEvent  to process beginning and ending element in XML configuration. For &lt;/encoder&gt; , Joran will use reflection to call property setter of ConsolerAppender, and inject aforesaid logstashEncoder as property encoder .   This is the last step of this setup. When there are logs to be wrote, the log Appender’s internal encoder  (that’s LogstashEncoder in this case) will be invoked to render output message.   –End–   ","categories": [],
        "tags": ["SpringBoot","Java","Logback"],
        "url": "/blogs/tech/en/boostrap_logging_in_springbootapplication",
        "teaser": "/assets/images/HighlyDistinguishLogo-stamp.png"
      }]
